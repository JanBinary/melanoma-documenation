[
  {
    "objectID": "presentation.html#relevanz",
    "href": "presentation.html#relevanz",
    "title": "Melanoma Pr√§sentation",
    "section": "1.1 Relevanz",
    "text": "1.1 Relevanz\n\n\nAlle 120 Minuten stirbt ein Mensch an Hautkrebs. Wie kann k√ºnstliche Intelligenz Leben retten? [1]\n\n\n\n\n1895 entdeckte Wilhelm R√∂ntgen die R√∂ntgenstrahlen und revolutionierte die Medizin. Steht uns mit k√ºnstlicher Intelligenz eine noch gr√∂√üere Revolution bevor? [2]\n\n\n\n120 Minuten - Todesursachenstatistik 2022\nWie kann Ki helfen (joke)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#was-ist-hautkrebs",
    "href": "presentation.html#was-ist-hautkrebs",
    "title": "Melanoma Pr√§sentation",
    "section": "1.2 Was ist Hautkrebs?",
    "text": "1.2 Was ist Hautkrebs?\n\nHautkrebs entsteht, wenn bestimmte Zellen der Haut unkontrolliert wachsen und in das umliegende Gewebe eindringen. Man unterscheidet dabei zwischen hellem Hautkrebs und schwarzem Hautkrebs.\n\n\n\nAbbildung¬†1: Querschnitt vom Hautkrebs\n\n\nheller - lokal aber auch tief\nschwarzer - kann fr√ºh streuen (Blut- Lymphbahnen), Metastasen in anderen Organen bilden",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#nicht-invasive-diagnoseverfahren",
    "href": "presentation.html#nicht-invasive-diagnoseverfahren",
    "title": "Melanoma Pr√§sentation",
    "section": "1.3 nicht-invasive Diagnoseverfahren",
    "text": "1.3 nicht-invasive Diagnoseverfahren\n\nKlinische Untersuchung\n\nVisuelle Begutachtung auff√§lliger Hautver√§nderungen durch Hautarzt\n\nDermatoskopie\n\nVergr√∂√üert und beleuchtet die Haut, macht Pigment- und Gef√§√üstrukturen sichtbar\n\nKonfokale Laserscanmikroskopie\nOptische Koh√§renztomographie\n\n\n\nAuge // sehr ungenau\nDermatoskopie // Signifikant besser, minimal mehr Aufwand, aktueller Standard\nLaserscanmikroskopie und Optische Koh√§renztomographie // Spezial Equipment + teuer\nBiopsie // Diagnosensicherung",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#ki-und-computer-vision",
    "href": "presentation.html#ki-und-computer-vision",
    "title": "Melanoma Pr√§sentation",
    "section": "1.4 KI und Computer Vision",
    "text": "1.4 KI und Computer Vision\n\n\n\nJan\n\nAuf Historie eingehen (CNN -&gt; ViT -&gt; CNN bis Hybrid)\nImageNet: Gro√üe Datensammlung (14.197.122 Bilder)\nHerausforderungen mit der Zeit: Gr√∂√üere Bilder, Generalisierte Probleme (eine L√∂sung f√ºr alles)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#ziele",
    "href": "presentation.html#ziele",
    "title": "Melanoma Pr√§sentation",
    "section": "1.5 Ziele",
    "text": "1.5 Ziele\n\nAuseinandersetzung mit k√ºnstlicher Intelligenz\nVerst√§ndnis der Potenziale und Grenzen der Technologie\nEinordnung der Ergebnisse\nEinsch√§tzung, ob und wie KI in diesem Anwendungsfeld eingesetzt werden kann\n\n\nMarc",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#abgrenzung",
    "href": "presentation.html#abgrenzung",
    "title": "Melanoma Pr√§sentation",
    "section": "1.6 Abgrenzung",
    "text": "1.6 Abgrenzung\n\nFokus auf Dermatoskopie\nKleiner Datensatz\n\n10015 Samples\n7 Features",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#unser-vorgehen",
    "href": "presentation.html#unser-vorgehen",
    "title": "Melanoma Pr√§sentation",
    "section": "1.7 Unser Vorgehen",
    "text": "1.7 Unser Vorgehen\n\n\n\n\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    section Recherche\n    Thema definieren           :done,    lit1, 2025-04-04,2025-04-05\n    Literatur auswerten        :done,    lit2, 2025-04-05,2025-04-22\n    \n    section Planung\n    Ziele definieren           :done,    plan1, 2025-04-05,2025-04-09\n    Ressourcen planen          :done,    plan2, 2025-04-09,2025-04-12\n    \n    section Grundlagen\n    Theoretische Basis         :done,         grund1, 2025-04-09,2025-04-27\n    Framework definieren       :done,         grund2, 2025-04-09,2025-04-12\n    \n    section Exploration\n    Datensatz Verst√§ndnis      :done,         data1, 2025-04-14,2025-04-27\n    Explorative Analyse        :done,         data2, 2025-04-14,2025-05-11\n    \n    section Modell\n    Mathematische Grundlagen   :done,          model4, 2025-04-29, 2025-05-13\n    Modell konzipieren         :done,         model1, 2025-04-29,2025-05-17\n    Implementierung            :done,         model2, 2025-05-21,2025-06-23\n    Testing & Validierung      :done,         model3, 2025-06-08,2025-06-28\n    \n    section Diskussion\n    Ergebnisse interpretieren  :done,         dis1, 2025-06-10,2025-06-28\n    Fazit & Ausblick          :done,          dis2, 2025-06-24,2025-06-29\n    Finalisierung             :active,          disk3, 2025-06-26,2025-07-02\n\n\n\n\n\n\n\n\nAbbildung¬†2: Projektablauf",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#grundlagen-computer-vision",
    "href": "presentation.html#grundlagen-computer-vision",
    "title": "Melanoma Pr√§sentation",
    "section": "2.1 Grundlagen Computer Vision",
    "text": "2.1 Grundlagen Computer Vision\n\nWie kann ein Computer sehen?\n\n\nFaltung (Convolution)\n\\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)K(i, j)}\n\\]\nAufmerksamkeitsmechanismus (Attention)\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n\\]\n\n\na offset zur Kernelmitte (a = (n + 1)/2) -&gt; Zur Verschiebung des Filters um den Pixel herum\nUmfrage (wer kennt sich mit ViT aus)\nQ (Query) : Was will ich wissen?\nK (Key) : Was ist daf√ºr relevant?\nV (Value) : Welchen Kontext trage ich bei?\nT : Transposition (QKT ergibt eine quadratische Matrix)\ndk : Dimensionen der Key Vektoren",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#was-passiert-bei-der-faltung",
    "href": "presentation.html#was-passiert-bei-der-faltung",
    "title": "Melanoma Pr√§sentation",
    "section": "2.2 Was passiert bei der Faltung",
    "text": "2.2 Was passiert bei der Faltung\n\nüîç Ein Filter \\(K\\) √ºberlagert das Bild \\(I\\) und blickt auf einen \\(n \\times n\\) Bereich\n‚ûï √úberlappende Werte werden multipliziert und aufsummiert\nüéØ Ein einzelner neuer Wert entsteht \\(I^{\\ast} (x, y)\\)\n\n\\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)K(i, j)}\n\\]",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#anwendung-der-faltung-in-cnns",
    "href": "presentation.html#anwendung-der-faltung-in-cnns",
    "title": "Melanoma Pr√§sentation",
    "section": "2.3 Anwendung der Faltung in CNNs",
    "text": "2.3 Anwendung der Faltung in CNNs\n\n‚úÖ Je nach Wahl des Kernels \\(K\\) werden unterschiedliche Merkmale hervorgehoben\nüèéÔ∏è Der Kernel \\(K\\) bewegt sich st√ºckweise √ºber das komplette Bild \\(I\\)\nüèÉüèø‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è Die Schrittweite des Kernels ist variabel und wird als Stride bezeichnet\nüÖæÔ∏è Padding ist m√∂glich (0 Werte am Rand der Matrix)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#begriffskl√§rung-convolution",
    "href": "presentation.html#begriffskl√§rung-convolution",
    "title": "Melanoma Pr√§sentation",
    "section": "2.4 Begriffskl√§rung Convolution",
    "text": "2.4 Begriffskl√§rung Convolution\n\n\n\n\n\nConv\n\n\n\n\n\n\nDepthwise Conv\n\n\n\n\n\nUnterschied: Bei depthwise ein Kernel pro Kanal\nNormale Convolution ist deutlich Rechenaufw√§nder (jeder Filter auf alle Kan√§le)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#section-2",
    "href": "presentation.html#section-2",
    "title": "Melanoma Pr√§sentation",
    "section": "",
    "text": "Eingehen auf Faltungsprozesse in dem CNN Beispiel\nResultat der Faltung sind Feature Maps",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#pooling-in-cnns",
    "href": "presentation.html#pooling-in-cnns",
    "title": "Melanoma Pr√§sentation",
    "section": "2.5 Pooling in CNNs",
    "text": "2.5 Pooling in CNNs\n\n‚¨áÔ∏è Reduziert die Bildgr√∂√üe durch Zusammenfassen benachtbarter Pixel\nüíª Verringert Rechenaufwand und Anzahl der Paramter\nüü∞ Ziel ist Translationsinvarianz (\\(f(A) = f(A + t)\\))\nüèÉüèø‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è Ein Fenster bewegt sich st√ºckweise √ºber das komplette Bild (√§hnlich zur Faltung)\n\n\n\nTranslationsinvarianz: Ein Objekt wird unabh√§ngig von seiner Position erkannt\nVorteil: Gr√∂√üe der Matrizen wird reduziert (Rechenaufwand)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#section-4",
    "href": "presentation.html#section-4",
    "title": "Melanoma Pr√§sentation",
    "section": "",
    "text": "Eingehen auf Pooling (kleine Feature Maps)\nR√ºckschluss zu Translationsinvarianz",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#aktivierungsfunktionen",
    "href": "presentation.html#aktivierungsfunktionen",
    "title": "Melanoma Pr√§sentation",
    "section": "2.6 Aktivierungsfunktionen",
    "text": "2.6 Aktivierungsfunktionen\n\nüìè Werden nach Faltungen / Linearkombinationen eingesetzt\nüé¢ Ohne Aktivierung w√§re ein tiefes Netz nur eine lineare Funktion\nüìä Helfen beim Lernen von Mustern und Entscheidungsgrenzen\nüíØ Werden punktweise auf jedes Pixel / Feature angewendet\n\n\n\nLinearkombination: Gewichtete Summe von Vectoren\nWerden auf FeatureMap punktweise eingesetzt\nWerden auch in NNs verwendet\nEs gibt Unterschiedliche Funktionen je nach Anwendungsfall",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#architektur-eines-cnns",
    "href": "presentation.html#architektur-eines-cnns",
    "title": "Melanoma Pr√§sentation",
    "section": "2.7 Architektur eines CNNs",
    "text": "2.7 Architektur eines CNNs\n\n\n\nR√ºckschluss auf Aktivierungsfunktionen",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#verdichtungs--und-verst√§rkungsbl√∂cke-se",
    "href": "presentation.html#verdichtungs--und-verst√§rkungsbl√∂cke-se",
    "title": "Melanoma Pr√§sentation",
    "section": "2.8 Verdichtungs- und Verst√§rkungsbl√∂cke (SE1)",
    "text": "2.8 Verdichtungs- und Verst√§rkungsbl√∂cke (SE1)\n\n‚ÑπÔ∏è Analysiert wie wichtig jeder Kanal der Feature Map ist und verst√§rkt oder unterdr√ºckt\n‚ÑπÔ∏è Verwendet Global Average Pooling\nüéØ Adaptive Belohnung relevanter Kan√§le\nüéØ Bessere Genauigkeit durch gezieltere Merkmalsverarbeitung\n\n\n\nSqeeze and Excitation\nOutput kein Vector\nVector wird nur zu gewichtung verwendet\nKann mit Koeffizienten versehen werden um Einfluss zu reduzieren / verst√§rken\n\n\nSqueeze and Excitation",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#designpatterns-mit-faltung",
    "href": "presentation.html#designpatterns-mit-faltung",
    "title": "Melanoma Pr√§sentation",
    "section": "2.9 Designpatterns mit Faltung",
    "text": "2.9 Designpatterns mit Faltung\n\n\n\n\n\n\nAbbildung¬†5: MBConv und fused-MBConv\n\n\n\n\n\nDepthwise Conv : Getrennte Faltung pro Kanal [5]",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#gradienten-und-deren-aussagekraft",
    "href": "presentation.html#gradienten-und-deren-aussagekraft",
    "title": "Melanoma Pr√§sentation",
    "section": "2.10 Gradienten und deren Aussagekraft",
    "text": "2.10 Gradienten und deren Aussagekraft\n\n‚ÑπÔ∏è Der Gradient ist die Ableitung einer Funktion nach ihren Eingabewerten\nüéØ Zeigt Richtung des st√§rksten Wachstums und Einfluss der Parameter\nüñºÔ∏è Dargestellt durch den Nabla Operator \\(\\nabla\\)\n\n\n\nEin Vector aus partiellen Ableitungen\nZeigt ebenfalls Einfluss einzelner Parameter an\nKann in mehrdimensionalen Bereich als Kostenlandschaft interpretiert werden bsp. sp√§ter",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#section-7",
    "href": "presentation.html#section-7",
    "title": "Melanoma Pr√§sentation",
    "section": "",
    "text": "\\[\nf : \\mathbb{R}^2 \\to \\mathbb{R}\n\\]\n\\[\nf(x, y) = 2x^2 + 3y\n\\]\n\\[\n\\nabla f =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4x \\\\\n3\n\\end{pmatrix}\n\\]\n\n\nf ist eine Funktion die ein Element aus einem 2d Raum auf einen Wert abbildet\njedem Punkt (x, y) wird eine Reele Zahl zugeordnet\nx gr√∂√üter Einfluss auf die Steigung von f\ny konstant positiven Einfluss (Steigung als f‚Äô)\nje gr√∂√üer x wird desto horizontaler wird der Vektor (bspw. unterschied x = 1000 und x = 1001)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#section-8",
    "href": "presentation.html#section-8",
    "title": "Melanoma Pr√§sentation",
    "section": "",
    "text": "\\[\n\\nabla f =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4x \\\\\n3\n\\end{pmatrix}\n\\]\n\n‚ÑπÔ∏è \\(x\\) hat den gr√∂√üten Einfluss auf die Steigung von \\(f\\)\n‚ÑπÔ∏è \\(y\\) hat konstant positiven Einfluss auf die Funktion\n‚ÑπÔ∏è \\(\\nabla f\\): Richtung des gr√∂√üten Wachstums\n‚ÑπÔ∏è \\(-\\nabla f\\): Richtung der gr√∂√üten Abnahme",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#kostenfunktionen-loss-function",
    "href": "presentation.html#kostenfunktionen-loss-function",
    "title": "Melanoma Pr√§sentation",
    "section": "2.11 Kostenfunktionen (Loss Function)",
    "text": "2.11 Kostenfunktionen (Loss Function)\n\n‚ÑπÔ∏è Messen den Unterschied zwischen Sch√§tzung \\(\\hat{y}\\) und Zielwert \\(y\\)\n‚ÑπÔ∏è Bei Klassifikationsproblemen eignet sich Cross Entropy Loss\nüéØ Fehlerwert durch Anpassung der Gewichte zu minimieren\n\n\n\n\\(\\hat{y}\\): Sch√§tzer, \\(y\\): Wert (Ermitteln Differenz)\nWird zur Anpassung von Gewichten verwendet (zsm. mit Gradienten)\nZiel beim Training : Soll iterativ reduziert werden",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#gewichtsanpassung",
    "href": "presentation.html#gewichtsanpassung",
    "title": "Melanoma Pr√§sentation",
    "section": "2.12 Gewichtsanpassung",
    "text": "2.12 Gewichtsanpassung\n\n‚ÑπÔ∏è Nutzt den Gradienten der Kostenfunktion, um zu bestimmen, wie stark jedes Gewicht beitr√§gt\n‚ÑπÔ∏è Beruht auf der Kettenregel der Ableitung\nüéØ Fehlerwert durch Anpassung der Gewichte zu minimieren\n\n\n\nGrundlage: Neuronen, FeatureMaps, etc. werden als Funktionen interpretiert\nIN NNs sind eingaben der Funktion Gewichte, Ausgabe Vorheriger Neuronen (ebenfalls Funktionen) und Bias\nDurch Innere Funktionen (Neuron -&gt; Neuron (evtl. mit Aktivierungsfunktion)) kann partiell bis zu den Gewichten abgeleitet werden\nEinfluss einzelner Paramter zur Steigung der Kostenfunktion kann ermittelt werden",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#section-11",
    "href": "presentation.html#section-11",
    "title": "Melanoma Pr√§sentation",
    "section": "",
    "text": "Kettenregel\n\\[\nf(x) = u(v(x))\n\\]\n\\[\nf'(x) = u'(v(x)) v'(x)\n\\]\n\n\n\nwichtig: √§u√üere mal innere Ableitung\nverschachtelte Funktionen: R√ºckschluss zu Neuronen sp√§ter Schichten\n\n\n\nReminder",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#section-12",
    "href": "presentation.html#section-12",
    "title": "Melanoma Pr√§sentation",
    "section": "",
    "text": "Kettenregel\n\\[\nL(\\sigma(z_6))\n\\]\n\\[\n\\frac{dL}{dz_6} = \\frac{dL}{d \\sigma (z_6)} \\frac{d \\sigma (z_6)}{dz_6}\n\\]\n\n\nd: totale Ableitung (ber√ºcksichtigt alle Abh√§ngigkeiten)\ndel (Partialzeichen): partielle Ableitung (ber√ºcksichtig eine Variable alle anderen fix)\nHier egal weil simples Beispiel mit einer Variable\n\n\n\nProblem: \\(z_6(\\sigma (z_5))\\), \\(z_5(\\sigma (z_4))\\), ‚Ä¶\n\n\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\n\n\\[\n\\frac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x))\n\\]\n\n\n\n\nHier das Problem erw√§hnen sigmoud Ableitung in dem Produkt (aus Kettenregel)\nSigmoud Ableitung sehr schnell nahe null\nViele null werte im Produkt = wenig Gewichtskorrektur\n\n\n\n\nReminder",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#l√∂sung-durch-skip-connections",
    "href": "presentation.html#l√∂sung-durch-skip-connections",
    "title": "Melanoma Pr√§sentation",
    "section": "2.13 L√∂sung durch Skip Connections",
    "text": "2.13 L√∂sung durch Skip Connections\n\n‚ÑπÔ∏è √úberspringen einer oder mehrerer Schichten, indem Eingabe / Neuron direkt zu sp√§teren Schichten weiterleitet\nüéØ Hilft beim Training tiefer Netze durch verbesserung des Gradientenfluss\n\n\n\nAbbildung¬†6: Skip Connections\n\n\nMathematisch als zus√§tzliche Eingabe gewertet (in die Funktion Neuron / Ausgabe)\nAnwendung Gewichtskorrektur ohne gro√ües Produkt\nHebt Einluss fr√ºherer Schichten hervor\nF√ºhrt zu einer Gl√§ttung der Kostenlandschaft",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#fehlerlandschaft-nach-skip-connections",
    "href": "presentation.html#fehlerlandschaft-nach-skip-connections",
    "title": "Melanoma Pr√§sentation",
    "section": "2.14 Fehlerlandschaft nach Skip Connections",
    "text": "2.14 Fehlerlandschaft nach Skip Connections\n\n\nAbbildung¬†8: Fehlerlandschaft Kostenfunktion\n\n\nBeispiel beim Training versucht man einen Ball in globale Minima zu rollen\nZeigt Gradienten (Vectorfeld) einer Kostenlandschaft\nAuswirkung von Skip Connections (globales Minima einfacher zu erkennen)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#vorstellung-des-datensatzes",
    "href": "presentation.html#vorstellung-des-datensatzes",
    "title": "Melanoma Pr√§sentation",
    "section": "3.1 Vorstellung des Datensatzes",
    "text": "3.1 Vorstellung des Datensatzes\n\n\n\n\nTabelle¬†1: Datensatz\n\n\n\n\n\n\n\n\n\n\nlesion_id\nimage_id\ndx\ndx_type\nage\nsex\nlocalization\n\n\n\n\n5080\nHAM_0004175\nISIC_0028806\nnv\nfollow_up\n50.0\nfemale\nabdomen\n\n\n564\nHAM_0006595\nISIC_0028223\nbkl\nhisto\n55.0\nfemale\nface\n\n\n7369\nHAM_0005248\nISIC_0032948\nnv\nhisto\n45.0\nfemale\nupper extremity\n\n\n5150\nHAM_0001592\nISIC_0030686\nnv\nfollow_up\n55.0\nmale\nback\n\n\n9790\nHAM_0004609\nISIC_0024710\nakiec\nhisto\n75.0\nmale\nback\n\n\n9103\nHAM_0000976\nISIC_0031880\nnv\nhisto\n40.0\nmale\nchest\n\n\n3013\nHAM_0000256\nISIC_0029347\nnv\nfollow_up\n50.0\nmale\nback\n\n\n8961\nHAM_0000762\nISIC_0028982\nnv\nhisto\n30.0\nfemale\nback\n\n\n\n\n\n\n\n\n\n\n\n\ndx = Diagnose der Hautl√§sion\ndx_type = Diagnosemethode\nlesion_id = L√§sions ID (7.470)\nabdomen: Bauchregion\nupper extremity: Schulter / Arm",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#beispiele-aus-dem-datensatz",
    "href": "presentation.html#beispiele-aus-dem-datensatz",
    "title": "Melanoma Pr√§sentation",
    "section": "3.2 Beispiele aus dem Datensatz",
    "text": "3.2 Beispiele aus dem Datensatz\n\nAKIECBCCBKLDFMELNVVASC\n\n\n\n\n\n\n\n\nAbbildung¬†9: Aktinische Keratose\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†10: Basalzellkarzinom\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†11: Keratosen-√§hnliche L√§sionen\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†12: Dermatofibrom\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†13: Melanom\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†14: Melanozytische N√§vi\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†15: Gef√§√ül√§sionen\n\n\n\n\n\n\n\nGef√§hrlich:\n\nmel : Melanom (gef√§hrlichste)\nbcc: Basalzellkarzinom\nakiec: Aktinische Keratose",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#datenexploration---altersgruppen",
    "href": "presentation.html#datenexploration---altersgruppen",
    "title": "Melanoma Pr√§sentation",
    "section": "3.3 Datenexploration - Altersgruppen",
    "text": "3.3 Datenexploration - Altersgruppen\n\n\nAbbildung¬†16: Anzahl der gef√§hrlichen Diagnosen nach Alter",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#datenexploration---k√∂rperstellen",
    "href": "presentation.html#datenexploration---k√∂rperstellen",
    "title": "Melanoma Pr√§sentation",
    "section": "3.4 Datenexploration - K√∂rperstellen",
    "text": "3.4 Datenexploration - K√∂rperstellen\n\n\nAbbildung¬†17: Verteilung der K√∂rperstellen nach Diagnose (gef√§hrl. Diagnosen)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#datenexploration---anteile-gef√§hrlicher-diagnosen",
    "href": "presentation.html#datenexploration---anteile-gef√§hrlicher-diagnosen",
    "title": "Melanoma Pr√§sentation",
    "section": "3.5 Datenexploration - Anteile gef√§hrlicher Diagnosen",
    "text": "3.5 Datenexploration - Anteile gef√§hrlicher Diagnosen\n\n\nAbbildung¬†18: Anteil gef√§hrlicher Diagnosen unter allen gef√§hrlichen F√§llen",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#unser-modell-efficientnetv2-bb",
    "href": "presentation.html#unser-modell-efficientnetv2-bb",
    "title": "Melanoma Pr√§sentation",
    "section": "3.6 Unser Modell (EfficientNetV2 BB)",
    "text": "3.6 Unser Modell (EfficientNetV2 BB)\n\n‚ÑπÔ∏è Kombination aus Neural Architecture Search1 und manuellem Feintuning\n‚ÑπÔ∏è Integriert Fused-MBConv Bl√∂cke (Abbildung¬†5)\n‚ÑπÔ∏è Basiert auf ImageNet Datenbasis\nüéØ Schnelles Training gro√üer Modelle\nüéØ Geeignet f√ºr Transfer Learning durch Generalisierung\n\n\nSergei\n\nNAS -&gt; gewisse Architekturentscheidungen werden beim Training getroffen (bsp. Random) und die beste wird umgesetzt\nearly Optimierung durch Fused-MBConv\nFused-MBConv: Am Anfang effizienter (Bei weniger Dimensionen durch Verwendung normaler Faltung)\nSp√§ter Depthwise effizienter (ein Kernel pro Kanal nicht jeder Kernel alle Kan√§le)\n\n\nNAS : Automatisierter Prozess zur Architekturfindung in CNNs",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#modellevaluierung",
    "href": "presentation.html#modellevaluierung",
    "title": "Melanoma Pr√§sentation",
    "section": "3.7 Modellevaluierung",
    "text": "3.7 Modellevaluierung\n\n\n\n\n\n\nAbbildung¬†21: Benchmark TPU days\n\n\n\n\n\n\nTPU - Chip von Google f√ºr KI Training\nTPU days - Rechenleistung davon √ºber 24h\n\n\n\nTPU day : Rechenkapazit√§t die eine TPU in einem Tag liefert [6]",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#ergebnisse-der-eigenentwicklung",
    "href": "presentation.html#ergebnisse-der-eigenentwicklung",
    "title": "Melanoma Pr√§sentation",
    "section": "4.1 Ergebnisse der Eigenentwicklung",
    "text": "4.1 Ergebnisse der Eigenentwicklung\n\n\nAbbildung¬†24: Trainingsverlauf\n\n\nAufteilung 20% Testen; 80% Trainieren\nTrain Accuracy steigt mit den Epochen\nVali Accuracy leichter positiver Trend, f√§ngt an zu stagnieren\nWenn Train steigt und Vali stagniert -&gt; Overfitting\nSelbiges unten aus der Lossfunction-Sicht",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#potenzial",
    "href": "presentation.html#potenzial",
    "title": "Melanoma Pr√§sentation",
    "section": "4.2 Potenzial",
    "text": "4.2 Potenzial\n\nWo sehen wir konkretes Verbesserungspotential?\n\nKostenfunktion anhand von Gewichtungen anpassen, um falschklassifizierte echte gef√§hrliche Diagnosen st√§rker zu bestrafen\nWeiteres Training auf einer gr√∂√üeren Datenbasis inkl. h√∂herem Anteil von gef√§hrlichenen Diagnosen\nTraining eines spezifischen CNNs statt allgemeinem EfficientNetV2-CNN\nHinzuf√ºgen weiterer Layers im neuronalen Netz\n\n\n\n\nDatensatz aus dem Havard-Universe -&gt; non plus Ultra\nKI sollte auch mit schlechteren Bildern klarkommen\nEfficientNetv2-B0 verwendet (beschleunigung vom Training)\nVielfalt der Hauttypen im Trainingsdatensatz ist ein ausschlaggebender Faktor",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#grenzen",
    "href": "presentation.html#grenzen",
    "title": "Melanoma Pr√§sentation",
    "section": "4.3 Grenzen",
    "text": "4.3 Grenzen\n\nWo sehen wir konkrete Limits?\n\nBei der Nachvollziehbarkeit der Kriterien zur Klassifikation durch das CNN (Blackbox)\nPerfekte Filterwerte nicht realistisch (nach aktueller Forschung)\nBiopsie zur Sicherung der Diagnose wird nicht ersetzt\n\n\n\n\nNachvollziehbarkeit der Features teilweise m√∂glich: Gradient-weighted Class Activation Mapping (Grad-CAM)\nVideo zum Verfahren: Grad-CAM",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#zusammenfassung",
    "href": "presentation.html#zusammenfassung",
    "title": "Melanoma Pr√§sentation",
    "section": "4.4 Zusammenfassung",
    "text": "4.4 Zusammenfassung\n\nUnser Modell ist funktional, aber m√ºsste weiter angepasst werden, um die Genauigkeit zu verbessern\n\nVerbesserungspotenziale Potenziale sind vorhanden\n\n\n\nMarc",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#r√ºckblick",
    "href": "presentation.html#r√ºckblick",
    "title": "Melanoma Pr√§sentation",
    "section": "4.5 R√ºckblick",
    "text": "4.5 R√ºckblick\n\nAlle 120 Minuten stirbt ein Mensch an Hautkrebs. Wie kann k√ºnstliche Intelligenz Leben retten?\n\nAktuell kann KI vorallem bestehende Prozesse in der Diagnostik unterst√ºtzen und beschleunigen, um auf diese Weise Menschenleben zu retten",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#ausblick",
    "href": "presentation.html#ausblick",
    "title": "Melanoma Pr√§sentation",
    "section": "4.6 Ausblick",
    "text": "4.6 Ausblick\n\n\nAbbildung¬†26: Ablauf Smartphone App [7]\n\n\nHausartz erstellt unter anweisung der App 2 Fotos (nah- und Fern; Standartisiert -&gt; App: gleichen Einstellungen in Aufl√∂sung, Bildausschnitt, Helligkeit und Kontrast)\nKI f√ºhrt priorisierung durch (harmlos¬´, ¬ªriskant¬´ oder ¬ªgef√§hrlich¬´) !!keine Diagnose!!\nDermatologe betrachet Bilder und setzt pers√∂nliches oder telefonisches/Video Gespr√§ch an\nVerdachtsf√§lle m√ºssen vorort genauer untersucht werden\n\n\nBilddaten und Infos von 4000 Patienten\n80% der F√§lle bei denen der Hautartz aufgesucht wird stellen sich als ungef√§hrlich heraus\nBeschleunigung der Diagnose Prozesse",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#alternative-modelle",
    "href": "presentation.html#alternative-modelle",
    "title": "Melanoma Pr√§sentation",
    "section": "4.7 Alternative Modelle",
    "text": "4.7 Alternative Modelle\n\n\nAbbildung¬†28: Deep Learning Models [9]\n\n\nBoxplot\nDurchschnittliche Genauigkeit aller Modelle: 86.20%\nEfficientNet: worst 5.3%; best: 90%\nAm H√§ufigsten verwendetes Modell (EfficientNet)",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#llms-im-test",
    "href": "presentation.html#llms-im-test",
    "title": "Melanoma Pr√§sentation",
    "section": "4.8 LLMs im Test",
    "text": "4.8 LLMs im Test\n\n\n\nTabelle¬†3: Test mit LLMs\n\n\n\n\n\nImage Id\ndx\nClaude\nChatGPT\nOwn Model\n\n\n\n\nISIC_0031923\nnv\nmel\nmel\nnv\n\n\nISIC_0026652\nmel\nbkl\nnv\nmel\n\n\nISIC_0030583\nbkl\nbcc\nbcc\nbkl\n\n\n\n\n\n\n\nFazit: LLMs nicht zur Diagnose verwenden üòâ\n\n\n\nWichtig: Nicht signifikant (LLMs nicht daf√ºr konzipiert) - Test nur aus Spa√ü\n\nPrompt: Bitte klassifiere die folgenden Bilder von Hautl√§sionen anhand der angeh√§ngten Klassen label_mappings = { ‚Äúakiec‚Äù: 0, ‚Äúbcc‚Äù: 1, ‚Äúbkl‚Äù: 2, ‚Äúdf‚Äù: 3, ‚Äúnv‚Äù: 4, ‚Äúmel‚Äù: 5, ‚Äúvasc‚Äù: 6 }",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "presentation.html#unsere-erfahrungen",
    "href": "presentation.html#unsere-erfahrungen",
    "title": "Melanoma Pr√§sentation",
    "section": "5.1 Unsere Erfahrungen",
    "text": "5.1 Unsere Erfahrungen\n\nProgrammierung\n\nFramework verst√§ndnis\n\nDokumentation\n\nUmformulierung\nDarstellung\n\nAllgemein\n\nWissen\nRecherche\n\n\n\nAlle",
    "crumbs": [
      "Melanoma Pr√§sentation"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html",
    "href": "kapitel/grundlagen.html",
    "title": "Grundlagen",
    "section": "",
    "text": "Hautkrebs bezeichnet allgemein die maligne1 Neubildung bestimmter Hautzellen, die unkontrolliert wachsen und in umliegendes Gewebe eindringen k√∂nnen. Die h√§ufigsten Formen entstehen aus Zellen der Epidermis, insbesondere aus Keratinozyten und Melanozyten. Medizinisch wird Hautkrebs in ‚Äûhellen Hautkrebs‚Äú (Nicht-Melanom-Hautkrebs, NMSC), der vorwiegend aus Keratinozyten entsteht und ‚Äûschwarzen Hautkrebs‚Äú (malignes Melanom), der aus Melanozyten hervorgeht, unterschieden. [1]\n\n\n\nHeller Hautkrebs umfasst haupts√§chlich das Basalzellkarzinom (Basaliom) und das Plattenepithelkarzinom (Spinaliom) sowie deren Vorstufen, z.‚ÄØB. aktinische Keratosen. Diese Tumore neigen meist zu lokal invasivem Wachstum, metastasieren aber selten.\nBeispiel: Ein Basalzellkarzinom w√§chst oft tief in die Hautschichten und kann sogar Knorpel oder Knochen angreifen, bleibt aber fast immer auf diese Region begrenzt und streut nicht in Lunge oder Leber.\nDer ‚Äûschwarze Hautkrebs‚Äú, das maligne Melanom, geht von den pigmentbildenden Melanozyten aus und gilt als besonders aggressiv, da er fr√ºh metastasieren kann.\nBeispiel: Ein malignes Melanom kann bereits in einem fr√ºhen Stadium Krebszellen √ºber Blut- oder Lymphbahnen streuen und so Metastasen in anderen Organen wie Lunge, Leber oder Gehirn bilden, selbst wenn der Prim√§rtumor an der Haut noch relativ klein erscheint. [2]\n\n\n\n\n\n\nTabelle¬†1: Hautkrebsarten\n\n\n\n\n\n\n\n\n\n\n\n\nHautkrebsart\nUrsprung\nCharakteristika\nMetastasierung\nH√§ufigkeit\n\n\n\n\nBasalzellkarzinom (Basaliom)\nBasalzellen der Epidermis\nLangsames Wachstum, selten metastasierend\nSehr selten\nH√§ufigster Hautkrebs weltweit\n\n\nAktinische Keratosen & intraepitheliales Karzinom (Morbus Bowen)\nKeratinozyten der Epidermis\nFr√ºhform/Pr√§kanzerose, kann ins Plattenepithelkarzinom √ºbergehen\nLokal begrenzt, √úbergang zu invasiv m√∂glich\nSehr h√§ufig, v.‚ÄØa. bei starker UV-Exposition\n\n\nMalignes Melanom\nMelanozyten\nHohe Aggressivit√§t, unregelm√§√üige Pigmentierung, schnelles Wachstum\nFr√ºhzeitige Metastasierung\nSeltener, aber f√ºr die meisten Hautkrebs-Todesf√§lle verantwortlich",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#medizinische-definition",
    "href": "kapitel/grundlagen.html#medizinische-definition",
    "title": "Grundlagen",
    "section": "",
    "text": "Hautkrebs bezeichnet allgemein die maligne1 Neubildung bestimmter Hautzellen, die unkontrolliert wachsen und in umliegendes Gewebe eindringen k√∂nnen. Die h√§ufigsten Formen entstehen aus Zellen der Epidermis, insbesondere aus Keratinozyten und Melanozyten. Medizinisch wird Hautkrebs in ‚Äûhellen Hautkrebs‚Äú (Nicht-Melanom-Hautkrebs, NMSC), der vorwiegend aus Keratinozyten entsteht und ‚Äûschwarzen Hautkrebs‚Äú (malignes Melanom), der aus Melanozyten hervorgeht, unterschieden. [1]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#unterscheidung-schwarzer-vs.-heller-hautkrebs",
    "href": "kapitel/grundlagen.html#unterscheidung-schwarzer-vs.-heller-hautkrebs",
    "title": "Grundlagen",
    "section": "",
    "text": "Heller Hautkrebs umfasst haupts√§chlich das Basalzellkarzinom (Basaliom) und das Plattenepithelkarzinom (Spinaliom) sowie deren Vorstufen, z.‚ÄØB. aktinische Keratosen. Diese Tumore neigen meist zu lokal invasivem Wachstum, metastasieren aber selten.\nBeispiel: Ein Basalzellkarzinom w√§chst oft tief in die Hautschichten und kann sogar Knorpel oder Knochen angreifen, bleibt aber fast immer auf diese Region begrenzt und streut nicht in Lunge oder Leber.\nDer ‚Äûschwarze Hautkrebs‚Äú, das maligne Melanom, geht von den pigmentbildenden Melanozyten aus und gilt als besonders aggressiv, da er fr√ºh metastasieren kann.\nBeispiel: Ein malignes Melanom kann bereits in einem fr√ºhen Stadium Krebszellen √ºber Blut- oder Lymphbahnen streuen und so Metastasen in anderen Organen wie Lunge, Leber oder Gehirn bilden, selbst wenn der Prim√§rtumor an der Haut noch relativ klein erscheint. [2]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#tabellarische-√ºbersicht-der-wichtigsten-hautkrebsarten",
    "href": "kapitel/grundlagen.html#tabellarische-√ºbersicht-der-wichtigsten-hautkrebsarten",
    "title": "Grundlagen",
    "section": "",
    "text": "Tabelle¬†1: Hautkrebsarten\n\n\n\n\n\n\n\n\n\n\n\n\nHautkrebsart\nUrsprung\nCharakteristika\nMetastasierung\nH√§ufigkeit\n\n\n\n\nBasalzellkarzinom (Basaliom)\nBasalzellen der Epidermis\nLangsames Wachstum, selten metastasierend\nSehr selten\nH√§ufigster Hautkrebs weltweit\n\n\nAktinische Keratosen & intraepitheliales Karzinom (Morbus Bowen)\nKeratinozyten der Epidermis\nFr√ºhform/Pr√§kanzerose, kann ins Plattenepithelkarzinom √ºbergehen\nLokal begrenzt, √úbergang zu invasiv m√∂glich\nSehr h√§ufig, v.‚ÄØa. bei starker UV-Exposition\n\n\nMalignes Melanom\nMelanozyten\nHohe Aggressivit√§t, unregelm√§√üige Pigmentierung, schnelles Wachstum\nFr√ºhzeitige Metastasierung\nSeltener, aber f√ºr die meisten Hautkrebs-Todesf√§lle verantwortlich",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#invasive-und-nicht-invasive-diagnoseverfahren",
    "href": "kapitel/grundlagen.html#invasive-und-nicht-invasive-diagnoseverfahren",
    "title": "Grundlagen",
    "section": "Invasive und nicht-invasive Diagnoseverfahren",
    "text": "Invasive und nicht-invasive Diagnoseverfahren\nDie Diagnose von Hautkrebs basiert auf einer Kombination aus nicht-invasiven und invasiven Verfahren. Zu den nicht-invasiven Methoden z√§hlt die visuelle Inspektion mit dem blo√üen Auge, die durch den Einsatz der Dermatoskopie (Auflichtmikroskopie) erg√§nzt wird. Die Dermatoskopie erm√∂glicht eine vergr√∂√üerte, strukturierte Ansicht von Pigmentmustern und Gef√§√üstrukturen, die unter der Hautoberfl√§che liegen. [3]\nWeitere nicht-invasive Verfahren sind die konfokale Laserscanmikroskopie und optische Koh√§renztomographie, die jedoch prim√§r in spezialisierten Zentren Anwendung finden. [4]\nBei einem begr√ºndeten Verdacht auf Malignit√§t kommt die Biopsie als invasives Diagnoseverfahren zum Einsatz. Dabei wird eine Gewebeprobe entnommen, histopathologisch untersucht und definitive Aussagen zur Tumorart, Eindringtiefe und m√∂glichen Metastasierung getroffen. Die histologische Untersuchung gilt weiterhin als Standard der Hautkrebsdiagnostik. [5]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#kriterien-der-dermatoskopie",
    "href": "kapitel/grundlagen.html#kriterien-der-dermatoskopie",
    "title": "Grundlagen",
    "section": "Kriterien der Dermatoskopie",
    "text": "Kriterien der Dermatoskopie\nDie Dermatoskopie basiert auf standardisierten Klassifikationssystemen, um maligne von benignen Hautl√§sionen zu unterscheiden. Eines der bekanntesten Kriterienmodelle ist die ‚ÄûABCD-Regel‚Äú, die asymmetrische Form, unregelm√§√üige Begrenzung, Farbvariationen (Colorit) und Durchmesser ber√ºcksichtigt. Zus√§tzlich kommen Mustererkennungssysteme wie das 7-Punkte-Checklist-System oder das Menzies-Diagnoseschema zum Einsatz. Diese Systeme bewerten mikroskopische Strukturen wie Pigmentnetz, globul√§re Muster, Pseudopodien und Gef√§√üanomalien. [6] [7]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#bildgebende-diagnostik",
    "href": "kapitel/grundlagen.html#bildgebende-diagnostik",
    "title": "Grundlagen",
    "section": "Bildgebende Diagnostik",
    "text": "Bildgebende Diagnostik\nEin Schwerpunkt liegt in der automatisierten Analyse medizinischer Bilddaten, etwa in der Radiologie, Pathologie oder Dermatologie. KI-Modelle, insbesondere Convolutional Neural Networks (CNNs), erreichen in der Diagnose von Brustkrebs oder Lungenkarzinomen teilweise eine Genauigkeit, die mit der von Fach√§rzten vergleichbar ist. [9]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#vorhersage-und-fr√ºhwarnsysteme",
    "href": "kapitel/grundlagen.html#vorhersage-und-fr√ºhwarnsysteme",
    "title": "Grundlagen",
    "section": "Vorhersage und Fr√ºhwarnsysteme",
    "text": "Vorhersage und Fr√ºhwarnsysteme\nKI wird auch genutzt, um Krankheitsverl√§ufe vorherzusagen, beispielsweise bei chronischen Erkrankungen oder Sepsis. Prognosemodelle analysieren elektronische Patientendaten in Echtzeit und unterst√ºtzen √Ñrztinnen und √Ñrzte bei der Fr√ºherkennung kritischer Zust√§nde. [10]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#strukturvorhersage-von-proteinen",
    "href": "kapitel/grundlagen.html#strukturvorhersage-von-proteinen",
    "title": "Grundlagen",
    "section": "Strukturvorhersage von Proteinen",
    "text": "Strukturvorhersage von Proteinen\nEin Meilenstein ist die Anwendung von KI in der Molekularbiologie. DeepMind entwickelte mit AlphaFold ein System, das die dreidimensionale Struktur von Proteinen mit hoher Genauigkeit vorhersagen kann, eine Aufgabe, die jahrzehntelang als eines der gr√∂√üten ungel√∂sten Probleme der Biologie galt. Die M√∂glichkeit, Proteinstrukturen pr√§zise zu bestimmen, beschleunigt die Wirkstoffforschung erheblich. [11]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#virtuelle-assistenten-und-robotik",
    "href": "kapitel/grundlagen.html#virtuelle-assistenten-und-robotik",
    "title": "Grundlagen",
    "section": "Virtuelle Assistenten und Robotik",
    "text": "Virtuelle Assistenten und Robotik\nDar√ºber hinaus kommen KI-Systeme als virtuelle Assistenten zum Einsatz, die Patient:innen bei der Terminplanung oder Medikamentenerinnerung unterst√ºtzen. Auch Roboter mit KI-basierten Steuerungen helfen im OP, indem sie pr√§zise Schnitte durchf√ºhren oder √Ñrztinnen und √Ñrzte bei minimalinvasiven Eingriffen entlasten. [12]",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html#footnotes",
    "href": "kapitel/grundlagen.html#footnotes",
    "title": "Grundlagen",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\nMaligne = b√∂sartig‚Ü©Ô∏é",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/diskussion.html",
    "href": "kapitel/diskussion.html",
    "title": "Diskussion der Ergebnisse",
    "section": "",
    "text": "1 Interpretation dert Forschungsergebnisse\n\n\n2 Vergleich mit anderen Modellen\n\n\n3 Ausblick und m√∂gliche Verbesserungen",
    "crumbs": [
      "Diskussion der Ergebnisse"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projekt√ºbersicht",
    "section": "",
    "text": "Projekt√ºbersicht\nWillkommen zum Melanoma Hautkrebs Diagnose Projekt, das im Rahmen des Moduls Advanced Topics in Computer Science an der Fachhochschule der Wirtschaft durchgef√ºhrt wurde. Ziel dieses Projekts ist es, zu untersuchen, wie gut ein selbst entwickeltes Modell zur Klassifikation von Hautl√§sionen gegen andere bestehende Modelle, wie Large Language Models (LLMs) und andere g√§ngige Klassifikationsmethoden, abschneidet.\nIm Rahmen der Arbeit wird nicht nur die Erkennung von Melanomen (Hautkrebs) angestrebt, sondern es sollen vielmehr verschiedene Diagnosen anhand der vorliegenden Hautl√§sionen gestellt werden.\n\nDer Begriff k√ºnstliche Intelligenz hat aktuell keine einheitliche, allgemein akzeptierte Definition, was von f√ºhrenden Forschern wie Geoffrey Hinton, Yoshua Bengio und Yann LeCun, betont wird. Gemeinsam ist den verschiedenen Definitionen jedoch der Gedanke, Maschinen mit der F√§higkeit auszustatten, Aufgaben zu l√∂sen, die typischerweise menschliche Intelligenz erfordern. Dazu z√§hlen Lernen aus Daten (Machine Learning), Mustererkennung, Sprachverarbeitung und Entscheidungsfindung. Hinton, Bengio und LeCun betonen dabei insbesondere die Bedeutung von Deep Learning, einem Teilbereich des maschinellen Lernens, der auf k√ºnstlichen neuronalen Netzen basiert und gro√üe Datenmengen nutzt, um komplexe Muster zu erkennen.\n\nDie Struktur der Arbeit folgt dem Verlauf des Projekts: Zu Beginn werden die dermatologischen Grundlagen behandelt, um ein Verst√§ndnis f√ºr die Arten von Hautl√§sionen zu entwickeln, die f√ºr die Diagnose von Hautkrebs relevant sind. Es folgt eine Exploration der Metadaten, die den Datensatz beschreibt, der f√ºr das Modell verwendet wird, und die verwendeten Bilddaten erl√§utert. Der zentrale Abschnitt befasst sich mit der Entwicklung und dem Training des Modells, das auf Basis dieser Daten erstellt wurde. Abschlie√üend wird die Diskussion der Ergebnisse durchgef√ºhrt, um die Leistungsf√§higkeit des entwickelten Modells mit bestehenden Technologien zu vergleichen.\n\n\n\nAutoren\nDieses Projekt wurde von folgenden Studierenden der Fachhochschule der Wirtschaft durchgef√ºhrt:\n\nSergei Wendlang ‚Äì Wirtschaftsinformatik, Schwerpunkt Data Science\nJan Henrik Uemann ‚Äì Wirtschaftsinformatik, Schwerpunkt Data Science\nMarc P√∂ppelbaum ‚Äì Wirtschaftsinformatik, Schwerpunkt Business Process Management\n\n\n\n\nWeitere Informationen\nF√ºr dieses Projekt wurde der Datensatz von ISIC (International Skin Imaging Collaboration) verwendet. Der Datensatz ist unter folgendem Link zug√§nglich:\nHAM10000.\nDas gesamte Projekt ist auf Render gehostet, und die Webseite wurde mit Quarto erstellt."
  },
  {
    "objectID": "kapitel/datenexploration.html",
    "href": "kapitel/datenexploration.html",
    "title": "Datenexploration",
    "section": "",
    "text": "HAM10000 ist ein eine Sammlung von 10.015 dermatoskopischen Bildern, welche aus mehreren verschiedenen Quellen zu einem Datensatz im Harvard Dataverse zusammengetragen wurden.\n\n\nCode\nmetadata_df\n\n\n\n\nTabelle¬†1: Aufbau des Datensatzes\n\n\n\n\n\n\n\n\n\n\nlesion_id\nimage_id\ndx\ndx_type\nage\nsex\nlocalization\nage_group\n\n\n\n\n0\nHAM_0000118\nISIC_0027419\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n1\nHAM_0000118\nISIC_0025030\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n2\nHAM_0002730\nISIC_0026769\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n3\nHAM_0002730\nISIC_0025661\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n4\nHAM_0001466\nISIC_0031633\nbkl\nhisto\n75.0\nmale\near\n61-80\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10010\nHAM_0002867\nISIC_0033084\nakiec\nhisto\n40.0\nmale\nabdomen\n21-40\n\n\n10011\nHAM_0002867\nISIC_0033550\nakiec\nhisto\n40.0\nmale\nabdomen\n21-40\n\n\n10012\nHAM_0002867\nISIC_0033536\nakiec\nhisto\n40.0\nmale\nabdomen\n21-40\n\n\n10013\nHAM_0000239\nISIC_0032854\nakiec\nhisto\n80.0\nmale\nface\n61-80\n\n\n10014\nHAM_0003521\nISIC_0032258\nmel\nhisto\n70.0\nfemale\nback\n61-80\n\n\n\n\n10015 rows √ó 8 columns\n\n\n\n\n\n\nIn der oben abgebildeten Tabelle wird ein Ausschnitt des Datensatzes exemplarisch Abgebildet. Er besteht aus 10.015 Zeilen mit sieben Spalten. Entsprechend werden in dem Datensatz sieben unterschiedliche Attribute verwendet, um die dermatologischen Bilder zu identifizieren und klassifizieren. Die Attribute, welche im folgendenen n√§her erl√§utert werden sind lesion_id, image_id, dx, dx_type, age, sex und localization.\nlesion_id dient zur Identifikation der jeweils betrachtenden Hautl√§sionen. Insgesamt werden in diesem Datensatz 7.470 verschiedene Hautl√§sionen betrachtet, um √ºber entsprechende lesion_id identifiziert. Die Differenz von 2.545 Eintr√§gen entsteht durch die mehrfache Betrachtung von einzelnen Hautl√§sionen.\nDie image_id, wie die lesion_id, wird ebenfalls zur eindeutigen Identifikation verwendet. Im diesem Fall werden die dermatoskopischen Bilder identifiziert.\ndx gibt an, womit die abgebildeten Hautl√§sionen diagnostiziert wurden. Die im Datensatz verwendeten K√ºrzel werden in folgender Tabelle genauer erkl√§rt. Insgesamt werden kommen in diesem Datensatz sieben verschiedene Diagnosen vor.\n\n\n\nTabelle¬†2: Diagnosekategorien\n\n\n\n\n\n\n\n\n\nK√ºrzel\nDiagnosekategorie\n\n\n\n\nakiec\nAktinische Keratosen und intraepitheliales Karzinom / Bowen-Krankheit\n\n\nbcc\nBasalzellkarzinom\n\n\nbkl\nKeratosen-√§hnliche L√§sionen (Sonnenlentigines / seborrhoische Keratosen und lichen-planus-√§hnliche Keratosen)\n\n\ndf\nDermatofibrom\n\n\nnv\nMelanozytische N√§vi (Muttermal)\n\n\nmel\nMelanom\n\n\nvasc\nGef√§√ül√§sionen (Angiome, Angiokeratome, pyogene Granulome und Blutungen)\n\n\n\n\n\n\ndx_type definiert, wie die jeweilige Diagnose erreicht beziehungsweise welche Diagnosemethode verwendet wurde.\n\n\n\nTabelle¬†3: Diagnosemethoden\n\n\n\n\n\nK√ºrzel\nDiagnosemethode\n\n\n\n\nconfocal\nHistopathologie\n\n\nconsensus\nExpertenkonsens\n\n\nfollow_up\nNachuntersuchungen\n\n\nhisto\nHistopathologie\n\n\n\n\n\n\nage und sex beschreiben jeweils das Alter und das geschlecht der Personen, dessen Bilder und diagnosen in diesem Datensatz gesammelt wurden. Diese beiden Attribute stellen gleichzeitig die einzigen personenbezogenen Daten dar, welche im HAM10000 zu verf√ºgung stehen.\nlocalization gibt an, an welcher Stelle des K√∂rpers die entsprechende Hautl√§sion lokalisiert wurde. In der folgenden Tabelle werden die im Datensatz notierten K√∂rperstellen aufgelistet und √ºbersetzt.\n\n\n\nTabelle¬†4: K√∂rperstellen\n\n\n\n\n\nBezeichnung in Datensatz\n√úbersetzung\n\n\n\n\nscalp\nKopfhaut\n\n\nface\nGesicht\n\n\near\nOhr\n\n\nneck\nHals\n\n\nchest\nBrust\n\n\nback\nR√ºcken\n\n\ntrunk\nRumpf\n\n\nabdomen\nBauch\n\n\nupper extremity\nObere Extremit√§t\n\n\nlower extremity\nUntere Extremit√§t\n\n\nhand\nHand\n\n\nfoot\nFu√ü\n\n\narm\nArm\n\n\nleg\nBein\n\n\nacral\nAkral (Finger, Zehen etc.)",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#diagnoseverteilung-der-l√§sionen-nach-geschlecht",
    "href": "kapitel/datenexploration.html#diagnoseverteilung-der-l√§sionen-nach-geschlecht",
    "title": "Datenexploration",
    "section": "Diagnoseverteilung der L√§sionen nach Geschlecht",
    "text": "Diagnoseverteilung der L√§sionen nach Geschlecht\nDie Definitionen der K√ºrzel, welche eine Diagnose repr√§sentieren sind in Tabelle¬†2 zusammengefasst. Wie in Abbildung¬†4 zu erkennen ist, dominiert die Diagnose nv den Datensatz. Diese repr√§sentiert 66.95% der Diagnosen. Das Verh√§ltnis der Geschlechter male zu female betr√§gt 5406 / 4552. Demnach sind m√§nlichen Probanten im Datensatz leicht √ºberrepresentiert. Die gef√§hrlichen Diagnosen machen 19.51% des Datensatzes aus und das Verh√§ltnis der Geschlechter male zu female betr√§gt unter diesen 1227 / 727. Daraus folgt, dass die m√§nnlichen Probanten in der Gruppe von Probanten mit gef√§hrlichen Diagnosen h√∂her representiert ist, als im gesamten Datensatz.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='dx',\n    hue='sex',\n    multiple='stack',\n    palette=GENDER_PALETTE,\n    edgecolor='white'\n)\nplt.xlabel('Diagnose')\nplt.ylabel('Anzahl L√§sionen')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†4: Verteilung der Diagnosen nach Geschlecht",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-des-alters-der-probanten-nach-geschlecht",
    "href": "kapitel/datenexploration.html#verteilung-des-alters-der-probanten-nach-geschlecht",
    "title": "Datenexploration",
    "section": "Verteilung des Alters der Probanten nach Geschlecht",
    "text": "Verteilung des Alters der Probanten nach Geschlecht\nAnhand von Abbildung¬†5 l√§sst sich erkennen, dass das mittlere Alter der Probanten im gesamten Datensatz 51.86 Jahre betr√§gt (16.97 Jahre Standardabweichung). In der Gruppe der Probanten mit gef√§hrlichen Diagnosen betr√§gt das mittlere Alter 63.28 (14.53 Jahre Standardabweichung). Eine Visualisierung dieses Zusammenhangs findet sich in Abbildung¬†14.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='age',\n    hue='sex',\n    multiple='stack',\n    bins=10,\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    ax=ax[0]\n)\nax[0].set_title('Altersverteilung')\nax[0].set_ylabel('Anzahl L√§sionen')\nax[0].set_xlabel('Alter')\nsns.histplot(\n    data=metadata_dangerous_df,\n    x='age',\n    hue='sex',\n    multiple='stack',\n    bins=10,\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    legend=False,\n    ax=ax[1]\n)\nax[1].set_title('Altersverteilung (gef√§hrl. Diagnosen)')\nax[1].set_ylabel('')\nax[1].set_xlabel('Alter')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†5: Verteilung des Alters nach Geschlecht",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-diagnosemethoden-nach-geschlecht",
    "href": "kapitel/datenexploration.html#verteilung-der-diagnosemethoden-nach-geschlecht",
    "title": "Datenexploration",
    "section": "Verteilung der Diagnosemethoden nach Geschlecht",
    "text": "Verteilung der Diagnosemethoden nach Geschlecht\nDie Definition der K√ºrzel, welche Diagnosemethoden representieren ist in Tabelle¬†3 zusammengefasst. Abbildung¬†6 zeigt, dass die Diagnosemethode histo den Datensatz mit einem Anteil von 53.32% dominiert. Zudem werden gef√§hrliche Diagnosen ausschlie√ülich √ºber die Diagnosemethode histo diagnostiziert.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='dx_type',\n    hue='sex',\n    multiple='stack',\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    ax=ax[0]\n)\nax[0].set_title('Diagnosemethoden')\nax[0].set_ylabel('Anzahl L√§sionen')\nax[0].set_xlabel('Diagnosemethode')\nsns.histplot(\n    data=metadata_dangerous_df,\n    x='dx_type',\n    hue='sex',\n    multiple='stack',\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    legend=False,\n    ax=ax[1]\n)\nax[1].set_title('Diagnosemethoden (gef√§hrl. Diagnosen)')\nax[1].set_ylabel('')\nax[1].set_xlabel('Diagnosemethode')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†6: Verteilung der Diagnosemethoden nach Geschlecht",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-k√∂rperstellen-nach-alter",
    "href": "kapitel/datenexploration.html#verteilung-der-k√∂rperstellen-nach-alter",
    "title": "Datenexploration",
    "section": "Verteilung der K√∂rperstellen nach Alter",
    "text": "Verteilung der K√∂rperstellen nach Alter\nEine √úbersicht der im Datensatz aufgef√ºhrten K√∂rperstellen befindet sich in Tabelle¬†4. Anhand von Abbildung¬†7 l√§sst sich erkennen, dass in der Altersgruppe mit den meisten L√§sionen (41-60 Jahre) h√§ufig die K√∂rperstellen lower extremity, back, trunk, abdomen und upper extrmity betroffen sind.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_data = pd.crosstab(metadata_df['localization'], metadata_df['age_group'])\nsns.heatmap(\n    data=heatmap_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('K√∂rperstelle / Alter')\nplt.xlabel('Altersgruppe')\nplt.ylabel('K√∂rperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†7: Verteilung der K√∂rperstellen nach Atler\n\n\n\n\n\nNach Reduktion der Metadaten auf Datens√§tze mit gef√§hrlichen Diagnosen (siehe Abbildung¬†8) ist die neue dominierende Altersgruppe, wie Abbildung¬†5 und Abbildung¬†14 andeuten lassen, 61-80 Jahre. Die betroffenen K√∂rperstellen unter den 61-80 J√§hrigen mit gef√§hrlichen Diagnosen sind demnach back, face, upper extremity und lower extremity.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_dangerous_data = pd.crosstab(metadata_dangerous_df['localization'], metadata_dangerous_df['age_group'])\nsns.heatmap(\n    data=heatmap_dangerous_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('K√∂rperstelle / Alter (gef√§hrl. Diagnosen)')\nplt.xlabel('Altersgruppe')\nplt.ylabel('K√∂rperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†8: Verteilung der K√∂rperstellen nach Atler (gef√§hrl. Diagnosen)",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-k√∂rperstellen-nach-diagnose",
    "href": "kapitel/datenexploration.html#verteilung-der-k√∂rperstellen-nach-diagnose",
    "title": "Datenexploration",
    "section": "Verteilung der K√∂rperstellen nach Diagnose",
    "text": "Verteilung der K√∂rperstellen nach Diagnose\nAnhand von Abbildung¬†9 l√§sst sich erkennen, dass der Datensatz bei einer Betrachtung nach K√∂rperstelle Tabelle¬†4 und Diagnosen Tabelle¬†2 √ºber 3 Extrema verf√ºgt, welche 41.41% des Datensatzes repr√§sentieren. Es handelt sich um die Diagnose nv and den K√∂rperstellen back, lower extremity und trunk.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_data = pd.crosstab(metadata_df['localization'], metadata_df['dx'])\nsns.heatmap(\n    data=heatmap_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('K√∂rperstelle / Diagnose')\nplt.xlabel('Diagnose')\nplt.ylabel('K√∂rperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†9: Verteilung der K√∂rperstellen nach Diagnose\n\n\n\n\n\nReduziert auf die gef√§hrlichen F√§lle ist die h√§ufigste Diagnose mel (siehe Abbildung¬†11). Diese tritt dominant an den K√∂rperstellen back, upper extremity und lower extremity auf.\n\nHeatmap K√∂rperstellenAnteil Diagnosen\n\n\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_dangerous_data = pd.crosstab(metadata_dangerous_df['localization'], metadata_dangerous_df['dx'])\nsns.heatmap(\n    data=heatmap_dangerous_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('K√∂rperstelle / Diagnose (gef√§hrl. Diagnosen)')\nplt.xlabel('Diagnose')\nplt.ylabel('K√∂rperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†10: Verteilung der K√∂rperstellen nach Diagnose (gef√§hrl. Diagnosen)\n\n\n\n\n\n\n\n\n\nCode\ndangerous_dx_counts = metadata_dangerous_df['dx'].value_counts()\ntotal_dangerous = dangerous_dx_counts.sum()\n\nlabels = [f\"{dx} ({count}/{total_dangerous})\" for dx, count in dangerous_dx_counts.items()]\n\nplt.figure(figsize=STANDARD_FIGSIZE)\nplt.pie(\n    dangerous_dx_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=sns.color_palette('Reds', n_colors=len(dangerous_dx_counts))\n)\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†11: Anteil gef√§hrlicher Diagnosen unter allen gef√§hrlichen F√§llen\n\n\n\n\n\n\n\n\nBetrachtet man jedoch alle F√§lle gruppiert nach K√∂rperstelle, wie in Tabelle¬†5, so lassen sich an der K√∂rperstelle face am h√§ufigsten gef√§hrliche Diagnosen verorten.\n\n\nCode\nmetadata_df['is_dangerous'] = metadata_df['dx'].isin(dangerous_dx)\ndangerous_share_per_localization = (\n    pd.DataFrame({\n        'dangerous_share_percent': (\n            metadata_df\n            .groupby('localization')['is_dangerous']\n            .mean()\n            .sort_values(ascending=False) * 100\n        )\n    })\n    .round(2)\n    .reset_index()\n)\n\ndangerous_share_per_localization\n\n\n\n\nTabelle¬†5: Ranking der K√∂rperstellen mit den meisten gef√§hrlichen Diagnosen\n\n\n\n\n\n\n\n\n\n\nlocalization\ndangerous_share_percent\n\n\n\n\n0\nface\n42.68\n\n\n1\nscalp\n36.72\n\n\n2\near\n35.71\n\n\n3\nneck\n31.55\n\n\n4\nchest\n31.20\n\n\n5\nupper extremity\n28.98\n\n\n6\nback\n24.59\n\n\n7\nhand\n17.78\n\n\n8\nlower extremity\n15.17\n\n\n9\nfoot\n10.03\n\n\n10\nabdomen\n8.71\n\n\n11\nunknown\n6.41\n\n\n12\ntrunk\n4.20\n\n\n13\nacral\n0.00\n\n\n14\ngenital\n0.00",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-diagnosen-nach-alter",
    "href": "kapitel/datenexploration.html#verteilung-der-diagnosen-nach-alter",
    "title": "Datenexploration",
    "section": "Verteilung der Diagnosen nach Alter",
    "text": "Verteilung der Diagnosen nach Alter\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_data = pd.crosstab(metadata_df['dx'], metadata_df['age_group'])\nsns.heatmap(\n    data=heatmap_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Diagnose / Alter')\nplt.xlabel('Altersgruppe')\nplt.ylabel('Diagnose')\nplt.yticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_dangerous_data = pd.crosstab(metadata_dangerous_df['dx'], metadata_df['age_group'])\nsns.heatmap(\n    data=heatmap_dangerous_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Diagnose / Alter (gef√§hrl. Diagnosen)')\nplt.xlabel('Altersgruppe')\nplt.ylabel('Diagnose')\nplt.yticks(rotation=0)\nplt.show()",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#beispielfotos-anhand-h√§ufig-auftretender-attribute",
    "href": "kapitel/datenexploration.html#beispielfotos-anhand-h√§ufig-auftretender-attribute",
    "title": "Datenexploration",
    "section": "Beispielfotos anhand h√§ufig auftretender Attribute",
    "text": "Beispielfotos anhand h√§ufig auftretender Attribute\n\n\nCode\nall_images = os.listdir('../images')\nisic_images = [file_name for file_name in all_images if file_name.startswith('ISIC')]\ncolumns = 2\nrows = (len(isic_images) + columns - 1) // columns\nfig, ax = plt.subplots(rows, columns, figsize=(8, 4*rows))\nax = ax.flatten()\nfor index, image_name in enumerate(isic_images):\n    image_path = os.path.join('../images', image_name)\n    image = Image.open(image_path)\n    metadata = metadata_df[metadata_df['image_id'] == image_name.removesuffix('.jpg')]\n    ax[index].imshow(image)\n    ax[index].set_title(f'{metadata['dx'].iloc[0]} - {metadata['age'].iloc[0]:.0f} years - {metadata['localization'].iloc[0]}')\nplt.show()",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#sec-korrelation",
    "href": "kapitel/datenexploration.html#sec-korrelation",
    "title": "Datenexploration",
    "section": "Korrelationsanalyse nach Spearman",
    "text": "Korrelationsanalyse nach Spearman\nEin Weg um diesen Zusammenhang zu untersuchen ist der Korrelationskoeffizient nach Spearman [2].\nDie Wahl des Spearman-Korrelationskoeffizienten rechtfertigt sich durch die folgenden Eingeschaften:\n\nEr setzt keine Normalverteilung voraus\nEr misst den monotonen Zusammenhang zwischen zwei Merkmalen\nEr ist robust gegen√ºber Ausrei√üern, da auf Rangwerte zur√ºckgegriffen wird\nEr eignet sich f√ºr ordinal skalierte oder kategorisiert kodierte Merkmale, wie sie im Datensatz vorliegen\n\nSeien \\(X=(x_1, x_2, ...,x_n)\\) und \\(Y=(y_1, y_2, ..., y_n)\\) zwei Merkmalsreihen mit \\(n\\) Beobachtungen. Dann ist der Spearman-Korrelationskoeffizient \\(\\rho_s\\) definiert als Pearson-Korrelationskoeffizient der Rangwerte \\(R(x_i)\\) und \\(R(y_i)\\) mit \\(d_i = R(x_i) - R(y_i)\\):\n\\[\n\\rho_s = 1 - \\frac{6 \\sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}\n\\]\nAngewendet auf den in Kapitel¬†1 beschriebenen Datensatz ergibt sich die in Abbildung¬†12 dargestellt Korrelationsmatrix. Es l√§sst sich ein Zusammenhang zwischen dx und age sowie dx_type und dx erkennen.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nsns.heatmap(\n    data=corr_matrix,\n    annot=True,\n    cmap='coolwarm',\n    fmt='.2f'\n)\nplt.yticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†12: Korrelationsmatrix des Datensatzes\n\n\n\n\n\nDas der Zusammenhang der Variablen im gesamten Datensatz nicht besonders signifikant ist (mit Ausnahme des Alters) l√§sst sich ebenfalls visuell in Abbildung¬†13 erkennen. Somit l√§sst sich aus den Ergebnissen von Kapitel¬†2 und Kapitel¬†3.1 schlussfolgern, dass das Risiko f√ºr gef√§hrliche Diagnosen mit dem Alter zunimmt. Dies wird ebenfalls in Abbildung¬†14 visualisiert. Zudem l√§sst sich anhand von Abbildung¬†10 erkennen, dass es wahrscheinlicher ist, eine gef√§hrliche Diagnose im Bereich des R√ºckens, Gesichts oder der oberen Extremit√§t zu erhalten. Diese Verteilung kann aber zum Teil auf die Anzahl der Muttermale an den jeweiligen K√∂rperstellen zur√ºckgef√ºhrt werden (siehe dazu Abbildung¬†7). Einen interessanten Zusammenhang, beschreibt die Anzahl der gef√§hrlichen Diagnosen an einer K√∂rperstelle in Relation zu den gesamten Diagnosen an der jeweiligen Stelle (siehe Tabelle¬†5). Auff√§llig ist, dass unbedeckte K√∂rperstellen, welche viel Sonneneinstrahlung abbekommen tendenziell gef√§hrdeter sind als bspw. der Genitalbereich.\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\ngrid = sns.PairGrid(metadata_df[predictor_columns], diag_sharey=False)\ngrid.map_upper(sns.scatterplot, s=15)\ngrid.map_lower(sns.kdeplot)\ngrid.map_diag(sns.kdeplot, lw=2)\nplt.show()\n\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†13: : Paar Gatter der Pr√§diktor Attribute",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#anzahl-der-gef√§hrlichen-diagnosen-nach-alter",
    "href": "kapitel/datenexploration.html#anzahl-der-gef√§hrlichen-diagnosen-nach-alter",
    "title": "Datenexploration",
    "section": "Anzahl der gef√§hrlichen Diagnosen nach Alter",
    "text": "Anzahl der gef√§hrlichen Diagnosen nach Alter\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='age_group',\n    hue='is_dangerous',\n    palette=DANGEROUS_PALETTE,\n    multiple='stack',\n    edgecolor='white'\n)\nplt.xlabel('Altersgruppe')\nplt.ylabel('Anzahl L√§sionen')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†14: Anzahl der gef√§hrlichen Diagnosen nach Alter",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/einfuehrung.html",
    "href": "kapitel/einfuehrung.html",
    "title": "Einleitung",
    "section": "",
    "text": "Motivation\nK√ºnstliche Intelligenz. Ein Begriff, welcher scheinbar immer h√§ufiger in unseren Alltag, aber auch in weiten Bereichen der Wirtschaft Einzug hat [1, S. 4‚Äì10]. Dementsprechend wird es ebenfalls immer wichtiger zu verstehen, was k√ºnstliche Intellignez (KI) eigentlich ist. Hierf√ºr wird diese Arbeit im Rahmen des Studienmoduls ‚ÄòAdvanced Topics in Computer Science‚Äô ausgearbeitet.\nDas Ziel hierbei ist es, das allgemeine Verst√§ndnis des Themenfeldes ‚ÄòK√ºnstliche Intelligenz‚Äô durch n√§here Betrachtung eines spezifischen Themas, zu steigern beziehungsweise zu verbesseren. Zu diesem Zweck besch√§ftigt sich diese wissenschaftliche Arbeit mit dem Aufbau und Analyse eines KI-Modells, welches durch die Analyse von Bildern eine Hautkrebsdiagnose durchf√ºhren soll.\nDieses Thema wurde ausgew√§hlt, nicht nur, weil es, wie im folgenden Abschnitt¬†2 n√§her erl√§utert, ein relevantes Thema ist, sondern auch, weil es eine Reihe von Einblicken gew√§hrt, wie KIs Bilder analysieren und interpretieren k√∂nnen und wie dies sowohl im Alltag als auch in der Wirtschaft zum Einsatz kommen k√∂nnte. Des Weiteren werden auch generelle Erkenntnisse zum Aufsetzen und Trainieren von KI-Modellen gewonnen.\n\n\nRelevanz des Themas\nIm Bereich des Gesundheitsheitwesens weitet sich der Einsatz und Nutzen von K√ºnstlicher Intelligenz ebenfalls immer weiter aus [1, S. 29ff]. Der Einsatz umfasst dabei unter anderem die Einsatzbereiche der medizinischen Bildgebung beziehungsweise der Diagnostik [2, Abs. 1], welche in dieser Arbeit von besonderem Interesse sind.\nJedes Jahr werden ungef√§hr 123.000 F√§lle von Hautkrebs identifiziert. Dabei ist in dem Bereich der Dermatologie, welcher erste Diagnosen oft durch visuelle Mustererkennung erstellen muss, die Nutzung von KI besonderes hilfreich und vegelichweise gut umsetztbar.[2, Kap. 6.3.5.4]\nMelanoma, die t√∂dlichste Art von Hautkrebs, ist die 19. h√§ufigste auftretetende Krebsart. Gleichzeitg ist aber auch so, dass wenn Melanoma in einem fr√ºhen Stadium entdeckt wird, dass die √úberlebenschance der Betroffenen √ºber 95% liegt.\nJedoch ist eine manuelle Analyse durch den Dermatologen, f√ºr jedes Muttermal oder Hautirretation, mit einem hohen Kosten- und Zeitaufwand verbunden.[3]\n\n\nForschungsgegenstand und Vorgehensweise\nDaher soll, wie zuvor erw√§hnt Kapitel¬†1, in dieser Arbeit ein Modell √ºber einen Machine-Learning Ansatz erstellt und betrachtet werden. Hierf√ºr wird der HAM10000 Datensatz [4] als Trainingsgrundlage verwendet. Dieser wird in der Datenexploration n√§her betrachtet. Ebenfalls werden statistische Metriken und Abh√§ngigkeiten analysiert.\nZun√§chst werden jedoch thematische Grundlagen definiert. In dem Abschnitt werden dann Hautkrebsarten und entsprechende Diagnosewerkzeuge, sowie eine Definition von k√ºnstliche Intelligenz und die zugeh√∂rigen Anwendungenbereiche im medizinischen Sektor.\nNach der Datenexploration werden anschlie√üend die Datenvorbereitung, die Wahl des KI-Modell Ansatz, des Trainingsprozess und Paramenter, sowie die Validierung und die Evaluation protokoliert.\nAbschlie√üend werden dann die Forschungsergebnisse interpretiert, das erstellte Modell mit weiteren bereits vorhandenen Modellen verglichen und ein Ausblick, sowie m√∂gliche Verbesserungen gegeben.\n\n\n\n\n\nLiteratur\n\n[1] Statista, ‚ÄûK√ºnstliche Intelligenz - Hauptanwendungsfelder‚Äú, Statista, 2024. Verf√ºgbar unter: https/de.statista.com/statistik/studie/id/38585/dokument/hauptanwendungsfelder-von-ki/\n\n\n[2] J. Du, M. Huang, und L. Liu, ‚ÄûKI-unterst√ºtzte Krankheitsvorhersage in der visualisierten Medizin‚Äú, in Visualisierung in der Medizin, Z. Liu, Hrsg., Singapore: Springer Nature Singapore, 2025, 6, S. 117‚Äì139. doi: https://doi.org/10.1007/978-981-97-9693-9_6.\n\n\n[3] S. Vashdev Asrani, J. Lalchandani, I. Desai, T. Jeswani, und S. Khedkar, ‚ÄûSkInspection: Recognizing the Type of Skin Cancer Using Deep Learning‚Äú, in Proceedings of the International Health Informatics Conference, S. Jain, B. K. Bhargava, D. Kalra, und S. Groppe, Hrsg., Singapore: Springer Nature Singapore, 2025, 12, S. 163‚Äì174. doi: https://doi.org/10.1007/978-981-97-7190-5_12.\n\n\n[4] P. Tschandl und V. Group, ‚ÄûThe HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions‚Äú. https://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/DBW86T, 2018. doi: 10.7910/DVN/DBW86T.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html",
    "href": "kapitel/modellentwicklung.html",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Da die im Datensatz enthaltenen Metadaten der L√§sionen lediglich Tendenzen (siehe Datenexploration) erkennen lassen und alleinstehend nicht zur eindeutigen Identifikation von Diagnosen verwendet werden k√∂nnen, m√ºssen ebenfalls die Bilder der betroffenen Hautstellen untersucht werden. F√ºr diese Analyse werden unterschiedliche Ans√§tze des maschinellen Lernens untersucht.\n\n\nConvolutional Neural Networks stellen einen Spezialtypen k√ºnstlicher neuronaler Netzwerke dar der besonders f√ºr die Verarbeitung Bilddaten konzipiert wurde. Im Gegensatz zu herk√∂mmlichen neuronalen Netzwerken verwenden CNNs spezielle Operationen, die auf der mathematischen Faltung (Convolution) basieren. [1]\n\n\nBei der Verarbeitung von digitalen Signale oder Bildern k√∂nnen die Eingabeparameter meistens durch diskrete Funktionen abgebildet werden.\n\nDefinition 1 Seien \\(f, g: D \\to \\mathbb{C}\\) mit dem diskreten Definitionsbereich \\(D \\subseteq \\mathbb{Z}\\). Dann ist die diskrete Faltung definiert durch:\n\\[\n(f * g)(n)=\\sum_{k \\in D}{f(k)g(n-k)}\n\\]\n\nFaltungsmatritzen (auch Kernel oder Filter) sind meist quadratische Matrizen ungerader Abmessungen in unterschiedlichen Gr√∂√üen. Einige Bildbearbeitungsoperationen k√∂nnen als lineares System interpretiert werden, wobei eine diskrete Faltung (siehe Definition¬†1) angewendet wird. F√ºr diskrete zweidimensionale Funktionen (digitale Bilder, Signale, etc.) ergibt sich die folgende Berechnungsformel\n\nDefinition 2 Sei \\(I^{\\ast} (x,y)\\) das Ergebnispixel, \\(I\\) das Bild, auf welches der Filter angewendet wird, \\(a\\) die Koordinate des Mittelungspunkts in der quadratischen Faltungsmatrx und \\(k(i, j)\\) ein Element der Faltungsmatrix. Dann ist die Berechnungsformel der diskreten Faltung definiert durch: \\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)k(i, j)}\n\\]\n\nDas in Definition¬†2 dargestellte mathematische Konzept der diskreten Faltung wird auf ein 16x16 Pixel gro√ües Bild angewendet (mit den Werten 0 f√ºr schwarz und 1 f√ºr wei√ü). Um die visuelle Wirkung der Faltungsoperation zu demonstrieren, wird eine spezifische Faltungsmatrix \\(k\\) verwendet.\n\\[\nk = \\frac{1}{9} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n\\]\nDie hier verwendete Faltungsmatrix \\(k\\) ist ein 3√ó3 Mittelwertfilter, bei dem jedes Element den Wert \\(\\frac{1}{9}\\) hat. Dieser Filter f√ºhrt zu einer Gl√§ttung des Bildes (siehe Abbildung¬†1), da er f√ºr jedes Pixel den Durchschnitt der umliegenden 3√ó3 Nachbarschaft berechnet. Im Beispiel wird ein ringf√∂rmiges Muster gegl√§ttet, was die Kanten des Rings weicher erscheinen l√§sst und einen Unsch√§rfeeffekt erzeugt. [2]\n\n\nCode\nimage = np.zeros((16, 16))\nfor i in range(16):\n    for j in range(16):\n        dist = np.sqrt((i-7.5)**2 + (j-7.5)**2)\n        if 3 &lt;= dist &lt;= 6:\n            image[i, j] = 1.0\nkernel = 1/9 * np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\nfiltered_image = signal.convolve2d(image, kernel, mode='same', boundary='symm')\nfig, axes = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\naxes[0].imshow(image, cmap='gray')\naxes[0].set_title('Originalbild (16x16)')\naxes[0].axis('off')\n\naxes[1].imshow(filtered_image, cmap='gray')\naxes[1].set_title('Nach Faltung mit k')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†1: Beispiel einer Faltungsoperation\n\n\n\n\n\n\n\n\nEin CNN besteht aus mehreren Schichten, welche die Grundlage f√ºr die meinsten modernen Bilderkennungmodelle darstellen.\n\n\n\n\n\n\nflowchart TD\n    Input(\"Input Image\") --&gt; Conv\n\n    subgraph Conv[\"Convolutional Layer\"]\n        direction LR\n        C1[\"Feature Extraction\"] --- C2[\"Filter Application\"]\n    end\n\n    Conv --&gt; Pool\n\n    subgraph Pool[\"Pooling Layer\"]\n        direction LR\n        P1[\"Dimension Reduction\"] --- P2[\"Parameter Reduction\"]\n    end\n\n    Pool --&gt; FC\n\n    subgraph FC[\"Fully Connected Layer\"]\n        direction LR\n        F1[\"Feature Classification\"] --- F2[\"Output Prediction\"]\n    end\n\n    FC --&gt; Output(\"Classification Result\")\n\n    style Input fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Output fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Conv fill:#d1ecf1,stroke:#0c5460,stroke-width:1px\n    style Pool fill:#d4edda,stroke:#155724,stroke-width:1px\n    style FC fill:#f8d7da,stroke:#721c24,stroke-width:1px\n\n\n\n\nAbbildung¬†2: CNN Funktionsweise\n\n\n\n\n\nDer in Abbildung¬†2 dargestellte Prozessfluss beschreibt die Grundidee von CNNs. Die Schichten, welche verwendet werden erf√ºllen dabei die folgenden Aufgaben:\n\nConvolutional Layer: Extrahiert Merkmale durch die Anwendung der in Kapitel¬†1.1.1 beschriebenen Filter.\nPooling Layer: Reduziert Dimensionen und erh√∂ht die Berechnungseffizienz.\nFully Connected Layer: Klassifiziert die extrahierten Merkmale.\n\nZur Reduktion der Dimensionen (pooling) k√∂nnen je nach Kontext verschiedene Funktionen angewendet werden. Es ist m√∂glich nach Maxima, Minima und Durchschnittswerten zu reduzieren. Dabei werden, √§hnlich wie bei der Faltung, Matrizen auf einem Wert abgebildet. Diese Pooling-Matrix bewegt sich √ºber die Urspr√ºngliche Matrix. Dabei sind folgende Parameter entscheident:\n\nFilter-Size (Pooling Gr√∂√üe): Definiert die Gr√∂√üe der Pooling Matrix.\nStride: Bestimmt die Schrittgr√∂√üe, mit der sich die Pooling Matrix √ºber die Urspr√ºngliche Matrix bewegt.\n\n\nDefinition 3 Die gr√∂√üe der aus dem Pooling Prozess hervorgehenden reduzierten Matrix berechne sich mit der folgenden Formel. Seien \\(h\\) die H√∂he der Eingabematrix, \\(w\\) die Breite der Eingabematrix, \\(c\\) die Anzahl der Eingabematrizen (Annahme: Alle Eingabematrizen sind gleich gro√ü), \\(f\\) die Gr√∂√üe der Pooling-Matrix und \\(s\\) der Stride.\n\\[\nc \\left(\\left\\lfloor\\frac{h-f+1}{s}\\right\\rfloor + 1\\right) \\left(\\left\\lfloor\\frac{w-f+1}{s}\\right\\rfloor + 1\\right)\n\\]\n\nIm folgenden wird dargestellt, wie die Gr√∂√üe eines quadratischen Bildes mit einem Farbkanal (mit den Werten 0 f√ºr schwarz und 1 f√ºr wei√ü) durch den Pooling Prozess reduziert werden kann. Zur Berechnung der Bildgr√∂√üe nach dem Pooling Prozess wird die Formel aus Definition¬†3 angewendet. [3]\n\n\nCode\npicture_width = np.array([counter for counter in range(16, 500)])\n\ndef picture_size(width):\n    return width**2\n\ndef pooling_size(width, matrix_size, stride):\n    output_dim = (np.floor((width - matrix_size + 1) / stride) + 1)\n    return int(output_dim**2)\n\nfilter_size = 3\nstride = 2\n\noriginal_sizes = [picture_size(w) for w in picture_width]\npooled_sizes = [pooling_size(w, filter_size, stride) for w in picture_width]\n\nplt.figure(figsize=STANDARD_FIGSIZE)\nplt.plot(picture_width, original_sizes, 'b-', label='Originalbild')\nplt.plot(picture_width, pooled_sizes, 'r-', label=f'Nach Pooling (Filter: {filter_size}x{filter_size}, Stride: {stride})')\nplt.xlabel('Breite des Originalbildes (Pixel)')\nplt.ylabel('Anzahl Pixel')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†3: Auswirkung von Pooling auf die Bildgr√∂√üe\n\n\n\n\n\nIn den Abbildungen Abbildung¬†4 und Abbildung¬†5 wird exemplarisch dargestellt, wie sich unterschiedliche Pooling-Methoden auf ein Eingabebild auswirken. Dabei wird jeweils ein Filter der Gr√∂√üe 3√ó3 mit einem Stride von 2 √ºber das Bild bewegt.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\npicture = np.random.randint(0, 31, size=(9, 9), dtype=np.uint8)\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\navg_pooled = F.avg_pool2d(tensor, kernel_size=3, stride=2)\n\nvmin, vmax = 0, 31\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(avg_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Durchschnitts Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†4: Beispiel f√ºr Durchschnitts-Pooling (3x3, Stride 2)\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Verwende das gleiche Bild wie oben\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nmax_pooled = F.max_pool2d(tensor, kernel_size=3, stride=2)\n\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(max_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Maxima Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†5: Beispiel f√ºr Maxima-Pooling (3x3, Stride 2)\n\n\n\n\n\nBeide Methoden reduzieren die Bildgr√∂√üe und damit die Anzahl der zu verarbeitenden Parameter, wobei sie jeweils unterschiedliche Eigenschaften des urspr√ºnglichen Bildes erhalten oder herausfiltern. Die Wahl der Pooling-Methode h√§ngt daher stark vom Einsatzzweck und den gew√ºnschten Modell-Eigenschaften ab. [4]\n\n\n\n\nHerk√∂mmliche CNNs basieren auf einer sequentiellen Abfolge der in Kapitel¬†1.1.2 beschriebenen Operationen. U-Net hingegen ist eine spezielle Variante eines CNN, welche urspr√ºnglich f√ºr biomedizinische Bildsegmentierung konzipiert wurde.\n\n\nDie U-Net Architektur besteht aus einer kontrahierenden Komponente (Encoder-Pfad) und einer expandierenden Komponente (Decoder-Pfad), welche zusammen eine U-Form bilden.\n\n\n\n\n\n\nAbbildung¬†6: U-Net Architektur (aus Ronneberger 2015)\n\n\n\nWie in Abbildung¬†6 zu erkennen ist, wird auf dem Encoder-Pfad zur Feature Extraktion die Aktivierungsfunktion Rectified Linear Units (ReLU) verwendet (siehe Definition¬†4). Folglich besteht der Encoder-Pfad aus der wiederholten Anwendung von Faltungen (siehe Definition¬†2) gefolgt von Aktivierungsfunktionen und Max-Pooling (siehe Abbildung¬†5) zur Dimensionsreduktion. Dabei verdoppelt jeder Downsampling-Schritt die Anzahl der Feature-Kan√§le1.\n\nDefinition 4 Sei \\(f\\) die ReLU Aktivierungsfunktion und \\(x\\) das Eingabepixel.\n\\[\nf(x) = \\max (0, x)\n\\]\n\nDer Decoder-Pfad t√§tigt ein Upsampling der Feature-Map, gefolgt von einer Faltung, welche die Anzahl der Feature-Kan√§le halbiert. Entscheident f√ºr die Leistungsf√§higkeit sind die sogenannten ‚ÄúSkip Connections‚Äù, welche eine direkte Verbindung zwischen den Feature-Maps im Encoder- und Decoder-Pfad symbolisieren. [5]\n\n\n\n\nDefinition 5 Die Sigmoid-Funktion \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) hat eine Ableitung, die wie folgt definiert ist:\n\\[\n\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n\\]\nDiese Ableitung erreicht ihren Maximalwert von 0,25 bei \\(x = 0\\) und n√§hert sich 0 an, wenn \\(|x|\\) gr√∂√üer wird. Dies ist eine der Hauptursachen f√ºr das Vanishing-Gradient-Problem in tiefen neuronalen Netzwerken.\n\nUrspr√ºnglich wurden Skip Connections entwickelt, um das Problem des ‚ÄúVerschwindenden Gradienten‚Äù (siehe Definition¬†6) in modernen neuronalen Netzwerken zu l√∂sen. Dieses Problem tritt auf, wenn Netzwerke durch Gradientenabstieg optimiert werden, wobei die Aktualisierung der Parameter in Richtung des negativen Gradienten der Kostenfunktion erfolgt.\n\nDefinition 6 Sei \\(L\\) die Kostenfunktion, \\(x_i\\) die Ausgabe (Aktivierung) der \\(i\\)-ten Schicht eines neuronalen Netzwerks und \\(i\\) der Index der Schicht. Die Ausgabe der n√§chsten Schicht \\(x_{i+1}\\) h√§ngt von \\(x_i\\) √ºber eine Transformation \\(f_i\\) ab, also \\(x_{i+1} = f_i(x_i)\\). Die Gradientenpropagation von der Kostenfunktion \\(L\\) zur Ausgabe \\(x_i\\) erfolgt gem√§√ü der Kettenregel:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_{i+1}} \\frac{\\partial x_{i+1}}{\\partial x_i}\n\\]\nDer Kern des Vanishing-Gradient-Problems liegt in der wiederholten Anwendung der Kettenregel √ºber viele Schichten hinweg. F√ºr eine Schicht \\(i\\) in einem Netzwerk mit \\(n\\) Schichten ergibt sich der Gradient \\(\\frac{\\partial L}{\\partial x_i}\\) durch die Verkettung der Gradienten aller nachfolgenden Schichten:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_n} \\cdot \\prod_{k=i}^{n-1} \\frac{\\partial x_{k+1}}{\\partial x_k}\n\\]\nWenn viele dieser Ableitungen Betr√§ge kleiner als 1 haben (z.B. bei Sigmoid-Aktivierungen), kann der Gesamtgradient schnell gegen Null tendieren, wodurch das Training tiefer Netzwerke erschwert wird.\n\nSkip Connections schaffen einen direkten Informationsfluss zwischen fr√ºheren und sp√§teren Schichten des neuronalen Netzwerks. Mathematisch kann eine Skip Connection als Addition der Eingabe \\(x\\) zur Ausgabe einer Transformation \\(F(x)\\) dargestellt werden:\n\\[\ny = F(x) + x\n\\]\nDiese einfache Operation hat weitreichende Auswirkungen auf die Gradientenausbreitung im Netzwerk. Skip Connections erm√∂glichen alternative Pfade f√ºr die Gradientenpropagation, wodurch Gradienten direkt zu fr√ºheren Schichten flie√üen k√∂nnen, ohne durch alle dazwischenliegenden Transformationen und Aktivierungsfunktionen abgeschw√§cht zu werden. Da die Ableitung der Identit√§tsfunktion konstant 1 ist, wird der Gradient entlang des Skip-Pfades nicht verringert:\n\\[\n\\frac{\\partial(x + F(x))}{\\partial x} = 1 + \\frac{\\partial F(x)}{\\partial x}\n\\]\nDiese architektonischen Ver√§nderungen f√ºhren zu einer signifikanten Gl√§ttung der Verlustlandschaft (siehe Abbildung¬†7), was die Optimierung neuronaler Netzwerke erheblich erleichtert und das Training tieferer Architekturen erm√∂glicht. [6]\n\n\n\n\n\n\nAbbildung¬†7: Verlustlandschaft (aus Hao Li 2017)\n\n\n\nIm U-Net werden entlang des Encoder-Pfads die Dimensionen der Feature-Maps reduziert und gleichzeitig die semantische Tiefe erh√∂ht. Bei diesem Prozess gehen globale Details verloren. Um diese Verluste auszugleichen, werden im U-Net die Feature-Maps aus jeder Schicht im Encoder-Pfad durch Skip Connections direkt den ensprechenden Schichten auf dem Decoder Pfad zugeordnet (die grauen Pfeile in Abbildung¬†6).\n\nDefinition 7 Sei \\(F_{enc}\\) eine Feature-Map auf dem Encoder-Pfad und \\(F_{dec}\\) die rekonstruktive Feature-Map auf dem Decoder-Pfad selbiger Hierrachiestufe. Der zusammengesetzte Tensor ist definiert als:\n\\[\nF_{concat} = \\text{concat}(F_{enc}, F_{dec})\n\\]\n\nDiese Verkn√ºpfungen erfolgen jedoch nicht aditiv wie zuvor beschrieben, sondern durch Konkatenation der Feature-Maps entlang der Kanalachse (siehe Definition¬†7). Diese Strategie erm√∂glicht es dem Decoder, sowohl die tiefen, abstrakten Merkmale aus dem Encoder als auch globalen Informationen zu ber√ºcksichtigen. Dadurch k√∂nnen pr√§zisere Segmentierungen und Rekonstruktionen erzielt werden. [5]\n\n\n\n\nVision Transformer ist ein Ansatz zur Verarbeitung von Bilddaten, der Transformer-Architekturen, die aus dem Bereich der nat√ºrlichen Sprachverarbeitung (NLP) bekannt sind, f√ºr Bilder nutzbar macht. Anstatt sich auf Faltungen wie bei CNNs (siehe Kapitel¬†1.1.1) zu st√ºtzen, basiert ViT auf der Selbstaufmerksamkeitsmechanik (Self-Attention), die es dem Modell erm√∂glicht, globale Abh√§ngigkeiten innerhalb eines Bildes effizient zu erfassen. [7]\n\n\nIm Gegensatz zu CNNs behandelt ein ViT ein Bild nicht als ein einzelnes, zusammenh√§ngendes Gitter von Pixeln, sondern zerlegt es in kleinere Teilbereiche, sogenannte Patches.\n\nDefinition 8 Sei \\(I \\in \\mathbb{R}^{H \\times W \\times C}\\) ein Bild mit H√∂he \\(H\\), Breite \\(W\\) und \\(C\\) Farbkan√§len. Dieses Bild wird in \\(n\\) Patches der Gr√∂√üe \\(P^2\\) zerlegt, wobei:\n\\[\nn = \\frac{HW}{P^2}\n\\]\nJeder Patch wird dann zu einem Vektor der L√§nge \\(P^2 C\\) abgeflacht und durch eine lineare Projektion in einen \\(D\\)-dimensionalen Vektorraum eingebettet.\n\nDadurch wird das Bild als Sequenz von eingebetteten Patches dargestellt, vergleichbar mit einer Wortfolge in einem Sprachmodell.\n\n\n\nDie Architektur eines Vision Transformers besteht aus mehreren Hauptkomponenten, die systematisch aufeinander aufbauen, um ein leistungsf√§higes Bildverarbeitungsmodell zu bilden. Die grundlegende Struktur wird in Abbildung¬†8 dargestellt.\n\n\n\n\n\n\nAbbildung¬†8: Vision Transformer Architektur (aus GeeksForGeeks 2025)\n\n\n\n\n\n\nMulti-Head Attention ist ein zentrales Konzept im Vision Transformer, welches es dem Modell erm√∂glicht, verschiedene Aspekte der Eingabesequenz parallel zu erfassen. Statt nur eine einzige Aufmerksamkeit (Attention) zu berechnen, werden mehrere unabh√§ngige Attention-K√∂pfe parallel berechnet, deren Ergebnisse anschlie√üend kombiniert werden.\n\nDefinition 9 Seien \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) die Abfrage-Matrix (Query), \\(K \\in \\mathbb{R}^{n \\times d_k}\\) die Schl√ºssel-Matrix (Key) und \\(V \\in \\mathbb{R}^{n \\times d_v}\\) die Wert-Matrix (Value), wobei \\(n\\) die Anzahl der Patches (Sequenzl√§nge), \\(d_k\\) die Dimension der Query- und Key-Vektoren und \\(d_v\\) die Dimension der Value-Vektoren ist.\nDie skalierte Punktprodukt-Attention wird berechnet als:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n\\]\nHierbei wird \\(QK^\\top\\) genutzt, um die √Ñhnlichkeit zwischen Abfragen und Schl√ºsseln zu messen, \\(\\sqrt{d_k}\\) dient als Skalierungsfaktor, um numerische Stabilit√§t bei gro√üen Dimensionen zu gew√§hrleisten, und die Softmax-Funktion sorgt daf√ºr, dass die resultierenden Gewichte normalisiert sind.\n\nUm verschiedene Repr√§sentationen gleichzeitig zu lernen, wird die Attention mehrfach mit unterschiedlichen Parametern berechnet. Jeder Attention-Kopf erh√§lt dabei eigene Gewichtsmatrizen und kann sich auf unterschiedliche Aspekte der Eingabedaten konzentrieren.\n\nDefinition 10 Sei \\(h\\) die Anzahl der K√∂pfe. F√ºr jeden Kopf \\(i \\in \\{1, \\dotsc, h\\}\\) existieren eigene Gewichtsmatrizen \\(W_i^Q \\in \\mathbb{R}^{D \\times d_k}\\) f√ºr Queries, \\(W_i^K \\in \\mathbb{R}^{D \\times d_k}\\) f√ºr Keys und \\(W_i^V \\in \\mathbb{R}^{D \\times d_v}\\) f√ºr Values.\nDie Multi-Head Attention berechnet f√ºr jeden Kopf:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\]\nAnschlie√üend werden die Ergebnisse aller K√∂pfe zusammengef√ºgt und nochmals linear projiziert:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dotsc, \\text{head}_h) W^O\n\\]\nmit \\(W^O \\in \\mathbb{R}^{h d_v \\times D}\\) als finaler Projektionsmatrix.\n\nDurch die Verwendung mehrerer Attention-K√∂pfe kann das Modell gleichzeitig verschiedene Beziehungen und Muster innerhalb der Bild-Patches erfassen. Stellen Sie sich vor, dass jeder Kopf wie ein Spezialist arbeitet, der bestimmte Eigenschaften im Bild sucht ‚Äì einer achtet vielleicht mehr auf Farben, ein anderer auf Formen, wieder ein anderer auf Texturen. Diese parallele Verarbeitung verbessert die F√§higkeit des Modells, komplexe Strukturen in den Daten zu erkennen. Am Ende werden all diese spezialisierten Beobachtungen zusammengef√ºhrt, um ein umfassendes Verst√§ndnis des Bildes zu erm√∂glichen.\n\n\n\nVision Transformer unterscheiden sich in mehreren wesentlichen Aspekten von CNNs2:\n\nVerarbeitungsmechanismus: CNNs verwenden lokale Faltungsoperationen, w√§hrend ViTs auf globaler Selbstaufmerksamkeit basieren.\nInduktive Bias: CNNs haben einen starken induktiven Bias f√ºr die lokale Struktur und Translationsinvarianz von Bildern, w√§hrend ViTs weniger inh√§rente Annahmen √ºber die Bildstruktur treffen.\nSkalierbarkeit: ViTs skalieren gut mit gr√∂√üeren Datenmengen und Modellgr√∂√üen und k√∂nnen bei ausreichendem Training CNNs in der Leistung √ºbertreffen.\nRessourcenbedarf: Die quadratische Komplexit√§t der Selbstaufmerksamkeit bez√ºglich der Sequenzl√§nge kann bei hochaufl√∂senden Bildern zu erheblichem Berechnungsaufwand f√ºhren.\n\nIn der praktischen Anwendung werden h√§ufig hybride Ans√§tze verfolgt, die St√§rken beider Architekturen kombinieren. Beispielsweise k√∂nnen CNN-basierte Feature-Extraktoren mit Transformer-Modulen erg√§nzt werden, um sowohl lokale Details als auch globale Kontextinformationen effektiv zu verarbeiten. [8]\nVision Transformer haben in vielen Computer-Vision-Aufgaben Benchmark-Ergebnisse erzielt und werden zunehmend f√ºr medizinische Bildverarbeitung, einschlie√ülich der Analyse von Hautl√§sionen, eingesetzt. Ihre F√§higkeit, komplexe Beziehungen √ºber gro√üe r√§umliche Entfernungen hinweg zu modellieren, macht sie besonders wertvoll f√ºr die Erkennung subtiler Muster in dermatologischen Bildern. [9]\n\n\n\n\nF√ºr die Klassifikation von Hautl√§sionen wird auf einen hybriden Modellansatz gesetzt, der die St√§rken von Convolutional Neural Networks und Vision Transformern kombiniert.\nAls Grundlage dient EfficientNetV2-B0, ein kompakter und schneller CNN-Backbone, der speziell f√ºr eine hohe Trainingsgeschwindigkeit und verbesserte Skalierbarkeit entwickelt wurde. EfficientNetV2 nutzt klassische Faltungen sowie optimierte MBConv- und Fused-MBConv-Bl√∂cke, wodurch sowohl die Modellgr√∂√üe verringert als auch die Extraktion lokaler Merkmale effizient gestaltet wird. Die resultierenden Feature-Maps besitzen bereits eine reduzierte r√§umliche Aufl√∂sung und werden als Eingabe f√ºr nachgelagerte Vision Transformer Encoder-Bl√∂cke verwendet.\nDie genaue Struktur von EfficientNetV2-B0 ist in Tabelle¬†1 dargestellt. Das Modell beginnt mit einer klassischen 3x3-Faltung (Stage 0) zur ersten Merkmalsextraktion, gefolgt von einer Reihe von Fused-MBConv-Bl√∂cken (Stages 1‚Äì3), die eine besonders effiziente Kombination aus Faltung und Punktweise-Faltung darstellen. Ab Stage 4 kommen regul√§re MBConv-Bl√∂cke mit Squeeze-and-Excitation (SE)-Modulen zum Einsatz, die wichtige Merkmale selektiv verst√§rken. Die Anzahl der Kan√§le steigt dabei schrittweise von 24 auf 256, w√§hrend gleichzeitig die r√§umliche Aufl√∂sung durch Strides von 2 reduziert wird. Die finale Stufe (Stage 7) verwendet eine 1x1-Faltung, Global Average Pooling und eine Fully Connected Schicht, um die endg√ºltige Repr√§sentation zu erzeugen.\n\n\n\nTabelle¬†1: EffiecientNetV2-B0 Architektur (aus Tan 2021)\n\n\n\n\n\n\n\n\n\n\n\n\nStage\nOperator\nStride\nChannels\nLayers\n\n\n\n\n0\nConv3x3\n2\n24\n1\n\n\n1\nFused-MBConv1, k3x3\n1\n24\n2\n\n\n2\nFused-MBConv4, k3x3\n2\n48\n4\n\n\n3\nFused-MBConv4, k3x3\n2\n64\n4\n\n\n4\nMBConv4, k3x3, SE0.25\n2\n128\n6\n\n\n5\nMBConv6, k3x3, SE0.25\n1\n160\n9\n\n\n6\nMBConv6, k3x3, SE0.25\n2\n256\n15\n\n\n7\nConv1x1 & Pooling & FC\n-\n1280\n1\n\n\n\n\n\n\nDurch diese Architektur werden die Vorteile der lokalen Detailerfassung des CNNs mit der globalen Kontextmodellierung des Transformers kombiniert. Zudem profitiert der Ansatz davon, dass der Transformer nur auf die komprimierten Feature-Maps angewendet wird, was die Berechnungslast erheblich reduziert, ohne die Modellqualit√§t zu beeintr√§chtigen. [10]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#convolutional-neural-netwroks-cnns",
    "href": "kapitel/modellentwicklung.html#convolutional-neural-netwroks-cnns",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Convolutional Neural Networks stellen einen Spezialtypen k√ºnstlicher neuronaler Netzwerke dar der besonders f√ºr die Verarbeitung Bilddaten konzipiert wurde. Im Gegensatz zu herk√∂mmlichen neuronalen Netzwerken verwenden CNNs spezielle Operationen, die auf der mathematischen Faltung (Convolution) basieren. [1]\n\n\nBei der Verarbeitung von digitalen Signale oder Bildern k√∂nnen die Eingabeparameter meistens durch diskrete Funktionen abgebildet werden.\n\nDefinition 1 Seien \\(f, g: D \\to \\mathbb{C}\\) mit dem diskreten Definitionsbereich \\(D \\subseteq \\mathbb{Z}\\). Dann ist die diskrete Faltung definiert durch:\n\\[\n(f * g)(n)=\\sum_{k \\in D}{f(k)g(n-k)}\n\\]\n\nFaltungsmatritzen (auch Kernel oder Filter) sind meist quadratische Matrizen ungerader Abmessungen in unterschiedlichen Gr√∂√üen. Einige Bildbearbeitungsoperationen k√∂nnen als lineares System interpretiert werden, wobei eine diskrete Faltung (siehe Definition¬†1) angewendet wird. F√ºr diskrete zweidimensionale Funktionen (digitale Bilder, Signale, etc.) ergibt sich die folgende Berechnungsformel\n\nDefinition 2 Sei \\(I^{\\ast} (x,y)\\) das Ergebnispixel, \\(I\\) das Bild, auf welches der Filter angewendet wird, \\(a\\) die Koordinate des Mittelungspunkts in der quadratischen Faltungsmatrx und \\(k(i, j)\\) ein Element der Faltungsmatrix. Dann ist die Berechnungsformel der diskreten Faltung definiert durch: \\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)k(i, j)}\n\\]\n\nDas in Definition¬†2 dargestellte mathematische Konzept der diskreten Faltung wird auf ein 16x16 Pixel gro√ües Bild angewendet (mit den Werten 0 f√ºr schwarz und 1 f√ºr wei√ü). Um die visuelle Wirkung der Faltungsoperation zu demonstrieren, wird eine spezifische Faltungsmatrix \\(k\\) verwendet.\n\\[\nk = \\frac{1}{9} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n\\]\nDie hier verwendete Faltungsmatrix \\(k\\) ist ein 3√ó3 Mittelwertfilter, bei dem jedes Element den Wert \\(\\frac{1}{9}\\) hat. Dieser Filter f√ºhrt zu einer Gl√§ttung des Bildes (siehe Abbildung¬†1), da er f√ºr jedes Pixel den Durchschnitt der umliegenden 3√ó3 Nachbarschaft berechnet. Im Beispiel wird ein ringf√∂rmiges Muster gegl√§ttet, was die Kanten des Rings weicher erscheinen l√§sst und einen Unsch√§rfeeffekt erzeugt. [2]\n\n\nCode\nimage = np.zeros((16, 16))\nfor i in range(16):\n    for j in range(16):\n        dist = np.sqrt((i-7.5)**2 + (j-7.5)**2)\n        if 3 &lt;= dist &lt;= 6:\n            image[i, j] = 1.0\nkernel = 1/9 * np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\nfiltered_image = signal.convolve2d(image, kernel, mode='same', boundary='symm')\nfig, axes = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\naxes[0].imshow(image, cmap='gray')\naxes[0].set_title('Originalbild (16x16)')\naxes[0].axis('off')\n\naxes[1].imshow(filtered_image, cmap='gray')\naxes[1].set_title('Nach Faltung mit k')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†1: Beispiel einer Faltungsoperation\n\n\n\n\n\n\n\n\nEin CNN besteht aus mehreren Schichten, welche die Grundlage f√ºr die meinsten modernen Bilderkennungmodelle darstellen.\n\n\n\n\n\n\nflowchart TD\n    Input(\"Input Image\") --&gt; Conv\n\n    subgraph Conv[\"Convolutional Layer\"]\n        direction LR\n        C1[\"Feature Extraction\"] --- C2[\"Filter Application\"]\n    end\n\n    Conv --&gt; Pool\n\n    subgraph Pool[\"Pooling Layer\"]\n        direction LR\n        P1[\"Dimension Reduction\"] --- P2[\"Parameter Reduction\"]\n    end\n\n    Pool --&gt; FC\n\n    subgraph FC[\"Fully Connected Layer\"]\n        direction LR\n        F1[\"Feature Classification\"] --- F2[\"Output Prediction\"]\n    end\n\n    FC --&gt; Output(\"Classification Result\")\n\n    style Input fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Output fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Conv fill:#d1ecf1,stroke:#0c5460,stroke-width:1px\n    style Pool fill:#d4edda,stroke:#155724,stroke-width:1px\n    style FC fill:#f8d7da,stroke:#721c24,stroke-width:1px\n\n\n\n\nAbbildung¬†2: CNN Funktionsweise\n\n\n\n\n\nDer in Abbildung¬†2 dargestellte Prozessfluss beschreibt die Grundidee von CNNs. Die Schichten, welche verwendet werden erf√ºllen dabei die folgenden Aufgaben:\n\nConvolutional Layer: Extrahiert Merkmale durch die Anwendung der in Kapitel¬†1.1.1 beschriebenen Filter.\nPooling Layer: Reduziert Dimensionen und erh√∂ht die Berechnungseffizienz.\nFully Connected Layer: Klassifiziert die extrahierten Merkmale.\n\nZur Reduktion der Dimensionen (pooling) k√∂nnen je nach Kontext verschiedene Funktionen angewendet werden. Es ist m√∂glich nach Maxima, Minima und Durchschnittswerten zu reduzieren. Dabei werden, √§hnlich wie bei der Faltung, Matrizen auf einem Wert abgebildet. Diese Pooling-Matrix bewegt sich √ºber die Urspr√ºngliche Matrix. Dabei sind folgende Parameter entscheident:\n\nFilter-Size (Pooling Gr√∂√üe): Definiert die Gr√∂√üe der Pooling Matrix.\nStride: Bestimmt die Schrittgr√∂√üe, mit der sich die Pooling Matrix √ºber die Urspr√ºngliche Matrix bewegt.\n\n\nDefinition 3 Die gr√∂√üe der aus dem Pooling Prozess hervorgehenden reduzierten Matrix berechne sich mit der folgenden Formel. Seien \\(h\\) die H√∂he der Eingabematrix, \\(w\\) die Breite der Eingabematrix, \\(c\\) die Anzahl der Eingabematrizen (Annahme: Alle Eingabematrizen sind gleich gro√ü), \\(f\\) die Gr√∂√üe der Pooling-Matrix und \\(s\\) der Stride.\n\\[\nc \\left(\\left\\lfloor\\frac{h-f+1}{s}\\right\\rfloor + 1\\right) \\left(\\left\\lfloor\\frac{w-f+1}{s}\\right\\rfloor + 1\\right)\n\\]\n\nIm folgenden wird dargestellt, wie die Gr√∂√üe eines quadratischen Bildes mit einem Farbkanal (mit den Werten 0 f√ºr schwarz und 1 f√ºr wei√ü) durch den Pooling Prozess reduziert werden kann. Zur Berechnung der Bildgr√∂√üe nach dem Pooling Prozess wird die Formel aus Definition¬†3 angewendet. [3]\n\n\nCode\npicture_width = np.array([counter for counter in range(16, 500)])\n\ndef picture_size(width):\n    return width**2\n\ndef pooling_size(width, matrix_size, stride):\n    output_dim = (np.floor((width - matrix_size + 1) / stride) + 1)\n    return int(output_dim**2)\n\nfilter_size = 3\nstride = 2\n\noriginal_sizes = [picture_size(w) for w in picture_width]\npooled_sizes = [pooling_size(w, filter_size, stride) for w in picture_width]\n\nplt.figure(figsize=STANDARD_FIGSIZE)\nplt.plot(picture_width, original_sizes, 'b-', label='Originalbild')\nplt.plot(picture_width, pooled_sizes, 'r-', label=f'Nach Pooling (Filter: {filter_size}x{filter_size}, Stride: {stride})')\nplt.xlabel('Breite des Originalbildes (Pixel)')\nplt.ylabel('Anzahl Pixel')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†3: Auswirkung von Pooling auf die Bildgr√∂√üe\n\n\n\n\n\nIn den Abbildungen Abbildung¬†4 und Abbildung¬†5 wird exemplarisch dargestellt, wie sich unterschiedliche Pooling-Methoden auf ein Eingabebild auswirken. Dabei wird jeweils ein Filter der Gr√∂√üe 3√ó3 mit einem Stride von 2 √ºber das Bild bewegt.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\npicture = np.random.randint(0, 31, size=(9, 9), dtype=np.uint8)\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\navg_pooled = F.avg_pool2d(tensor, kernel_size=3, stride=2)\n\nvmin, vmax = 0, 31\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(avg_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Durchschnitts Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†4: Beispiel f√ºr Durchschnitts-Pooling (3x3, Stride 2)\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Verwende das gleiche Bild wie oben\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nmax_pooled = F.max_pool2d(tensor, kernel_size=3, stride=2)\n\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(max_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Maxima Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung¬†5: Beispiel f√ºr Maxima-Pooling (3x3, Stride 2)\n\n\n\n\n\nBeide Methoden reduzieren die Bildgr√∂√üe und damit die Anzahl der zu verarbeitenden Parameter, wobei sie jeweils unterschiedliche Eigenschaften des urspr√ºnglichen Bildes erhalten oder herausfiltern. Die Wahl der Pooling-Methode h√§ngt daher stark vom Einsatzzweck und den gew√ºnschten Modell-Eigenschaften ab. [4]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#u-net",
    "href": "kapitel/modellentwicklung.html#u-net",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Herk√∂mmliche CNNs basieren auf einer sequentiellen Abfolge der in Kapitel¬†1.1.2 beschriebenen Operationen. U-Net hingegen ist eine spezielle Variante eines CNN, welche urspr√ºnglich f√ºr biomedizinische Bildsegmentierung konzipiert wurde.\n\n\nDie U-Net Architektur besteht aus einer kontrahierenden Komponente (Encoder-Pfad) und einer expandierenden Komponente (Decoder-Pfad), welche zusammen eine U-Form bilden.\n\n\n\n\n\n\nAbbildung¬†6: U-Net Architektur (aus Ronneberger 2015)\n\n\n\nWie in Abbildung¬†6 zu erkennen ist, wird auf dem Encoder-Pfad zur Feature Extraktion die Aktivierungsfunktion Rectified Linear Units (ReLU) verwendet (siehe Definition¬†4). Folglich besteht der Encoder-Pfad aus der wiederholten Anwendung von Faltungen (siehe Definition¬†2) gefolgt von Aktivierungsfunktionen und Max-Pooling (siehe Abbildung¬†5) zur Dimensionsreduktion. Dabei verdoppelt jeder Downsampling-Schritt die Anzahl der Feature-Kan√§le1.\n\nDefinition 4 Sei \\(f\\) die ReLU Aktivierungsfunktion und \\(x\\) das Eingabepixel.\n\\[\nf(x) = \\max (0, x)\n\\]\n\nDer Decoder-Pfad t√§tigt ein Upsampling der Feature-Map, gefolgt von einer Faltung, welche die Anzahl der Feature-Kan√§le halbiert. Entscheident f√ºr die Leistungsf√§higkeit sind die sogenannten ‚ÄúSkip Connections‚Äù, welche eine direkte Verbindung zwischen den Feature-Maps im Encoder- und Decoder-Pfad symbolisieren. [5]\n\n\n\n\nDefinition 5 Die Sigmoid-Funktion \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) hat eine Ableitung, die wie folgt definiert ist:\n\\[\n\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n\\]\nDiese Ableitung erreicht ihren Maximalwert von 0,25 bei \\(x = 0\\) und n√§hert sich 0 an, wenn \\(|x|\\) gr√∂√üer wird. Dies ist eine der Hauptursachen f√ºr das Vanishing-Gradient-Problem in tiefen neuronalen Netzwerken.\n\nUrspr√ºnglich wurden Skip Connections entwickelt, um das Problem des ‚ÄúVerschwindenden Gradienten‚Äù (siehe Definition¬†6) in modernen neuronalen Netzwerken zu l√∂sen. Dieses Problem tritt auf, wenn Netzwerke durch Gradientenabstieg optimiert werden, wobei die Aktualisierung der Parameter in Richtung des negativen Gradienten der Kostenfunktion erfolgt.\n\nDefinition 6 Sei \\(L\\) die Kostenfunktion, \\(x_i\\) die Ausgabe (Aktivierung) der \\(i\\)-ten Schicht eines neuronalen Netzwerks und \\(i\\) der Index der Schicht. Die Ausgabe der n√§chsten Schicht \\(x_{i+1}\\) h√§ngt von \\(x_i\\) √ºber eine Transformation \\(f_i\\) ab, also \\(x_{i+1} = f_i(x_i)\\). Die Gradientenpropagation von der Kostenfunktion \\(L\\) zur Ausgabe \\(x_i\\) erfolgt gem√§√ü der Kettenregel:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_{i+1}} \\frac{\\partial x_{i+1}}{\\partial x_i}\n\\]\nDer Kern des Vanishing-Gradient-Problems liegt in der wiederholten Anwendung der Kettenregel √ºber viele Schichten hinweg. F√ºr eine Schicht \\(i\\) in einem Netzwerk mit \\(n\\) Schichten ergibt sich der Gradient \\(\\frac{\\partial L}{\\partial x_i}\\) durch die Verkettung der Gradienten aller nachfolgenden Schichten:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_n} \\cdot \\prod_{k=i}^{n-1} \\frac{\\partial x_{k+1}}{\\partial x_k}\n\\]\nWenn viele dieser Ableitungen Betr√§ge kleiner als 1 haben (z.B. bei Sigmoid-Aktivierungen), kann der Gesamtgradient schnell gegen Null tendieren, wodurch das Training tiefer Netzwerke erschwert wird.\n\nSkip Connections schaffen einen direkten Informationsfluss zwischen fr√ºheren und sp√§teren Schichten des neuronalen Netzwerks. Mathematisch kann eine Skip Connection als Addition der Eingabe \\(x\\) zur Ausgabe einer Transformation \\(F(x)\\) dargestellt werden:\n\\[\ny = F(x) + x\n\\]\nDiese einfache Operation hat weitreichende Auswirkungen auf die Gradientenausbreitung im Netzwerk. Skip Connections erm√∂glichen alternative Pfade f√ºr die Gradientenpropagation, wodurch Gradienten direkt zu fr√ºheren Schichten flie√üen k√∂nnen, ohne durch alle dazwischenliegenden Transformationen und Aktivierungsfunktionen abgeschw√§cht zu werden. Da die Ableitung der Identit√§tsfunktion konstant 1 ist, wird der Gradient entlang des Skip-Pfades nicht verringert:\n\\[\n\\frac{\\partial(x + F(x))}{\\partial x} = 1 + \\frac{\\partial F(x)}{\\partial x}\n\\]\nDiese architektonischen Ver√§nderungen f√ºhren zu einer signifikanten Gl√§ttung der Verlustlandschaft (siehe Abbildung¬†7), was die Optimierung neuronaler Netzwerke erheblich erleichtert und das Training tieferer Architekturen erm√∂glicht. [6]\n\n\n\n\n\n\nAbbildung¬†7: Verlustlandschaft (aus Hao Li 2017)\n\n\n\nIm U-Net werden entlang des Encoder-Pfads die Dimensionen der Feature-Maps reduziert und gleichzeitig die semantische Tiefe erh√∂ht. Bei diesem Prozess gehen globale Details verloren. Um diese Verluste auszugleichen, werden im U-Net die Feature-Maps aus jeder Schicht im Encoder-Pfad durch Skip Connections direkt den ensprechenden Schichten auf dem Decoder Pfad zugeordnet (die grauen Pfeile in Abbildung¬†6).\n\nDefinition 7 Sei \\(F_{enc}\\) eine Feature-Map auf dem Encoder-Pfad und \\(F_{dec}\\) die rekonstruktive Feature-Map auf dem Decoder-Pfad selbiger Hierrachiestufe. Der zusammengesetzte Tensor ist definiert als:\n\\[\nF_{concat} = \\text{concat}(F_{enc}, F_{dec})\n\\]\n\nDiese Verkn√ºpfungen erfolgen jedoch nicht aditiv wie zuvor beschrieben, sondern durch Konkatenation der Feature-Maps entlang der Kanalachse (siehe Definition¬†7). Diese Strategie erm√∂glicht es dem Decoder, sowohl die tiefen, abstrakten Merkmale aus dem Encoder als auch globalen Informationen zu ber√ºcksichtigen. Dadurch k√∂nnen pr√§zisere Segmentierungen und Rekonstruktionen erzielt werden. [5]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#vision-transformer-vit",
    "href": "kapitel/modellentwicklung.html#vision-transformer-vit",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Vision Transformer ist ein Ansatz zur Verarbeitung von Bilddaten, der Transformer-Architekturen, die aus dem Bereich der nat√ºrlichen Sprachverarbeitung (NLP) bekannt sind, f√ºr Bilder nutzbar macht. Anstatt sich auf Faltungen wie bei CNNs (siehe Kapitel¬†1.1.1) zu st√ºtzen, basiert ViT auf der Selbstaufmerksamkeitsmechanik (Self-Attention), die es dem Modell erm√∂glicht, globale Abh√§ngigkeiten innerhalb eines Bildes effizient zu erfassen. [7]\n\n\nIm Gegensatz zu CNNs behandelt ein ViT ein Bild nicht als ein einzelnes, zusammenh√§ngendes Gitter von Pixeln, sondern zerlegt es in kleinere Teilbereiche, sogenannte Patches.\n\nDefinition 8 Sei \\(I \\in \\mathbb{R}^{H \\times W \\times C}\\) ein Bild mit H√∂he \\(H\\), Breite \\(W\\) und \\(C\\) Farbkan√§len. Dieses Bild wird in \\(n\\) Patches der Gr√∂√üe \\(P^2\\) zerlegt, wobei:\n\\[\nn = \\frac{HW}{P^2}\n\\]\nJeder Patch wird dann zu einem Vektor der L√§nge \\(P^2 C\\) abgeflacht und durch eine lineare Projektion in einen \\(D\\)-dimensionalen Vektorraum eingebettet.\n\nDadurch wird das Bild als Sequenz von eingebetteten Patches dargestellt, vergleichbar mit einer Wortfolge in einem Sprachmodell.\n\n\n\nDie Architektur eines Vision Transformers besteht aus mehreren Hauptkomponenten, die systematisch aufeinander aufbauen, um ein leistungsf√§higes Bildverarbeitungsmodell zu bilden. Die grundlegende Struktur wird in Abbildung¬†8 dargestellt.\n\n\n\n\n\n\nAbbildung¬†8: Vision Transformer Architektur (aus GeeksForGeeks 2025)\n\n\n\n\n\n\nMulti-Head Attention ist ein zentrales Konzept im Vision Transformer, welches es dem Modell erm√∂glicht, verschiedene Aspekte der Eingabesequenz parallel zu erfassen. Statt nur eine einzige Aufmerksamkeit (Attention) zu berechnen, werden mehrere unabh√§ngige Attention-K√∂pfe parallel berechnet, deren Ergebnisse anschlie√üend kombiniert werden.\n\nDefinition 9 Seien \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) die Abfrage-Matrix (Query), \\(K \\in \\mathbb{R}^{n \\times d_k}\\) die Schl√ºssel-Matrix (Key) und \\(V \\in \\mathbb{R}^{n \\times d_v}\\) die Wert-Matrix (Value), wobei \\(n\\) die Anzahl der Patches (Sequenzl√§nge), \\(d_k\\) die Dimension der Query- und Key-Vektoren und \\(d_v\\) die Dimension der Value-Vektoren ist.\nDie skalierte Punktprodukt-Attention wird berechnet als:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n\\]\nHierbei wird \\(QK^\\top\\) genutzt, um die √Ñhnlichkeit zwischen Abfragen und Schl√ºsseln zu messen, \\(\\sqrt{d_k}\\) dient als Skalierungsfaktor, um numerische Stabilit√§t bei gro√üen Dimensionen zu gew√§hrleisten, und die Softmax-Funktion sorgt daf√ºr, dass die resultierenden Gewichte normalisiert sind.\n\nUm verschiedene Repr√§sentationen gleichzeitig zu lernen, wird die Attention mehrfach mit unterschiedlichen Parametern berechnet. Jeder Attention-Kopf erh√§lt dabei eigene Gewichtsmatrizen und kann sich auf unterschiedliche Aspekte der Eingabedaten konzentrieren.\n\nDefinition 10 Sei \\(h\\) die Anzahl der K√∂pfe. F√ºr jeden Kopf \\(i \\in \\{1, \\dotsc, h\\}\\) existieren eigene Gewichtsmatrizen \\(W_i^Q \\in \\mathbb{R}^{D \\times d_k}\\) f√ºr Queries, \\(W_i^K \\in \\mathbb{R}^{D \\times d_k}\\) f√ºr Keys und \\(W_i^V \\in \\mathbb{R}^{D \\times d_v}\\) f√ºr Values.\nDie Multi-Head Attention berechnet f√ºr jeden Kopf:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\]\nAnschlie√üend werden die Ergebnisse aller K√∂pfe zusammengef√ºgt und nochmals linear projiziert:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dotsc, \\text{head}_h) W^O\n\\]\nmit \\(W^O \\in \\mathbb{R}^{h d_v \\times D}\\) als finaler Projektionsmatrix.\n\nDurch die Verwendung mehrerer Attention-K√∂pfe kann das Modell gleichzeitig verschiedene Beziehungen und Muster innerhalb der Bild-Patches erfassen. Stellen Sie sich vor, dass jeder Kopf wie ein Spezialist arbeitet, der bestimmte Eigenschaften im Bild sucht ‚Äì einer achtet vielleicht mehr auf Farben, ein anderer auf Formen, wieder ein anderer auf Texturen. Diese parallele Verarbeitung verbessert die F√§higkeit des Modells, komplexe Strukturen in den Daten zu erkennen. Am Ende werden all diese spezialisierten Beobachtungen zusammengef√ºhrt, um ein umfassendes Verst√§ndnis des Bildes zu erm√∂glichen.\n\n\n\nVision Transformer unterscheiden sich in mehreren wesentlichen Aspekten von CNNs2:\n\nVerarbeitungsmechanismus: CNNs verwenden lokale Faltungsoperationen, w√§hrend ViTs auf globaler Selbstaufmerksamkeit basieren.\nInduktive Bias: CNNs haben einen starken induktiven Bias f√ºr die lokale Struktur und Translationsinvarianz von Bildern, w√§hrend ViTs weniger inh√§rente Annahmen √ºber die Bildstruktur treffen.\nSkalierbarkeit: ViTs skalieren gut mit gr√∂√üeren Datenmengen und Modellgr√∂√üen und k√∂nnen bei ausreichendem Training CNNs in der Leistung √ºbertreffen.\nRessourcenbedarf: Die quadratische Komplexit√§t der Selbstaufmerksamkeit bez√ºglich der Sequenzl√§nge kann bei hochaufl√∂senden Bildern zu erheblichem Berechnungsaufwand f√ºhren.\n\nIn der praktischen Anwendung werden h√§ufig hybride Ans√§tze verfolgt, die St√§rken beider Architekturen kombinieren. Beispielsweise k√∂nnen CNN-basierte Feature-Extraktoren mit Transformer-Modulen erg√§nzt werden, um sowohl lokale Details als auch globale Kontextinformationen effektiv zu verarbeiten. [8]\nVision Transformer haben in vielen Computer-Vision-Aufgaben Benchmark-Ergebnisse erzielt und werden zunehmend f√ºr medizinische Bildverarbeitung, einschlie√ülich der Analyse von Hautl√§sionen, eingesetzt. Ihre F√§higkeit, komplexe Beziehungen √ºber gro√üe r√§umliche Entfernungen hinweg zu modellieren, macht sie besonders wertvoll f√ºr die Erkennung subtiler Muster in dermatologischen Bildern. [9]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#wahl-des-modellansatzes-f√ºr-die-hautl√§sionsklassifikation",
    "href": "kapitel/modellentwicklung.html#wahl-des-modellansatzes-f√ºr-die-hautl√§sionsklassifikation",
    "title": "Modellentwicklung",
    "section": "",
    "text": "F√ºr die Klassifikation von Hautl√§sionen wird auf einen hybriden Modellansatz gesetzt, der die St√§rken von Convolutional Neural Networks und Vision Transformern kombiniert.\nAls Grundlage dient EfficientNetV2-B0, ein kompakter und schneller CNN-Backbone, der speziell f√ºr eine hohe Trainingsgeschwindigkeit und verbesserte Skalierbarkeit entwickelt wurde. EfficientNetV2 nutzt klassische Faltungen sowie optimierte MBConv- und Fused-MBConv-Bl√∂cke, wodurch sowohl die Modellgr√∂√üe verringert als auch die Extraktion lokaler Merkmale effizient gestaltet wird. Die resultierenden Feature-Maps besitzen bereits eine reduzierte r√§umliche Aufl√∂sung und werden als Eingabe f√ºr nachgelagerte Vision Transformer Encoder-Bl√∂cke verwendet.\nDie genaue Struktur von EfficientNetV2-B0 ist in Tabelle¬†1 dargestellt. Das Modell beginnt mit einer klassischen 3x3-Faltung (Stage 0) zur ersten Merkmalsextraktion, gefolgt von einer Reihe von Fused-MBConv-Bl√∂cken (Stages 1‚Äì3), die eine besonders effiziente Kombination aus Faltung und Punktweise-Faltung darstellen. Ab Stage 4 kommen regul√§re MBConv-Bl√∂cke mit Squeeze-and-Excitation (SE)-Modulen zum Einsatz, die wichtige Merkmale selektiv verst√§rken. Die Anzahl der Kan√§le steigt dabei schrittweise von 24 auf 256, w√§hrend gleichzeitig die r√§umliche Aufl√∂sung durch Strides von 2 reduziert wird. Die finale Stufe (Stage 7) verwendet eine 1x1-Faltung, Global Average Pooling und eine Fully Connected Schicht, um die endg√ºltige Repr√§sentation zu erzeugen.\n\n\n\nTabelle¬†1: EffiecientNetV2-B0 Architektur (aus Tan 2021)\n\n\n\n\n\n\n\n\n\n\n\n\nStage\nOperator\nStride\nChannels\nLayers\n\n\n\n\n0\nConv3x3\n2\n24\n1\n\n\n1\nFused-MBConv1, k3x3\n1\n24\n2\n\n\n2\nFused-MBConv4, k3x3\n2\n48\n4\n\n\n3\nFused-MBConv4, k3x3\n2\n64\n4\n\n\n4\nMBConv4, k3x3, SE0.25\n2\n128\n6\n\n\n5\nMBConv6, k3x3, SE0.25\n1\n160\n9\n\n\n6\nMBConv6, k3x3, SE0.25\n2\n256\n15\n\n\n7\nConv1x1 & Pooling & FC\n-\n1280\n1\n\n\n\n\n\n\nDurch diese Architektur werden die Vorteile der lokalen Detailerfassung des CNNs mit der globalen Kontextmodellierung des Transformers kombiniert. Zudem profitiert der Ansatz davon, dass der Transformer nur auf die komprimierten Feature-Maps angewendet wird, was die Berechnungslast erheblich reduziert, ohne die Modellqualit√§t zu beeintr√§chtigen. [10]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#bildnormalisierung",
    "href": "kapitel/modellentwicklung.html#bildnormalisierung",
    "title": "Modellentwicklung",
    "section": "Bildnormalisierung",
    "text": "Bildnormalisierung",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#footnotes",
    "href": "kapitel/modellentwicklung.html#footnotes",
    "title": "Modellentwicklung",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\nBausteine der Feature Extraktion, die verschiede Aspekte des Eingabebilds wie Kanten, Texturen, Farben, etc. kodieren.‚Ü©Ô∏é\nChatGPT Prompt: Wie unterscheiden sich ViTs von CNNs im Bereich der medizinischen Bildverarbeitung. Bitte nenne 4 Punkte.‚Ü©Ô∏é",
    "crumbs": [
      "Modellentwicklung"
    ]
  }
]
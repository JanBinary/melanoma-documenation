[
  {
    "objectID": "presentation.html#forschungsgrundlage",
    "href": "presentation.html#forschungsgrundlage",
    "title": "Melanoma Präsentation",
    "section": "1.1 Forschungsgrundlage",
    "text": "1.1 Forschungsgrundlage\n\nAlle 120 Minuten stirbt ein Mensch an Hautkrebs. Wie kann künstliche Intelligenz Leben retten? [1]\n\n\n1895 entdeckte Wilhelm Röntgen die Röntgenstrahlen und revolutionierte die Medizin. Steht uns mit künstlicher Intelligenz eine noch größere Revolution bevor? [2]",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#ki-und-bildanalyse",
    "href": "presentation.html#ki-und-bildanalyse",
    "title": "Melanoma Präsentation",
    "section": "1.2 KI und Bildanalyse",
    "text": "1.2 KI und Bildanalyse",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#unser-forschungsgegenstand",
    "href": "presentation.html#unser-forschungsgegenstand",
    "title": "Melanoma Präsentation",
    "section": "1.3 Unser Forschungsgegenstand",
    "text": "1.3 Unser Forschungsgegenstand\n\nAKIECBCCBKLDFMELNVVASC\n\n\n\n\n\n\n\n\nAbbildung 1: Aktinische Keratose\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 2: Basalzellkarzinom\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 3: Keratosen-ähnliche Läsionen\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 4: Dermatofibrom\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 5: Melanom\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 6: Melanozytische Nävi\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 7: Gefäßläsionen",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#unser-vorgehen",
    "href": "presentation.html#unser-vorgehen",
    "title": "Melanoma Präsentation",
    "section": "1.4 Unser Vorgehen",
    "text": "1.4 Unser Vorgehen\n\n\n\n\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    section Recherche\n    Thema definieren           :done,    lit1, 2025-04-04,2025-04-05\n    Literatur auswerten        :done,    lit2, 2025-04-05,2025-04-22\n    \n    section Planung\n    Ziele definieren           :done,    plan1, 2025-04-05,2025-04-09\n    Ressourcen planen          :done,    plan2, 2025-04-09,2025-04-12\n    \n    section Grundlagen\n    Theoretische Basis         :done,         grund1, 2025-04-09,2025-04-27\n    Framework definieren       :done,         grund2, 2025-04-09,2025-04-12\n    \n    section Exploration\n    Datensatz Verständnis      :done,         data1, 2025-04-14,2025-04-27\n    Explorative Analyse        :done,         data2, 2025-04-14,2025-05-11\n    \n    section Modell\n    Mathematische Grundlagen   :done,          model4, 2025-04-29, 2025-05-13\n    Modell konzipieren         :done,         model1, 2025-04-29,2025-05-17\n    Implementierung            :done,         model2, 2025-05-21,2025-06-23\n    Testing & Validierung      :done,         model3, 2025-06-08,2025-06-28\n    \n    section Diskussion\n    Ergebnisse interpretieren  :done,         dis1, 2025-06-10,2025-06-28\n    Fazit & Ausblick          :done,          dis2, 2025-06-24,2025-06-29\n    Finalisierung             :active,          disk3, 2025-06-26,2025-07-02\n\n\n\n\n\n\n\n\nAbbildung 8: Projektablauf",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#arten-von-hautkrebs",
    "href": "presentation.html#arten-von-hautkrebs",
    "title": "Melanoma Präsentation",
    "section": "2.1 Arten von Hautkrebs",
    "text": "2.1 Arten von Hautkrebs\nplatzhalter\n\n\nDas ist eine Side Note",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#bisherige-diagnoseverfahren",
    "href": "presentation.html#bisherige-diagnoseverfahren",
    "title": "Melanoma Präsentation",
    "section": "2.2 Bisherige Diagnoseverfahren",
    "text": "2.2 Bisherige Diagnoseverfahren\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#grundlagen-computer-vision",
    "href": "presentation.html#grundlagen-computer-vision",
    "title": "Melanoma Präsentation",
    "section": "2.3 Grundlagen Computer Vision",
    "text": "2.3 Grundlagen Computer Vision\n\nWie kann ein Computer sehen?\n\n\nFaltung (Convolution)\n\\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)K(i, j)}\n\\]\nAufmerksamkeitsmechanismus (Attention)\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n\\]",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#was-passiert-bei-der-faltung",
    "href": "presentation.html#was-passiert-bei-der-faltung",
    "title": "Melanoma Präsentation",
    "section": "2.4 Was passiert bei der Faltung",
    "text": "2.4 Was passiert bei der Faltung\n\n🔍 Ein Filter \\(K\\) überlagert das Bild \\(I\\) und blickt auf einen \\(n \\times n\\) Bereich\n➕ Überlappende Werte werden multipliziert und aufsummiert\n🎯 Ein einzelner neuer Wert entsteht \\(I^{\\ast} (x, y)\\)\n\n\\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)K(i, j)}\n\\]",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#anwendung-der-faltung-in-cnns",
    "href": "presentation.html#anwendung-der-faltung-in-cnns",
    "title": "Melanoma Präsentation",
    "section": "2.5 Anwendung der Faltung in CNNs",
    "text": "2.5 Anwendung der Faltung in CNNs\n\n✅ Je nach Wahl des Kernels \\(K\\) werden unterschiedliche Merkmale hervorgehoben\n🏎️ Der Kernel \\(K\\) bewegt sich stückweise über das komplette Bild \\(I\\)\n🏃🏿‍♂️‍➡️ Die Schrittweite des Kernels ist variabel und wird als Stride bezeichnet\n🅾️ Padding ist möglich (0 Werte am Rand der Matrix)",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#pooling-in-cnns",
    "href": "presentation.html#pooling-in-cnns",
    "title": "Melanoma Präsentation",
    "section": "2.6 Pooling in CNNs",
    "text": "2.6 Pooling in CNNs\n\n⬇️ Reduziert die Bildgröße durch Zusammenfassen benachtbarter Pixel\n💻 Verringert Rechenaufwand und Anzahl der Paramter\n🟰 Teilweise translationsinvariant (\\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\), \\(f(A) = f(A + t)\\) )\n🏃🏿‍♂️‍➡️ Ein Fenster bewegt sich stückweise über das komplette Bild (ähnlich zur Faltung)",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#aktivierungsfunktionen",
    "href": "presentation.html#aktivierungsfunktionen",
    "title": "Melanoma Präsentation",
    "section": "2.7 Aktivierungsfunktionen",
    "text": "2.7 Aktivierungsfunktionen\n\n📏 Werden nach Faltungen / Linearkombinationen eingesetzt\n🎢 Ohne Aktivierung wäre ein tiefes Netz nur eine lineare Funktion\n📊 Helfen beim Lernen von Mustern und Entscheidungsgrenzen\n💯 Werden punktweise auf jedes Pixel / Feature angewendet",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#architektur-eines-cnns",
    "href": "presentation.html#architektur-eines-cnns",
    "title": "Melanoma Präsentation",
    "section": "2.8 Architektur eines CNNs",
    "text": "2.8 Architektur eines CNNs\n\n\nAbbildung 10: CNN Architektur (aus Medium)",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#gradienten-und-deren-aussagekraft",
    "href": "presentation.html#gradienten-und-deren-aussagekraft",
    "title": "Melanoma Präsentation",
    "section": "2.9 Gradienten und deren Aussagekraft",
    "text": "2.9 Gradienten und deren Aussagekraft\n\nℹ️ Der Gradient ist die Ableitung einer Funktion nach ihren Eingabewerten\n🎯 Zeigt Richtung des stärksten Wachstums und Einfluss der Parameter\n🖼️ Dargestellt durch den Nabla Operator \\(\\nabla\\)",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#section-5",
    "href": "presentation.html#section-5",
    "title": "Melanoma Präsentation",
    "section": "",
    "text": "\\[\nf : \\mathbb{R}^2 \\to \\mathbb{R}\n\\]\n\\[\nf(x, y) = 2x^2 + 3y\n\\]\n\\[\n\\nabla f =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4x \\\\\n3\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#section-6",
    "href": "presentation.html#section-6",
    "title": "Melanoma Präsentation",
    "section": "",
    "text": "\\[\n\\nabla f =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4x \\\\\n3\n\\end{pmatrix}\n\\]\n\nℹ️ \\(x\\) hat den größten Einfluss auf die Steigung von \\(f\\)\nℹ️ \\(y\\) hat konstant positiven Einfluss auf die Funktion\nℹ️ \\(\\nabla f\\): Richtung des größten Wachstums\nℹ️ \\(-\\nabla f\\): Richtung der größten Abnahme",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#kostenfunktionen-loss-function",
    "href": "presentation.html#kostenfunktionen-loss-function",
    "title": "Melanoma Präsentation",
    "section": "2.10 Kostenfunktionen (Loss Function)",
    "text": "2.10 Kostenfunktionen (Loss Function)\n\nℹ️ Messen den Unterschied zwischen Schätzung \\(\\hat{y}\\) und Zielwert \\(y\\)\nℹ️ Bei Klassifikationsproblemen eignet sich Cross Entropy Loss\n🎯 Fehlerwert durch Anpassung der Gewichte zu minimieren",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#gewichtsanpassung",
    "href": "presentation.html#gewichtsanpassung",
    "title": "Melanoma Präsentation",
    "section": "2.11 Gewichtsanpassung",
    "text": "2.11 Gewichtsanpassung\n\nℹ️ Nutzt den Gradienten der Kostenfunktion, um zu bestimmen, wie stark jedes Gewicht beiträgt\nℹ️ Beruht auf der Kettenregel der Ableitung\n🎯 Fehlerwert durch Anpassung der Gewichte zu minimieren",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#section-9",
    "href": "presentation.html#section-9",
    "title": "Melanoma Präsentation",
    "section": "",
    "text": "Kettenregel\n\\[\nf(x) = u(v(x))\n\\]\n\\[\nf'(x) = u'(v(x)) v'(x)\n\\]\n\n\nReminder",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#section-10",
    "href": "presentation.html#section-10",
    "title": "Melanoma Präsentation",
    "section": "",
    "text": "Kettenregel\n\\[\nL(\\sigma(z_6))\n\\]\n\\[\n\\frac{dL}{dz_6} = \\frac{dL}{d \\sigma (z_6)} \\frac{d \\sigma (z_6)}{dz_6}\n\\]\n\nProblem: \\(z_6 = \\sigma (z_5)\\), \\(z_5 = \\sigma (z_4)\\), …\n\n\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\n\n\\[\n\\frac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x))\n\\]\n\n\n\n\nReminder",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#vorstellung-des-datensatzes",
    "href": "presentation.html#vorstellung-des-datensatzes",
    "title": "Melanoma Präsentation",
    "section": "2.12 Vorstellung des Datensatzes",
    "text": "2.12 Vorstellung des Datensatzes\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#modellevaluierung",
    "href": "presentation.html#modellevaluierung",
    "title": "Melanoma Präsentation",
    "section": "3.1 Modellevaluierung",
    "text": "3.1 Modellevaluierung\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#gewonnene-erfahrungen",
    "href": "presentation.html#gewonnene-erfahrungen",
    "title": "Melanoma Präsentation",
    "section": "3.2 Gewonnene Erfahrungen",
    "text": "3.2 Gewonnene Erfahrungen\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#ergebnisse-der-eigenentwicklung",
    "href": "presentation.html#ergebnisse-der-eigenentwicklung",
    "title": "Melanoma Präsentation",
    "section": "4.1 Ergebnisse der Eigenentwicklung",
    "text": "4.1 Ergebnisse der Eigenentwicklung\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#alternative-modelle",
    "href": "presentation.html#alternative-modelle",
    "title": "Melanoma Präsentation",
    "section": "4.2 Alternative Modelle",
    "text": "4.2 Alternative Modelle\nplatzhalter\n4.2.1 LLMs im Test\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "presentation.html#ausblick",
    "href": "presentation.html#ausblick",
    "title": "Melanoma Präsentation",
    "section": "4.3 Ausblick",
    "text": "4.3 Ausblick\nplatzhalter",
    "crumbs": [
      "Melanoma Präsentation"
    ]
  },
  {
    "objectID": "kapitel/grundlagen.html",
    "href": "kapitel/grundlagen.html",
    "title": "Grundlagen",
    "section": "",
    "text": "Definition von Hautkrebsarten\n\n\nBisherige Diagnosewerkzeuge und Kriterien\n\n\nDefinition von künstlicher Intelligenz\n\n\nAnwendung von künstlicher Intelligenz im medizinischen Sektor",
    "crumbs": [
      "Grundlagen"
    ]
  },
  {
    "objectID": "kapitel/diskussion.html",
    "href": "kapitel/diskussion.html",
    "title": "Diskussion der Ergebnisse",
    "section": "",
    "text": "1 Interpretation dert Forschungsergebnisse\n\n\n2 Vergleich mit anderen Modellen\n\n\n3 Ausblick und mögliche Verbesserungen",
    "crumbs": [
      "Diskussion der Ergebnisse"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projektübersicht",
    "section": "",
    "text": "Projektübersicht\nWillkommen zum Melanoma Hautkrebs Diagnose Projekt, das im Rahmen des Moduls Advanced Topics in Computer Science an der Fachhochschule der Wirtschaft durchgeführt wurde. Ziel dieses Projekts ist es, zu untersuchen, wie gut ein selbst entwickeltes Modell zur Klassifikation von Hautläsionen gegen andere bestehende Modelle, wie Large Language Models (LLMs) und andere gängige Klassifikationsmethoden, abschneidet.\nIm Rahmen der Arbeit wird nicht nur die Erkennung von Melanomen (Hautkrebs) angestrebt, sondern es sollen vielmehr verschiedene Diagnosen anhand der vorliegenden Hautläsionen gestellt werden.\n\nDer Begriff künstliche Intelligenz in diesem Projekt bezieht sich auf Modelle, die durch maschinelles Lernen erstellt wurden und die Klassifikation von Daten ermöglichen. Dies schließt auch die Anwendung von Large Language Models wie ChatGPT ein, die auf ähnliche Prinzipien der maschinellen Lernens zurückgreifen.\n\nDie Struktur der Arbeit folgt dem Verlauf des Projekts: Zu Beginn werden die dermatologischen Grundlagen behandelt, um ein Verständnis für die Arten von Hautläsionen zu entwickeln, die für die Diagnose von Hautkrebs relevant sind. Es folgt eine Exploration der Metadaten, die den Datensatz beschreibt, der für das Modell verwendet wird, und die verwendeten Bilddaten erläutert. Der zentrale Abschnitt befasst sich mit der Entwicklung und dem Training des Modells, das auf Basis dieser Daten erstellt wurde. Abschließend wird die Diskussion der Ergebnisse durchgeführt, um die Leistungsfähigkeit des entwickelten Modells mit bestehenden Technologien zu vergleichen.\n\n\n\nAutoren\nDieses Projekt wurde von folgenden Studierenden der Fachhochschule der Wirtschaft durchgeführt:\n\nSergei Wendlang – Wirtschaftsinformatik, Schwerpunkt Data Science\nJan Henrik Uemann – Wirtschaftsinformatik, Schwerpunkt Data Science\nMarc Pöppelbaum – Wirtschaftsinformatik, Schwerpunkt Business Process Management\n\n\n\n\nWeitere Informationen\nFür dieses Projekt wurde der Datensatz von ISIC (International Skin Imaging Collaboration) verwendet. Der Datensatz ist unter folgendem Link zugänglich:\nHAM10000.\nDas gesamte Projekt ist auf Render gehostet, und die Webseite wurde mit Quarto erstellt."
  },
  {
    "objectID": "kapitel/datenexploration.html",
    "href": "kapitel/datenexploration.html",
    "title": "Datenexploration",
    "section": "",
    "text": "HAM10000 ist ein eine Sammlung von 10.015 dermatoskopischen Bildern, welche aus mehreren verschiedenen Quellen zu einem Datensatz im Harvard Dataverse zusammengetragen wurden.\n\n\nCode\nmetadata_df\n\n\n\n\nTabelle 1: Aufbau des Datensatzes\n\n\n\n\n\n\n\n\n\n\nlesion_id\nimage_id\ndx\ndx_type\nage\nsex\nlocalization\nage_group\n\n\n\n\n0\nHAM_0000118\nISIC_0027419\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n1\nHAM_0000118\nISIC_0025030\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n2\nHAM_0002730\nISIC_0026769\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n3\nHAM_0002730\nISIC_0025661\nbkl\nhisto\n80.0\nmale\nscalp\n61-80\n\n\n4\nHAM_0001466\nISIC_0031633\nbkl\nhisto\n75.0\nmale\near\n61-80\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10010\nHAM_0002867\nISIC_0033084\nakiec\nhisto\n40.0\nmale\nabdomen\n21-40\n\n\n10011\nHAM_0002867\nISIC_0033550\nakiec\nhisto\n40.0\nmale\nabdomen\n21-40\n\n\n10012\nHAM_0002867\nISIC_0033536\nakiec\nhisto\n40.0\nmale\nabdomen\n21-40\n\n\n10013\nHAM_0000239\nISIC_0032854\nakiec\nhisto\n80.0\nmale\nface\n61-80\n\n\n10014\nHAM_0003521\nISIC_0032258\nmel\nhisto\n70.0\nfemale\nback\n61-80\n\n\n\n\n10015 rows × 8 columns\n\n\n\n\n\n\nIn der oben abgebildeten Tabelle wird ein Ausschnitt des Datensatzes exemplarisch Abgebildet. Er besteht aus 10.015 Zeilen mit sieben Spalten. Entsprechend werden in dem Datensatz sieben unterschiedliche Attribute verwendet, um die dermatologischen Bilder zu identifizieren und klassifizieren. Die Attribute, welche im folgendenen näher erläutert werden sind lesion_id, image_id, dx, dx_type, age, sex und localization.\nlesion_id dient zur Identifikation der jeweils betrachtenden Hautläsionen. Insgesamt werden in diesem Datensatz 7.470 verschiedene Hautläsionen betrachtet, um über entsprechende lesion_id identifiziert. Die Differenz von 2.545 Einträgen entsteht durch die mehrfache Betrachtung von einzelnen Hautläsionen.\nDie image_id, wie die lesion_id, wird ebenfalls zur eindeutigen Identifikation verwendet. Im diesem Fall werden die dermatoskopischen Bilder identifiziert.\ndx gibt an, womit die abgebildeten Hautläsionen diagnostiziert wurden. Die im Datensatz verwendeten Kürzel werden in folgender Tabelle genauer erklärt. Insgesamt werden kommen in diesem Datensatz sieben verschiedene Diagnosen vor.\n\n\n\nTabelle 2: Diagnosekategorien\n\n\n\n\n\n\n\n\n\nKürzel\nDiagnosekategorie\n\n\n\n\nakiec\nAktinische Keratosen und intraepitheliales Karzinom / Bowen-Krankheit\n\n\nbcc\nBasalzellkarzinom\n\n\nbkl\nKeratosen-ähnliche Läsionen (Sonnenlentigines / seborrhoische Keratosen und lichen-planus-ähnliche Keratosen)\n\n\ndf\nDermatofibrom\n\n\nnv\nMelanozytische Nävi (Muttermal)\n\n\nmel\nMelanom\n\n\nvasc\nGefäßläsionen (Angiome, Angiokeratome, pyogene Granulome und Blutungen)\n\n\n\n\n\n\ndx_type definiert, wie die jeweilige Diagnose erreicht beziehungsweise welche Diagnosemethode verwendet wurde.\n\n\n\nTabelle 3: Diagnosemethoden\n\n\n\n\n\nKürzel\nDiagnosemethode\n\n\n\n\nconfocal\nHistopathologie\n\n\nconsensus\nExpertenkonsens\n\n\nfollow_up\nNachuntersuchungen\n\n\nhisto\nHistopathologie\n\n\n\n\n\n\nage und sex beschreiben jeweils das Alter und das geschlecht der Personen, dessen Bilder und diagnosen in diesem Datensatz gesammelt wurden. Diese beiden Attribute stellen gleichzeitig die einzigen personenbezogenen Daten dar, welche im HAM10000 zu verfügung stehen.\nlocalization gibt an, an welcher Stelle des Körpers die entsprechende Hautläsion lokalisiert wurde. In der folgenden Tabelle werden die im Datensatz notierten Körperstellen aufgelistet und übersetzt.\n\n\n\nTabelle 4: Körperstellen\n\n\n\n\n\nBezeichnung in Datensatz\nÜbersetzung\n\n\n\n\nscalp\nKopfhaut\n\n\nface\nGesicht\n\n\near\nOhr\n\n\nneck\nHals\n\n\nchest\nBrust\n\n\nback\nRücken\n\n\ntrunk\nRumpf\n\n\nabdomen\nBauch\n\n\nupper extremity\nObere Extremität\n\n\nlower extremity\nUntere Extremität\n\n\nhand\nHand\n\n\nfoot\nFuß\n\n\narm\nArm\n\n\nleg\nBein\n\n\nacral\nAkral (Finger, Zehen etc.)",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#diagnoseverteilung-der-läsionen-nach-geschlecht",
    "href": "kapitel/datenexploration.html#diagnoseverteilung-der-läsionen-nach-geschlecht",
    "title": "Datenexploration",
    "section": "Diagnoseverteilung der Läsionen nach Geschlecht",
    "text": "Diagnoseverteilung der Läsionen nach Geschlecht\nDie Definitionen der Kürzel, welche eine Diagnose repräsentieren sind in Tabelle 2 zusammengefasst. Wie in Abbildung 4 zu erkennen ist, dominiert die Diagnose nv den Datensatz. Diese repräsentiert 66.95% der Diagnosen. Das Verhältnis der Geschlechter male zu female beträgt 5406 / 4552. Demnach sind mänlichen Probanten im Datensatz leicht überrepresentiert. Die gefährlichen Diagnosen machen 19.51% des Datensatzes aus und das Verhältnis der Geschlechter male zu female beträgt unter diesen 1227 / 727. Daraus folgt, dass die männlichen Probanten in der Gruppe von Probanten mit gefährlichen Diagnosen höher representiert ist, als im gesamten Datensatz.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='dx',\n    hue='sex',\n    multiple='stack',\n    palette=GENDER_PALETTE,\n    edgecolor='white'\n)\nplt.xlabel('Diagnose')\nplt.ylabel('Anzahl Läsionen')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 4: Verteilung der Diagnosen nach Geschlecht",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-des-alters-der-probanten-nach-geschlecht",
    "href": "kapitel/datenexploration.html#verteilung-des-alters-der-probanten-nach-geschlecht",
    "title": "Datenexploration",
    "section": "Verteilung des Alters der Probanten nach Geschlecht",
    "text": "Verteilung des Alters der Probanten nach Geschlecht\nAnhand von Abbildung 5 lässt sich erkennen, dass das mittlere Alter der Probanten im gesamten Datensatz 51.86 Jahre beträgt (16.97 Jahre Standardabweichung). In der Gruppe der Probanten mit gefährlichen Diagnosen beträgt das mittlere Alter 63.28 (14.53 Jahre Standardabweichung). Eine Visualisierung dieses Zusammenhangs findet sich in Abbildung 14.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='age',\n    hue='sex',\n    multiple='stack',\n    bins=10,\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    ax=ax[0]\n)\nax[0].set_title('Altersverteilung')\nax[0].set_ylabel('Anzahl Läsionen')\nax[0].set_xlabel('Alter')\nsns.histplot(\n    data=metadata_dangerous_df,\n    x='age',\n    hue='sex',\n    multiple='stack',\n    bins=10,\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    legend=False,\n    ax=ax[1]\n)\nax[1].set_title('Altersverteilung (gefährl. Diagnosen)')\nax[1].set_ylabel('')\nax[1].set_xlabel('Alter')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 5: Verteilung des Alters nach Geschlecht",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-diagnosemethoden-nach-geschlecht",
    "href": "kapitel/datenexploration.html#verteilung-der-diagnosemethoden-nach-geschlecht",
    "title": "Datenexploration",
    "section": "Verteilung der Diagnosemethoden nach Geschlecht",
    "text": "Verteilung der Diagnosemethoden nach Geschlecht\nDie Definition der Kürzel, welche Diagnosemethoden representieren ist in Tabelle 3 zusammengefasst. Abbildung 6 zeigt, dass die Diagnosemethode histo den Datensatz mit einem Anteil von 53.32% dominiert. Zudem werden gefährliche Diagnosen ausschließlich über die Diagnosemethode histo diagnostiziert.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='dx_type',\n    hue='sex',\n    multiple='stack',\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    ax=ax[0]\n)\nax[0].set_title('Diagnosemethoden')\nax[0].set_ylabel('Anzahl Läsionen')\nax[0].set_xlabel('Diagnosemethode')\nsns.histplot(\n    data=metadata_dangerous_df,\n    x='dx_type',\n    hue='sex',\n    multiple='stack',\n    palette=GENDER_PALETTE,\n    edgecolor='white',\n    legend=False,\n    ax=ax[1]\n)\nax[1].set_title('Diagnosemethoden (gefährl. Diagnosen)')\nax[1].set_ylabel('')\nax[1].set_xlabel('Diagnosemethode')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 6: Verteilung der Diagnosemethoden nach Geschlecht",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-körperstellen-nach-alter",
    "href": "kapitel/datenexploration.html#verteilung-der-körperstellen-nach-alter",
    "title": "Datenexploration",
    "section": "Verteilung der Körperstellen nach Alter",
    "text": "Verteilung der Körperstellen nach Alter\nEine Übersicht der im Datensatz aufgeführten Körperstellen befindet sich in Tabelle 4. Anhand von Abbildung 7 lässt sich erkennen, dass in der Altersgruppe mit den meisten Läsionen (41-60 Jahre) häufig die Körperstellen lower extremity, back, trunk, abdomen und upper extrmity betroffen sind.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_data = pd.crosstab(metadata_df['localization'], metadata_df['age_group'])\nsns.heatmap(\n    data=heatmap_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Körperstelle / Alter')\nplt.xlabel('Altersgruppe')\nplt.ylabel('Körperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 7: Verteleilung der Körperstellen nach Atler\n\n\n\n\n\nNach Reduktion der Metadaten auf Datensätze mit gefährlichen Diagnosen (siehe Abbildung 8) ist die neue dominierende Altersgruppe, wie Abbildung 5 und Abbildung 14 andeuten lassen, 61-80 Jahre. Die betroffenen Körperstellen unter den 61-80 Jährigen mit gefährlichen Diagnosen sind demnach back, face, upper extremity und lower extremity.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_dangerous_data = pd.crosstab(metadata_dangerous_df['localization'], metadata_dangerous_df['age_group'])\nsns.heatmap(\n    data=heatmap_dangerous_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Körperstelle / Alter (gefährl. Diagnosen)')\nplt.xlabel('Altersgruppe')\nplt.ylabel('Körperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 8: Verteleilung der Körperstellen nach Atler (gefährl. Diagnosen)",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-körperstellen-nach-diagnose",
    "href": "kapitel/datenexploration.html#verteilung-der-körperstellen-nach-diagnose",
    "title": "Datenexploration",
    "section": "Verteilung der Körperstellen nach Diagnose",
    "text": "Verteilung der Körperstellen nach Diagnose\nAnhand von Abbildung 9 lässt sich erkennen, dass der Datensatz bei einer Betrachtung nach Körperstelle Tabelle 4 und Diagnosen Tabelle 2 über 3 Extrema verfügt, welche 41.41% des Datensatzes repräsentieren. Es handelt sich um die Diagnose nv and den Körperstellen back, lower extremity und trunk.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_data = pd.crosstab(metadata_df['localization'], metadata_df['dx'])\nsns.heatmap(\n    data=heatmap_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Körperstelle / Diagnose')\nplt.xlabel('Diagnose')\nplt.ylabel('Körperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 9: Verteleilung der Körperstellen nach Diagnose\n\n\n\n\n\nReduziert auf die gefährlichen Fälle ist die häufigste Diagnose mel (siehe Abbildung 11). Diese tritt dominant an den Körperstellen back, upper extremity und lower extremity auf.\n\nHeatmap KörperstellenAnteil Diagnosen\n\n\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_dangerous_data = pd.crosstab(metadata_dangerous_df['localization'], metadata_dangerous_df['dx'])\nsns.heatmap(\n    data=heatmap_dangerous_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Körperstelle / Diagnose (gefährl. Diagnosen)')\nplt.xlabel('Diagnose')\nplt.ylabel('Körperstelle')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 10: Verteleilung der Körperstellen nach Diagnose (gefährl. Diagnosen)\n\n\n\n\n\n\n\n\n\nCode\ndangerous_dx_counts = metadata_dangerous_df['dx'].value_counts()\ntotal_dangerous = dangerous_dx_counts.sum()\n\nlabels = [f\"{dx} ({count}/{total_dangerous})\" for dx, count in dangerous_dx_counts.items()]\n\nplt.figure(figsize=STANDARD_FIGSIZE)\nplt.pie(\n    dangerous_dx_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=sns.color_palette('Reds', n_colors=len(dangerous_dx_counts))\n)\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 11: Anteil gefährlicher Diagnosen unter allen gefährlichen Fällen\n\n\n\n\n\n\n\n\nBetrachtet man jedoch alle Fälle gruppiert nach Körperstelle, wie in Tabelle 5, so lassen sich an der Körperstelle face am häufigsten gefährliche Diagnosen verorten.\n\n\nCode\nmetadata_df['is_dangerous'] = metadata_df['dx'].isin(dangerous_dx)\ndangerous_share_per_localization = (\n    pd.DataFrame({\n        'dangerous_share_percent': (\n            metadata_df\n            .groupby('localization')['is_dangerous']\n            .mean()\n            .sort_values(ascending=False) * 100\n        )\n    })\n    .round(2)\n    .reset_index()\n)\n\ndangerous_share_per_localization\n\n\n\n\nTabelle 5: Ranking der Körperstellen mit den meisten gefährlichen Diagnosen\n\n\n\n\n\n\n\n\n\n\nlocalization\ndangerous_share_percent\n\n\n\n\n0\nface\n42.68\n\n\n1\nscalp\n36.72\n\n\n2\near\n35.71\n\n\n3\nneck\n31.55\n\n\n4\nchest\n31.20\n\n\n5\nupper extremity\n28.98\n\n\n6\nback\n24.59\n\n\n7\nhand\n17.78\n\n\n8\nlower extremity\n15.17\n\n\n9\nfoot\n10.03\n\n\n10\nabdomen\n8.71\n\n\n11\nunknown\n6.41\n\n\n12\ntrunk\n4.20\n\n\n13\nacral\n0.00\n\n\n14\ngenital\n0.00",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#verteilung-der-diagnosen-nach-alter",
    "href": "kapitel/datenexploration.html#verteilung-der-diagnosen-nach-alter",
    "title": "Datenexploration",
    "section": "Verteilung der Diagnosen nach Alter",
    "text": "Verteilung der Diagnosen nach Alter\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_data = pd.crosstab(metadata_df['dx'], metadata_df['age_group'])\nsns.heatmap(\n    data=heatmap_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Diagnose / Alter')\nplt.xlabel('Altersgruppe')\nplt.ylabel('Diagnose')\nplt.yticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nheatmap_dangerous_data = pd.crosstab(metadata_dangerous_df['dx'], metadata_df['age_group'])\nsns.heatmap(\n    data=heatmap_dangerous_data,\n    annot=True,\n    cmap='Reds',\n    fmt='d'\n)\nplt.title('Diagnose / Alter (gefährl. Diagnosen)')\nplt.xlabel('Altersgruppe')\nplt.ylabel('Diagnose')\nplt.yticks(rotation=0)\nplt.show()",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#beispielfotos-anhand-häufig-auftretender-attribute",
    "href": "kapitel/datenexploration.html#beispielfotos-anhand-häufig-auftretender-attribute",
    "title": "Datenexploration",
    "section": "Beispielfotos anhand häufig auftretender Attribute",
    "text": "Beispielfotos anhand häufig auftretender Attribute\n\n\nCode\nall_images = os.listdir('../images')\nisic_images = [file_name for file_name in all_images if file_name.startswith('ISIC')]\ncolumns = 2\nrows = (len(isic_images) + columns - 1) // columns\nfig, ax = plt.subplots(rows, columns, figsize=(8, 4*rows))\nax = ax.flatten()\nfor index, image_name in enumerate(isic_images):\n    image_path = os.path.join('../images', image_name)\n    image = Image.open(image_path)\n    metadata = metadata_df[metadata_df['image_id'] == image_name.removesuffix('.jpg')]\n    ax[index].imshow(image)\n    ax[index].set_title(f'{metadata['dx'].iloc[0]} - {metadata['age'].iloc[0]:.0f} years - {metadata['localization'].iloc[0]}')\nplt.show()",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#sec-korrelation",
    "href": "kapitel/datenexploration.html#sec-korrelation",
    "title": "Datenexploration",
    "section": "Korrelationsanalyse nach Spearman",
    "text": "Korrelationsanalyse nach Spearman\nEin Weg um diesen Zusammenhang zu untersuchen ist der Korrelationskoeffizient nach Spearman [2].\nDie Wahl des Spearman-Korrelationskoeffizienten rechtfertigt sich durch die folgenden Eingeschaften:\n\nEr setzt keine Normalverteilung voraus\nEr misst den monotonen Zusammenhang zwischen zwei Merkmalen\nEr ist robust gegenüber Ausreißern, da auf Rangwerte zurückgegriffen wird\nEr eignet sich für ordinal skalierte oder kategorisiert kodierte Merkmale, wie sie im Datensatz vorliegen\n\nSeien \\(X=(x_1, x_2, ...,x_n)\\) und \\(Y=(y_1, y_2, ..., y_n)\\) zwei Merkmalsreihen mit \\(n\\) Beobachtungen. Dann ist der Spearman-Korrelationskoeffizient \\(\\rho_s\\) definiert als Pearson-Korrelationskoeffizient der Rangwerte \\(R(x_i)\\) und \\(R(y_i)\\) mit \\(d_i = R(x_i) - R(y_i)\\):\n\\[\n\\rho_s = 1 - \\frac{6 \\sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}\n\\]\nAngewendet auf den in Kapitel 1 beschriebenen Datensatz ergibt sich die in Abbildung 12 dargestellt Korrelationsmatrix. Es lässt sich ein Zusammenhang zwischen dx und age sowie dx_type und dx erkennen.\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nsns.heatmap(\n    data=corr_matrix,\n    annot=True,\n    cmap='coolwarm',\n    fmt='.2f'\n)\nplt.yticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 12: Korrelationsmatrix des Datensatzes\n\n\n\n\n\nDas der Zusammenhang der Variablen im gesamten Datensatz nicht besonders signifikant ist (mit Ausnahme des Alters) lässt sich ebenfalls visuell in Abbildung 13 erkennen. Somit lässt sich aus den Ergebnissen von Kapitel 2 und Kapitel 3.1 schlussfolgern, dass das Risiko für gefährliche Diagnosen mit dem Alter zunimmt. Dies wird ebenfalls in Abbildung 14 visualisiert. Zudem lässt sich anhand von Abbildung 10 erkennen, dass es wahrscheinlicher ist, eine gefährliche Diagnose im Bereich des Rückens, Gesichts oder der oberen Extremität zu erhalten. Diese Verteilung kann aber zum Teil auf die Anzahl der Muttermale an den jeweiligen Körperstellen zurückgeführt werden (siehe dazu Abbildung 7). Einen interessanten Zusammenhang, beschreibt die Anzahl der gefährlichen Diagnosen an einer Körperstelle in Relation zu den gesamten Diagnosen an der jeweiligen Stelle (siehe Tabelle 5). Auffällig ist, dass unbedeckte Körperstellen, welche viel Sonneneinstrahlung abbekommen tendenziell gefährdeter sind als bspw. der Genitalbereich.\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\ngrid = sns.PairGrid(metadata_df[predictor_columns], diag_sharey=False)\ngrid.map_upper(sns.scatterplot, s=15)\ngrid.map_lower(sns.kdeplot)\ngrid.map_diag(sns.kdeplot, lw=2)\nplt.show()\n\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 13: : Paar Gatter der Prädiktor Attribute",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/datenexploration.html#anzahl-der-gefährlichen-diagnosen-nach-alter",
    "href": "kapitel/datenexploration.html#anzahl-der-gefährlichen-diagnosen-nach-alter",
    "title": "Datenexploration",
    "section": "Anzahl der gefährlichen Diagnosen nach Alter",
    "text": "Anzahl der gefährlichen Diagnosen nach Alter\n\n\nCode\nplt.figure(figsize=STANDARD_FIGSIZE)\nsns.histplot(\n    data=metadata_df,\n    x='age_group',\n    hue='is_dangerous',\n    palette=DANGEROUS_PALETTE,\n    multiple='stack',\n    edgecolor='white'\n)\nplt.xlabel('Altersgruppe')\nplt.ylabel('Anzahl Läsionen')\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 14: Anzahl der gefährlichen Diagnosen nach Alter",
    "crumbs": [
      "Datenexploration"
    ]
  },
  {
    "objectID": "kapitel/einfuehrung.html",
    "href": "kapitel/einfuehrung.html",
    "title": "Einleitung",
    "section": "",
    "text": "Motivation\nKünstliche Intelligenz. Ein Begriff, welcher scheinbar immer häufiger in unseren Alltag, aber auch in weiten Bereichen der Wirtschaft Einzug hat [1, S. 4–10]. Dementsprechend wird es ebenfalls immer wichtiger zu verstehen, was künstliche Intellignez (KI) eigentlich ist. Hierfür wird diese Arbeit im Rahmen des Studienmoduls ‘Advanced Topics in Computer Science’ ausgearbeitet.\nDas Ziel hierbei ist es, das allgemeine Verständnis des Themenfeldes ‘Künstliche Intelligenz’ durch nähere Betrachtung eines spezifischen Themas, zu steigern beziehungsweise zu verbesseren. Zu diesem Zweck beschäftigt sich diese wissenschaftliche Arbeit mit dem Aufbau und Analyse eines KI-Modells, welches durch die Analyse von Bildern eine Hautkrebsdiagnose durchführen soll.\nDieses Thema wurde ausgewählt, nicht nur, weil es, wie im folgenden Abschnitt 2 näher erläutert, ein relevantes Thema ist, sondern auch, weil es eine Reihe von Einblicken gewährt, wie KIs Bilder analysieren und interpretieren können und wie dies sowohl im Alltag als auch in der Wirtschaft zum Einsatz kommen könnte. Des Weiteren werden auch generelle Erkenntnisse zum Aufsetzen und Trainieren von KI-Modellen gewonnen.\n\n\nRelevanz des Themas\nIm Bereich des Gesundheitsheitwesens weitet sich der Einsatz und Nutzen von Künstlicher Intelligenz ebenfalls immer weiter aus [1, S. 29ff]. Der Einsatz umfasst dabei unter anderem die Einsatzbereiche der medizinischen Bildgebung beziehungsweise der Diagnostik [2, Abs. 1], welche in dieser Arbeit von besonderem Interesse sind.\nJedes Jahr werden ungefähr 123.000 Fälle von Hautkrebs identifiziert. Dabei ist in dem Bereich der Dermatologie, welcher erste Diagnosen oft durch visuelle Mustererkennung erstellen muss, die Nutzung von KI besonderes hilfreich und vegelichweise gut umsetztbar.[2, Kap. 6.3.5.4]\nMelanoma, die tödlichste Art von Hautkrebs, ist die 19. häufigste auftretetende Krebsart. Gleichzeitg ist aber auch so, dass wenn Melanoma in einem frühen Stadium entdeckt wird, dass die Überlebenschance der Betroffenen über 95% liegt.\nJedoch ist eine manuelle Analyse durch den Dermatologen, für jedes Muttermal oder Hautirretation, mit einem hohen Kosten- und Zeitaufwand verbunden.[3]\n\n\nForschungsgegenstand und Vorgehensweise\nDaher soll, wie zuvor erwähnt Kapitel 1, in dieser Arbeit ein Modell über einen Machine-Learning Ansatz erstellt und betrachtet werden. Hierfür wird der HAM10000 Datensatz [4] als Trainingsgrundlage verwendet. Dieser wird in der Datenexploration näher betrachtet. Ebenfalls werden statistische Metriken und Abhängigkeiten analysiert.\nZunächst werden jedoch thematische Grundlagen definiert. In dem Abschnitt werden dann Hautkrebsarten und entsprechende Diagnosewerkzeuge, sowie eine Definition von künstliche Intelligenz und die zugehörigen Anwendungenbereiche im medizinischen Sektor.\nNach der Datenexploration werden anschließend die Datenvorbereitung, die Wahl des KI-Modell Ansatz, des Trainingsprozess und Paramenter, sowie die Validierung und die Evaluation protokoliert.\nAbschließend werden dann die Forschungsergebnisse interpretiert, das erstellte Modell mit weiteren bereits vorhandenen Modellen verglichen und ein Ausblick, sowie mögliche Verbesserungen gegeben.\n\n\n\n\n\nLiteratur\n\n[1] Statista, „Künstliche Intelligenz - Hauptanwendungsfelder“, Statista, 2024. Verfügbar unter: https/de.statista.com/statistik/studie/id/38585/dokument/hauptanwendungsfelder-von-ki/\n\n\n[2] J. Du, M. Huang, und L. Liu, „KI-unterstützte Krankheitsvorhersage in der visualisierten Medizin“, in Visualisierung in der Medizin, Z. Liu, Hrsg., Singapore: Springer Nature Singapore, 2025, 6, S. 117–139. doi: https://doi.org/10.1007/978-981-97-9693-9_6.\n\n\n[3] S. Vashdev Asrani, J. Lalchandani, I. Desai, T. Jeswani, und S. Khedkar, „SkInspection: Recognizing the Type of Skin Cancer Using Deep Learning“, in Proceedings of the International Health Informatics Conference, S. Jain, B. K. Bhargava, D. Kalra, und S. Groppe, Hrsg., Singapore: Springer Nature Singapore, 2025, 12, S. 163–174. doi: https://doi.org/10.1007/978-981-97-7190-5_12.\n\n\n[4] P. Tschandl und V. Group, „The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions“. https://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/DBW86T, 2018. doi: 10.7910/DVN/DBW86T.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html",
    "href": "kapitel/modellentwicklung.html",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Da die im Datensatz enthaltenen Metadaten der Läsionen lediglich Tendenzen (siehe Datenexploration) erkennen lassen und alleinstehend nicht zur eindeutigen Identifikation von Diagnosen verwendet werden können, müssen ebenfalls die Bilder der betroffenen Hautstellen untersucht werden. Für diese Analyse werden unterschiedliche Ansätze des maschinellen Lernens untersucht.\n\n\nConvolutional Neural Networks stellen einen Spezialtypen künstlicher neuronaler Netzwerke dar der besonders für die Verarbeitung Bilddaten konzipiert wurde. Im Gegensatz zu herkömmlichen neuronalen Netzwerken verwenden CNNs spezielle Operationen, die auf der mathematischen Faltung (Convolution) basieren. [1]\n\n\nBei der Verarbeitung von digitalen Signale oder Bildern können die Eingabeparameter meistens durch diskrete Funktionen abgebildet werden.\n\nDefinition 1 Seien \\(f, g: D \\to \\mathbb{C}\\) mit dem diskreten Definitionsbereich \\(D \\subseteq \\mathbb{Z}\\). Dann ist die diskrete Faltung definiert durch:\n\\[\n(f * g)(n)=\\sum_{k \\in D}{f(k)g(n-k)}\n\\]\n\nFaltungsmatritzen (auch Kernel oder Filter) sind meist quadratische Matrizen ungerader Abmessungen in unterschiedlichen Größen. Einige Bildbearbeitungsoperationen können als lineares System interpretiert werden, wobei eine diskrete Faltung (siehe Definition 1) angewendet wird. Für diskrete zweidimensionale Funktionen (digitale Bilder, Signale, etc.) ergibt sich die folgende Berechnungsformel\n\nDefinition 2 Sei \\(I^{\\ast} (x,y)\\) das Ergebnispixel, \\(I\\) das Bild, auf welches der Filter angewendet wird, \\(a\\) die Koordinate des Mittelungspunkts in der quadratischen Faltungsmatrx und \\(k(i, j)\\) ein Element der Faltungsmatrix. Dann ist die Berechnungsformel der diskreten Faltung definiert durch: \\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)k(i, j)}\n\\]\n\nDas in Definition 2 dargestellte mathematische Konzept der diskreten Faltung wird auf ein 16x16 Pixel großes Bild angewendet (mit den Werten 0 für schwarz und 1 für weiß). Um die visuelle Wirkung der Faltungsoperation zu demonstrieren, wird eine spezifische Faltungsmatrix \\(k\\) verwendet.\n\\[\nk = \\frac{1}{9} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n\\]\nDie hier verwendete Faltungsmatrix \\(k\\) ist ein 3×3 Mittelwertfilter, bei dem jedes Element den Wert \\(\\frac{1}{9}\\) hat. Dieser Filter führt zu einer Glättung des Bildes (siehe Abbildung 1), da er für jedes Pixel den Durchschnitt der umliegenden 3×3 Nachbarschaft berechnet. Im Beispiel wird ein ringförmiges Muster geglättet, was die Kanten des Rings weicher erscheinen lässt und einen Unschärfeeffekt erzeugt. [2]\n\n\nCode\nimage = np.zeros((16, 16))\nfor i in range(16):\n    for j in range(16):\n        dist = np.sqrt((i-7.5)**2 + (j-7.5)**2)\n        if 3 &lt;= dist &lt;= 6:\n            image[i, j] = 1.0\nkernel = 1/9 * np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\nfiltered_image = signal.convolve2d(image, kernel, mode='same', boundary='symm')\nfig, axes = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\naxes[0].imshow(image, cmap='gray')\naxes[0].set_title('Originalbild (16x16)')\naxes[0].axis('off')\n\naxes[1].imshow(filtered_image, cmap='gray')\naxes[1].set_title('Nach Faltung mit k')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 1: Beispiel einer Faltungsoperation\n\n\n\n\n\n\n\n\nEin CNN besteht aus mehreren Schichten, welche die Grundlage für die meinsten modernen Bilderkennungmodelle darstellen.\n\n\n\n\n\n\nflowchart TD\n    Input(\"Input Image\") --&gt; Conv\n\n    subgraph Conv[\"Convolutional Layer\"]\n        direction LR\n        C1[\"Feature Extraction\"] --- C2[\"Filter Application\"]\n    end\n\n    Conv --&gt; Pool\n\n    subgraph Pool[\"Pooling Layer\"]\n        direction LR\n        P1[\"Dimension Reduction\"] --- P2[\"Parameter Reduction\"]\n    end\n\n    Pool --&gt; FC\n\n    subgraph FC[\"Fully Connected Layer\"]\n        direction LR\n        F1[\"Feature Classification\"] --- F2[\"Output Prediction\"]\n    end\n\n    FC --&gt; Output(\"Classification Result\")\n\n    style Input fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Output fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Conv fill:#d1ecf1,stroke:#0c5460,stroke-width:1px\n    style Pool fill:#d4edda,stroke:#155724,stroke-width:1px\n    style FC fill:#f8d7da,stroke:#721c24,stroke-width:1px\n\n\n\n\nAbbildung 2: CNN Funktionsweise\n\n\n\n\n\nDer in Abbildung 2 dargestellte Prozessfluss beschreibt die Grundidee von CNNs. Die Schichten, welche verwendet werden erfüllen dabei die folgenden Aufgaben:\n\nConvolutional Layer: Extrahiert Merkmale durch die Anwendung der in Kapitel 1.1.1 beschriebenen Filter.\nPooling Layer: Reduziert Dimensionen und erhöht die Berechnungseffizienz.\nFully Connected Layer: Klassifiziert die extrahierten Merkmale.\n\nZur Reduktion der Dimensionen (pooling) können je nach Kontext verschiedene Funktionen angewendet werden. Es ist möglich nach Maxima, Minima und Durchschnittswerten zu reduzieren. Dabei werden, ähnlich wie bei der Faltung, Matrizen auf einem Wert abgebildet. Diese Pooling-Matrix bewegt sich über die Ursprüngliche Matrix. Dabei sind folgende Parameter entscheident:\n\nFilter-Size (Pooling Größe): Definiert die Größe der Pooling Matrix.\nStride: Bestimmt die Schrittgröße, mit der sich die Pooling Matrix über die Ursprüngliche Matrix bewegt.\n\n\nDefinition 3 Die größe der aus dem Pooling Prozess hervorgehenden reduzierten Matrix berechne sich mit der folgenden Formel. Seien \\(h\\) die Höhe der Eingabematrix, \\(w\\) die Breite der Eingabematrix, \\(c\\) die Anzahl der Eingabematrizen (Annahme: Alle Eingabematrizen sind gleich groß), \\(f\\) die Größe der Pooling-Matrix und \\(s\\) der Stride.\n\\[\nc \\left(\\left\\lfloor\\frac{h-f+1}{s}\\right\\rfloor + 1\\right) \\left(\\left\\lfloor\\frac{w-f+1}{s}\\right\\rfloor + 1\\right)\n\\]\n\nIm folgenden wird dargestellt, wie die Größe eines quadratischen Bildes mit einem Farbkanal (mit den Werten 0 für schwarz und 1 für weiß) durch den Pooling Prozess reduziert werden kann. Zur Berechnung der Bildgröße nach dem Pooling Prozess wird die Formel aus Definition 3 angewendet. [3]\n\n\nCode\npicture_width = np.array([counter for counter in range(16, 500)])\n\ndef picture_size(width):\n    return width**2\n\ndef pooling_size(width, matrix_size, stride):\n    output_dim = (np.floor((width - matrix_size + 1) / stride) + 1)\n    return int(output_dim**2)\n\nfilter_size = 3\nstride = 2\n\noriginal_sizes = [picture_size(w) for w in picture_width]\npooled_sizes = [pooling_size(w, filter_size, stride) for w in picture_width]\n\nplt.figure(figsize=STANDARD_FIGSIZE)\nplt.plot(picture_width, original_sizes, 'b-', label='Originalbild')\nplt.plot(picture_width, pooled_sizes, 'r-', label=f'Nach Pooling (Filter: {filter_size}x{filter_size}, Stride: {stride})')\nplt.xlabel('Breite des Originalbildes (Pixel)')\nplt.ylabel('Anzahl Pixel')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 3: Auswirkung von Pooling auf die Bildgröße\n\n\n\n\n\nIn den Abbildungen Abbildung 4 und Abbildung 5 wird exemplarisch dargestellt, wie sich unterschiedliche Pooling-Methoden auf ein Eingabebild auswirken. Dabei wird jeweils ein Filter der Größe 3×3 mit einem Stride von 2 über das Bild bewegt.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\npicture = np.random.randint(0, 31, size=(9, 9), dtype=np.uint8)\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\navg_pooled = F.avg_pool2d(tensor, kernel_size=3, stride=2)\n\nvmin, vmax = 0, 31\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(avg_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Durchschnitts Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 4: Beispiel für Durchschnitts-Pooling (3x3, Stride 2)\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Verwende das gleiche Bild wie oben\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nmax_pooled = F.max_pool2d(tensor, kernel_size=3, stride=2)\n\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(max_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Maxima Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 5: Beispiel für Maxima-Pooling (3x3, Stride 2)\n\n\n\n\n\nBeide Methoden reduzieren die Bildgröße und damit die Anzahl der zu verarbeitenden Parameter, wobei sie jeweils unterschiedliche Eigenschaften des ursprünglichen Bildes erhalten oder herausfiltern. Die Wahl der Pooling-Methode hängt daher stark vom Einsatzzweck und den gewünschten Modell-Eigenschaften ab. [4]\n\n\n\n\nHerkömmliche CNNs basieren auf einer sequentiellen Abfolge der in Kapitel 1.1.2 beschriebenen Operationen. U-Net hingegen ist eine spezielle Variante eines CNN, welche ursprünglich für biomedizinische Bildsegmentierung konzipiert wurde.\n\n\nDie U-Net Architektur besteht aus einer kontrahierenden Komponente (Encoder-Pfad) und einer expandierenden Komponente (Decoder-Pfad), welche zusammen eine U-Form bilden.\n\n\n\n\n\n\nAbbildung 6: U-Net Architektur (aus Ronneberger 2015)\n\n\n\nWie in Abbildung 6 zu erkennen ist, wird auf dem Encoder-Pfad zur Feature Extraktion die Aktivierungsfunktion Rectified Linear Units (ReLU) verwendet (siehe Definition 4). Folglich besteht der Encoder-Pfad aus der wiederholten Anwendung von Faltungen (siehe Definition 2) gefolgt von Aktivierungsfunktionen und Max-Pooling (siehe Abbildung 5) zur Dimensionsreduktion. Dabei verdoppelt jeder Downsampling-Schritt die Anzahl der Feature-Kanäle1.\n\nDefinition 4 Sei \\(f\\) die ReLU Aktivierungsfunktion und \\(x\\) das Eingabepixel.\n\\[\nf(x) = \\max (0, x)\n\\]\n\nDer Decoder-Pfad tätigt ein Upsampling der Feature-Map, gefolgt von einer Faltung, welche die Anzahl der Feature-Kanäle halbiert. Entscheident für die Leistungsfähigkeit sind die sogenannten “Skip Connections”, welche eine direkte Verbindung zwischen den Feature-Maps im Encoder- und Decoder-Pfad symbolisieren. [5]\n\n\n\n\nDefinition 5 Die Sigmoid-Funktion \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) hat eine Ableitung, die wie folgt definiert ist:\n\\[\n\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n\\]\nDiese Ableitung erreicht ihren Maximalwert von 0,25 bei \\(x = 0\\) und nähert sich 0 an, wenn \\(|x|\\) größer wird. Dies ist eine der Hauptursachen für das Vanishing-Gradient-Problem in tiefen neuronalen Netzwerken.\n\nUrsprünglich wurden Skip Connections entwickelt, um das Problem des “Verschwindenden Gradienten” (siehe Definition 6) in modernen neuronalen Netzwerken zu lösen. Dieses Problem tritt auf, wenn Netzwerke durch Gradientenabstieg optimiert werden, wobei die Aktualisierung der Parameter in Richtung des negativen Gradienten der Kostenfunktion erfolgt.\n\nDefinition 6 Sei \\(L\\) die Kostenfunktion, \\(x_i\\) die Ausgabe (Aktivierung) der \\(i\\)-ten Schicht eines neuronalen Netzwerks und \\(i\\) der Index der Schicht. Die Ausgabe der nächsten Schicht \\(x_{i+1}\\) hängt von \\(x_i\\) über eine Transformation \\(f_i\\) ab, also \\(x_{i+1} = f_i(x_i)\\). Die Gradientenpropagation von der Kostenfunktion \\(L\\) zur Ausgabe \\(x_i\\) erfolgt gemäß der Kettenregel:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_{i+1}} \\frac{\\partial x_{i+1}}{\\partial x_i}\n\\]\nDer Kern des Vanishing-Gradient-Problems liegt in der wiederholten Anwendung der Kettenregel über viele Schichten hinweg. Für eine Schicht \\(i\\) in einem Netzwerk mit \\(n\\) Schichten ergibt sich der Gradient \\(\\frac{\\partial L}{\\partial x_i}\\) durch die Verkettung der Gradienten aller nachfolgenden Schichten:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_n} \\cdot \\prod_{k=i}^{n-1} \\frac{\\partial x_{k+1}}{\\partial x_k}\n\\]\nWenn viele dieser Ableitungen Beträge kleiner als 1 haben (z.B. bei Sigmoid-Aktivierungen), kann der Gesamtgradient schnell gegen Null tendieren, wodurch das Training tiefer Netzwerke erschwert wird.\n\nSkip Connections schaffen einen direkten Informationsfluss zwischen früheren und späteren Schichten des neuronalen Netzwerks. Mathematisch kann eine Skip Connection als Addition der Eingabe \\(x\\) zur Ausgabe einer Transformation \\(F(x)\\) dargestellt werden:\n\\[\ny = F(x) + x\n\\]\nDiese einfache Operation hat weitreichende Auswirkungen auf die Gradientenausbreitung im Netzwerk. Skip Connections ermöglichen alternative Pfade für die Gradientenpropagation, wodurch Gradienten direkt zu früheren Schichten fließen können, ohne durch alle dazwischenliegenden Transformationen und Aktivierungsfunktionen abgeschwächt zu werden. Da die Ableitung der Identitätsfunktion konstant 1 ist, wird der Gradient entlang des Skip-Pfades nicht verringert:\n\\[\n\\frac{\\partial(x + F(x))}{\\partial x} = 1 + \\frac{\\partial F(x)}{\\partial x}\n\\]\nDiese architektonischen Veränderungen führen zu einer signifikanten Glättung der Verlustlandschaft (siehe Abbildung 7), was die Optimierung neuronaler Netzwerke erheblich erleichtert und das Training tieferer Architekturen ermöglicht. [6]\n\n\n\n\n\n\nAbbildung 7: Verlustlandschaft (aus Hao Li 2017)\n\n\n\nIm U-Net werden entlang des Encoder-Pfads die Dimensionen der Feature-Maps reduziert und gleichzeitig die semantische Tiefe erhöht. Bei diesem Prozess gehen globale Details verloren. Um diese Verluste auszugleichen, werden im U-Net die Feature-Maps aus jeder Schicht im Encoder-Pfad durch Skip Connections direkt den ensprechenden Schichten auf dem Decoder Pfad zugeordnet (die grauen Pfeile in Abbildung 6).\n\nDefinition 7 Sei \\(F_{enc}\\) eine Feature-Map auf dem Encoder-Pfad und \\(F_{dec}\\) die rekonstruktive Feature-Map auf dem Decoder-Pfad selbiger Hierrachiestufe. Der zusammengesetzte Tensor ist definiert als:\n\\[\nF_{concat} = \\text{concat}(F_{enc}, F_{dec})\n\\]\n\nDiese Verknüpfungen erfolgen jedoch nicht aditiv wie zuvor beschrieben, sondern durch Konkatenation der Feature-Maps entlang der Kanalachse (siehe Definition 7). Diese Strategie ermöglicht es dem Decoder, sowohl die tiefen, abstrakten Merkmale aus dem Encoder als auch globalen Informationen zu berücksichtigen. Dadurch können präzisere Segmentierungen und Rekonstruktionen erzielt werden. [5]\n\n\n\n\nVision Transformer ist ein Ansatz zur Verarbeitung von Bilddaten, der Transformer-Architekturen, die aus dem Bereich der natürlichen Sprachverarbeitung (NLP) bekannt sind, für Bilder nutzbar macht. Anstatt sich auf Faltungen wie bei CNNs (siehe Kapitel 1.1.1) zu stützen, basiert ViT auf der Selbstaufmerksamkeitsmechanik (Self-Attention), die es dem Modell ermöglicht, globale Abhängigkeiten innerhalb eines Bildes effizient zu erfassen. [7]\n\n\nIm Gegensatz zu CNNs behandelt ein ViT ein Bild nicht als ein einzelnes, zusammenhängendes Gitter von Pixeln, sondern zerlegt es in kleinere Teilbereiche, sogenannte Patches.\n\nDefinition 8 Sei \\(I \\in \\mathbb{R}^{H \\times W \\times C}\\) ein Bild mit Höhe \\(H\\), Breite \\(W\\) und \\(C\\) Farbkanälen. Dieses Bild wird in \\(n\\) Patches der Größe \\(P^2\\) zerlegt, wobei:\n\\[\nn = \\frac{HW}{P^2}\n\\]\nJeder Patch wird dann zu einem Vektor der Länge \\(P^2 C\\) abgeflacht und durch eine lineare Projektion in einen \\(D\\)-dimensionalen Vektorraum eingebettet.\n\nDadurch wird das Bild als Sequenz von eingebetteten Patches dargestellt, vergleichbar mit einer Wortfolge in einem Sprachmodell.\n\n\n\nDie Architektur eines Vision Transformers besteht aus mehreren Hauptkomponenten, die systematisch aufeinander aufbauen, um ein leistungsfähiges Bildverarbeitungsmodell zu bilden. Die grundlegende Struktur wird in Abbildung 8 dargestellt.\n\n\n\n\n\n\nAbbildung 8: Vision Transformer Architektur (aus GeeksForGeeks 2025)\n\n\n\n\n\n\nMulti-Head Attention ist ein zentrales Konzept im Vision Transformer, welches es dem Modell ermöglicht, verschiedene Aspekte der Eingabesequenz parallel zu erfassen. Statt nur eine einzige Aufmerksamkeit (Attention) zu berechnen, werden mehrere unabhängige Attention-Köpfe parallel berechnet, deren Ergebnisse anschließend kombiniert werden.\n\nDefinition 9 Seien \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) die Abfrage-Matrix (Query), \\(K \\in \\mathbb{R}^{n \\times d_k}\\) die Schlüssel-Matrix (Key) und \\(V \\in \\mathbb{R}^{n \\times d_v}\\) die Wert-Matrix (Value), wobei \\(n\\) die Anzahl der Patches (Sequenzlänge), \\(d_k\\) die Dimension der Query- und Key-Vektoren und \\(d_v\\) die Dimension der Value-Vektoren ist.\nDie skalierte Punktprodukt-Attention wird berechnet als:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n\\]\nHierbei wird \\(QK^\\top\\) genutzt, um die Ähnlichkeit zwischen Abfragen und Schlüsseln zu messen, \\(\\sqrt{d_k}\\) dient als Skalierungsfaktor, um numerische Stabilität bei großen Dimensionen zu gewährleisten, und die Softmax-Funktion sorgt dafür, dass die resultierenden Gewichte normalisiert sind.\n\nUm verschiedene Repräsentationen gleichzeitig zu lernen, wird die Attention mehrfach mit unterschiedlichen Parametern berechnet. Jeder Attention-Kopf erhält dabei eigene Gewichtsmatrizen und kann sich auf unterschiedliche Aspekte der Eingabedaten konzentrieren.\n\nDefinition 10 Sei \\(h\\) die Anzahl der Köpfe. Für jeden Kopf \\(i \\in \\{1, \\dotsc, h\\}\\) existieren eigene Gewichtsmatrizen \\(W_i^Q \\in \\mathbb{R}^{D \\times d_k}\\) für Queries, \\(W_i^K \\in \\mathbb{R}^{D \\times d_k}\\) für Keys und \\(W_i^V \\in \\mathbb{R}^{D \\times d_v}\\) für Values.\nDie Multi-Head Attention berechnet für jeden Kopf:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\]\nAnschließend werden die Ergebnisse aller Köpfe zusammengefügt und nochmals linear projiziert:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dotsc, \\text{head}_h) W^O\n\\]\nmit \\(W^O \\in \\mathbb{R}^{h d_v \\times D}\\) als finaler Projektionsmatrix.\n\nDurch die Verwendung mehrerer Attention-Köpfe kann das Modell gleichzeitig verschiedene Beziehungen und Muster innerhalb der Bild-Patches erfassen. Stellen Sie sich vor, dass jeder Kopf wie ein Spezialist arbeitet, der bestimmte Eigenschaften im Bild sucht – einer achtet vielleicht mehr auf Farben, ein anderer auf Formen, wieder ein anderer auf Texturen. Diese parallele Verarbeitung verbessert die Fähigkeit des Modells, komplexe Strukturen in den Daten zu erkennen. Am Ende werden all diese spezialisierten Beobachtungen zusammengeführt, um ein umfassendes Verständnis des Bildes zu ermöglichen.\n\n\n\nVision Transformer unterscheiden sich in mehreren wesentlichen Aspekten von CNNs2:\n\nVerarbeitungsmechanismus: CNNs verwenden lokale Faltungsoperationen, während ViTs auf globaler Selbstaufmerksamkeit basieren.\nInduktive Bias: CNNs haben einen starken induktiven Bias für die lokale Struktur und Translationsinvarianz von Bildern, während ViTs weniger inhärente Annahmen über die Bildstruktur treffen.\nSkalierbarkeit: ViTs skalieren gut mit größeren Datenmengen und Modellgrößen und können bei ausreichendem Training CNNs in der Leistung übertreffen.\nRessourcenbedarf: Die quadratische Komplexität der Selbstaufmerksamkeit bezüglich der Sequenzlänge kann bei hochauflösenden Bildern zu erheblichem Berechnungsaufwand führen.\n\nIn der praktischen Anwendung werden häufig hybride Ansätze verfolgt, die Stärken beider Architekturen kombinieren. Beispielsweise können CNN-basierte Feature-Extraktoren mit Transformer-Modulen ergänzt werden, um sowohl lokale Details als auch globale Kontextinformationen effektiv zu verarbeiten. [8]\nVision Transformer haben in vielen Computer-Vision-Aufgaben Benchmark-Ergebnisse erzielt und werden zunehmend für medizinische Bildverarbeitung, einschließlich der Analyse von Hautläsionen, eingesetzt. Ihre Fähigkeit, komplexe Beziehungen über große räumliche Entfernungen hinweg zu modellieren, macht sie besonders wertvoll für die Erkennung subtiler Muster in dermatologischen Bildern. [9]\n\n\n\n\nFür die Klassifikation von Hautläsionen wird auf einen hybriden Modellansatz gesetzt, der die Stärken von Convolutional Neural Networks und Vision Transformern kombiniert.\nAls Grundlage dient EfficientNetV2-B0, ein kompakter und schneller CNN-Backbone, der speziell für eine hohe Trainingsgeschwindigkeit und verbesserte Skalierbarkeit entwickelt wurde. EfficientNetV2 nutzt klassische Faltungen sowie optimierte MBConv- und Fused-MBConv-Blöcke, wodurch sowohl die Modellgröße verringert als auch die Extraktion lokaler Merkmale effizient gestaltet wird. Die resultierenden Feature-Maps besitzen bereits eine reduzierte räumliche Auflösung und werden als Eingabe für nachgelagerte Vision Transformer Encoder-Blöcke verwendet.\nDie genaue Struktur von EfficientNetV2-B0 ist in Tabelle 1 dargestellt. Das Modell beginnt mit einer klassischen 3x3-Faltung (Stage 0) zur ersten Merkmalsextraktion, gefolgt von einer Reihe von Fused-MBConv-Blöcken (Stages 1–3), die eine besonders effiziente Kombination aus Faltung und Punktweise-Faltung darstellen. Ab Stage 4 kommen reguläre MBConv-Blöcke mit Squeeze-and-Excitation (SE)-Modulen zum Einsatz, die wichtige Merkmale selektiv verstärken. Die Anzahl der Kanäle steigt dabei schrittweise von 24 auf 256, während gleichzeitig die räumliche Auflösung durch Strides von 2 reduziert wird. Die finale Stufe (Stage 7) verwendet eine 1x1-Faltung, Global Average Pooling und eine Fully Connected Schicht, um die endgültige Repräsentation zu erzeugen.\n\n\n\nTabelle 1: EffiecientNetV2-B0 Architektur (aus Tan 2021)\n\n\n\n\n\n\n\n\n\n\n\n\nStage\nOperator\nStride\nChannels\nLayers\n\n\n\n\n0\nConv3x3\n2\n24\n1\n\n\n1\nFused-MBConv1, k3x3\n1\n24\n2\n\n\n2\nFused-MBConv4, k3x3\n2\n48\n4\n\n\n3\nFused-MBConv4, k3x3\n2\n64\n4\n\n\n4\nMBConv4, k3x3, SE0.25\n2\n128\n6\n\n\n5\nMBConv6, k3x3, SE0.25\n1\n160\n9\n\n\n6\nMBConv6, k3x3, SE0.25\n2\n256\n15\n\n\n7\nConv1x1 & Pooling & FC\n-\n1280\n1\n\n\n\n\n\n\nDurch diese Architektur werden die Vorteile der lokalen Detailerfassung des CNNs mit der globalen Kontextmodellierung des Transformers kombiniert. Zudem profitiert der Ansatz davon, dass der Transformer nur auf die komprimierten Feature-Maps angewendet wird, was die Berechnungslast erheblich reduziert, ohne die Modellqualität zu beeinträchtigen. [10]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#convolutional-neural-netwroks-cnns",
    "href": "kapitel/modellentwicklung.html#convolutional-neural-netwroks-cnns",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Convolutional Neural Networks stellen einen Spezialtypen künstlicher neuronaler Netzwerke dar der besonders für die Verarbeitung Bilddaten konzipiert wurde. Im Gegensatz zu herkömmlichen neuronalen Netzwerken verwenden CNNs spezielle Operationen, die auf der mathematischen Faltung (Convolution) basieren. [1]\n\n\nBei der Verarbeitung von digitalen Signale oder Bildern können die Eingabeparameter meistens durch diskrete Funktionen abgebildet werden.\n\nDefinition 1 Seien \\(f, g: D \\to \\mathbb{C}\\) mit dem diskreten Definitionsbereich \\(D \\subseteq \\mathbb{Z}\\). Dann ist die diskrete Faltung definiert durch:\n\\[\n(f * g)(n)=\\sum_{k \\in D}{f(k)g(n-k)}\n\\]\n\nFaltungsmatritzen (auch Kernel oder Filter) sind meist quadratische Matrizen ungerader Abmessungen in unterschiedlichen Größen. Einige Bildbearbeitungsoperationen können als lineares System interpretiert werden, wobei eine diskrete Faltung (siehe Definition 1) angewendet wird. Für diskrete zweidimensionale Funktionen (digitale Bilder, Signale, etc.) ergibt sich die folgende Berechnungsformel\n\nDefinition 2 Sei \\(I^{\\ast} (x,y)\\) das Ergebnispixel, \\(I\\) das Bild, auf welches der Filter angewendet wird, \\(a\\) die Koordinate des Mittelungspunkts in der quadratischen Faltungsmatrx und \\(k(i, j)\\) ein Element der Faltungsmatrix. Dann ist die Berechnungsformel der diskreten Faltung definiert durch: \\[\nI^{\\ast} (x, y) = \\sum^n_{i=1} \\sum^n_{j=1}{I(x-i+a, y-j+a)k(i, j)}\n\\]\n\nDas in Definition 2 dargestellte mathematische Konzept der diskreten Faltung wird auf ein 16x16 Pixel großes Bild angewendet (mit den Werten 0 für schwarz und 1 für weiß). Um die visuelle Wirkung der Faltungsoperation zu demonstrieren, wird eine spezifische Faltungsmatrix \\(k\\) verwendet.\n\\[\nk = \\frac{1}{9} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n\\]\nDie hier verwendete Faltungsmatrix \\(k\\) ist ein 3×3 Mittelwertfilter, bei dem jedes Element den Wert \\(\\frac{1}{9}\\) hat. Dieser Filter führt zu einer Glättung des Bildes (siehe Abbildung 1), da er für jedes Pixel den Durchschnitt der umliegenden 3×3 Nachbarschaft berechnet. Im Beispiel wird ein ringförmiges Muster geglättet, was die Kanten des Rings weicher erscheinen lässt und einen Unschärfeeffekt erzeugt. [2]\n\n\nCode\nimage = np.zeros((16, 16))\nfor i in range(16):\n    for j in range(16):\n        dist = np.sqrt((i-7.5)**2 + (j-7.5)**2)\n        if 3 &lt;= dist &lt;= 6:\n            image[i, j] = 1.0\nkernel = 1/9 * np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\nfiltered_image = signal.convolve2d(image, kernel, mode='same', boundary='symm')\nfig, axes = plt.subplots(1, 2, figsize=STANDARD_FIGSIZE)\naxes[0].imshow(image, cmap='gray')\naxes[0].set_title('Originalbild (16x16)')\naxes[0].axis('off')\n\naxes[1].imshow(filtered_image, cmap='gray')\naxes[1].set_title('Nach Faltung mit k')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 1: Beispiel einer Faltungsoperation\n\n\n\n\n\n\n\n\nEin CNN besteht aus mehreren Schichten, welche die Grundlage für die meinsten modernen Bilderkennungmodelle darstellen.\n\n\n\n\n\n\nflowchart TD\n    Input(\"Input Image\") --&gt; Conv\n\n    subgraph Conv[\"Convolutional Layer\"]\n        direction LR\n        C1[\"Feature Extraction\"] --- C2[\"Filter Application\"]\n    end\n\n    Conv --&gt; Pool\n\n    subgraph Pool[\"Pooling Layer\"]\n        direction LR\n        P1[\"Dimension Reduction\"] --- P2[\"Parameter Reduction\"]\n    end\n\n    Pool --&gt; FC\n\n    subgraph FC[\"Fully Connected Layer\"]\n        direction LR\n        F1[\"Feature Classification\"] --- F2[\"Output Prediction\"]\n    end\n\n    FC --&gt; Output(\"Classification Result\")\n\n    style Input fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Output fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style Conv fill:#d1ecf1,stroke:#0c5460,stroke-width:1px\n    style Pool fill:#d4edda,stroke:#155724,stroke-width:1px\n    style FC fill:#f8d7da,stroke:#721c24,stroke-width:1px\n\n\n\n\nAbbildung 2: CNN Funktionsweise\n\n\n\n\n\nDer in Abbildung 2 dargestellte Prozessfluss beschreibt die Grundidee von CNNs. Die Schichten, welche verwendet werden erfüllen dabei die folgenden Aufgaben:\n\nConvolutional Layer: Extrahiert Merkmale durch die Anwendung der in Kapitel 1.1.1 beschriebenen Filter.\nPooling Layer: Reduziert Dimensionen und erhöht die Berechnungseffizienz.\nFully Connected Layer: Klassifiziert die extrahierten Merkmale.\n\nZur Reduktion der Dimensionen (pooling) können je nach Kontext verschiedene Funktionen angewendet werden. Es ist möglich nach Maxima, Minima und Durchschnittswerten zu reduzieren. Dabei werden, ähnlich wie bei der Faltung, Matrizen auf einem Wert abgebildet. Diese Pooling-Matrix bewegt sich über die Ursprüngliche Matrix. Dabei sind folgende Parameter entscheident:\n\nFilter-Size (Pooling Größe): Definiert die Größe der Pooling Matrix.\nStride: Bestimmt die Schrittgröße, mit der sich die Pooling Matrix über die Ursprüngliche Matrix bewegt.\n\n\nDefinition 3 Die größe der aus dem Pooling Prozess hervorgehenden reduzierten Matrix berechne sich mit der folgenden Formel. Seien \\(h\\) die Höhe der Eingabematrix, \\(w\\) die Breite der Eingabematrix, \\(c\\) die Anzahl der Eingabematrizen (Annahme: Alle Eingabematrizen sind gleich groß), \\(f\\) die Größe der Pooling-Matrix und \\(s\\) der Stride.\n\\[\nc \\left(\\left\\lfloor\\frac{h-f+1}{s}\\right\\rfloor + 1\\right) \\left(\\left\\lfloor\\frac{w-f+1}{s}\\right\\rfloor + 1\\right)\n\\]\n\nIm folgenden wird dargestellt, wie die Größe eines quadratischen Bildes mit einem Farbkanal (mit den Werten 0 für schwarz und 1 für weiß) durch den Pooling Prozess reduziert werden kann. Zur Berechnung der Bildgröße nach dem Pooling Prozess wird die Formel aus Definition 3 angewendet. [3]\n\n\nCode\npicture_width = np.array([counter for counter in range(16, 500)])\n\ndef picture_size(width):\n    return width**2\n\ndef pooling_size(width, matrix_size, stride):\n    output_dim = (np.floor((width - matrix_size + 1) / stride) + 1)\n    return int(output_dim**2)\n\nfilter_size = 3\nstride = 2\n\noriginal_sizes = [picture_size(w) for w in picture_width]\npooled_sizes = [pooling_size(w, filter_size, stride) for w in picture_width]\n\nplt.figure(figsize=STANDARD_FIGSIZE)\nplt.plot(picture_width, original_sizes, 'b-', label='Originalbild')\nplt.plot(picture_width, pooled_sizes, 'r-', label=f'Nach Pooling (Filter: {filter_size}x{filter_size}, Stride: {stride})')\nplt.xlabel('Breite des Originalbildes (Pixel)')\nplt.ylabel('Anzahl Pixel')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 3: Auswirkung von Pooling auf die Bildgröße\n\n\n\n\n\nIn den Abbildungen Abbildung 4 und Abbildung 5 wird exemplarisch dargestellt, wie sich unterschiedliche Pooling-Methoden auf ein Eingabebild auswirken. Dabei wird jeweils ein Filter der Größe 3×3 mit einem Stride von 2 über das Bild bewegt.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\npicture = np.random.randint(0, 31, size=(9, 9), dtype=np.uint8)\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\navg_pooled = F.avg_pool2d(tensor, kernel_size=3, stride=2)\n\nvmin, vmax = 0, 31\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(avg_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Durchschnitts Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 4: Beispiel für Durchschnitts-Pooling (3x3, Stride 2)\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Verwende das gleiche Bild wie oben\ntensor = torch.tensor(picture, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nmax_pooled = F.max_pool2d(tensor, kernel_size=3, stride=2)\n\nsns.heatmap(pd.DataFrame(picture), cmap='gray', annot=True, ax=ax[0], vmin=vmin, vmax=vmax)\nax[0].set_title(\"Original\")\nsns.heatmap(pd.DataFrame(max_pooled.squeeze().numpy()), cmap='gray', annot=True, ax=ax[1], vmin=vmin, vmax=vmax)\nax[1].set_title(\"Maxima Pooling\\n(Filter: 3x3, Stride: 2)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAbbildung 5: Beispiel für Maxima-Pooling (3x3, Stride 2)\n\n\n\n\n\nBeide Methoden reduzieren die Bildgröße und damit die Anzahl der zu verarbeitenden Parameter, wobei sie jeweils unterschiedliche Eigenschaften des ursprünglichen Bildes erhalten oder herausfiltern. Die Wahl der Pooling-Methode hängt daher stark vom Einsatzzweck und den gewünschten Modell-Eigenschaften ab. [4]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#u-net",
    "href": "kapitel/modellentwicklung.html#u-net",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Herkömmliche CNNs basieren auf einer sequentiellen Abfolge der in Kapitel 1.1.2 beschriebenen Operationen. U-Net hingegen ist eine spezielle Variante eines CNN, welche ursprünglich für biomedizinische Bildsegmentierung konzipiert wurde.\n\n\nDie U-Net Architektur besteht aus einer kontrahierenden Komponente (Encoder-Pfad) und einer expandierenden Komponente (Decoder-Pfad), welche zusammen eine U-Form bilden.\n\n\n\n\n\n\nAbbildung 6: U-Net Architektur (aus Ronneberger 2015)\n\n\n\nWie in Abbildung 6 zu erkennen ist, wird auf dem Encoder-Pfad zur Feature Extraktion die Aktivierungsfunktion Rectified Linear Units (ReLU) verwendet (siehe Definition 4). Folglich besteht der Encoder-Pfad aus der wiederholten Anwendung von Faltungen (siehe Definition 2) gefolgt von Aktivierungsfunktionen und Max-Pooling (siehe Abbildung 5) zur Dimensionsreduktion. Dabei verdoppelt jeder Downsampling-Schritt die Anzahl der Feature-Kanäle1.\n\nDefinition 4 Sei \\(f\\) die ReLU Aktivierungsfunktion und \\(x\\) das Eingabepixel.\n\\[\nf(x) = \\max (0, x)\n\\]\n\nDer Decoder-Pfad tätigt ein Upsampling der Feature-Map, gefolgt von einer Faltung, welche die Anzahl der Feature-Kanäle halbiert. Entscheident für die Leistungsfähigkeit sind die sogenannten “Skip Connections”, welche eine direkte Verbindung zwischen den Feature-Maps im Encoder- und Decoder-Pfad symbolisieren. [5]\n\n\n\n\nDefinition 5 Die Sigmoid-Funktion \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) hat eine Ableitung, die wie folgt definiert ist:\n\\[\n\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n\\]\nDiese Ableitung erreicht ihren Maximalwert von 0,25 bei \\(x = 0\\) und nähert sich 0 an, wenn \\(|x|\\) größer wird. Dies ist eine der Hauptursachen für das Vanishing-Gradient-Problem in tiefen neuronalen Netzwerken.\n\nUrsprünglich wurden Skip Connections entwickelt, um das Problem des “Verschwindenden Gradienten” (siehe Definition 6) in modernen neuronalen Netzwerken zu lösen. Dieses Problem tritt auf, wenn Netzwerke durch Gradientenabstieg optimiert werden, wobei die Aktualisierung der Parameter in Richtung des negativen Gradienten der Kostenfunktion erfolgt.\n\nDefinition 6 Sei \\(L\\) die Kostenfunktion, \\(x_i\\) die Ausgabe (Aktivierung) der \\(i\\)-ten Schicht eines neuronalen Netzwerks und \\(i\\) der Index der Schicht. Die Ausgabe der nächsten Schicht \\(x_{i+1}\\) hängt von \\(x_i\\) über eine Transformation \\(f_i\\) ab, also \\(x_{i+1} = f_i(x_i)\\). Die Gradientenpropagation von der Kostenfunktion \\(L\\) zur Ausgabe \\(x_i\\) erfolgt gemäß der Kettenregel:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_{i+1}} \\frac{\\partial x_{i+1}}{\\partial x_i}\n\\]\nDer Kern des Vanishing-Gradient-Problems liegt in der wiederholten Anwendung der Kettenregel über viele Schichten hinweg. Für eine Schicht \\(i\\) in einem Netzwerk mit \\(n\\) Schichten ergibt sich der Gradient \\(\\frac{\\partial L}{\\partial x_i}\\) durch die Verkettung der Gradienten aller nachfolgenden Schichten:\n\\[\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial x_n} \\cdot \\prod_{k=i}^{n-1} \\frac{\\partial x_{k+1}}{\\partial x_k}\n\\]\nWenn viele dieser Ableitungen Beträge kleiner als 1 haben (z.B. bei Sigmoid-Aktivierungen), kann der Gesamtgradient schnell gegen Null tendieren, wodurch das Training tiefer Netzwerke erschwert wird.\n\nSkip Connections schaffen einen direkten Informationsfluss zwischen früheren und späteren Schichten des neuronalen Netzwerks. Mathematisch kann eine Skip Connection als Addition der Eingabe \\(x\\) zur Ausgabe einer Transformation \\(F(x)\\) dargestellt werden:\n\\[\ny = F(x) + x\n\\]\nDiese einfache Operation hat weitreichende Auswirkungen auf die Gradientenausbreitung im Netzwerk. Skip Connections ermöglichen alternative Pfade für die Gradientenpropagation, wodurch Gradienten direkt zu früheren Schichten fließen können, ohne durch alle dazwischenliegenden Transformationen und Aktivierungsfunktionen abgeschwächt zu werden. Da die Ableitung der Identitätsfunktion konstant 1 ist, wird der Gradient entlang des Skip-Pfades nicht verringert:\n\\[\n\\frac{\\partial(x + F(x))}{\\partial x} = 1 + \\frac{\\partial F(x)}{\\partial x}\n\\]\nDiese architektonischen Veränderungen führen zu einer signifikanten Glättung der Verlustlandschaft (siehe Abbildung 7), was die Optimierung neuronaler Netzwerke erheblich erleichtert und das Training tieferer Architekturen ermöglicht. [6]\n\n\n\n\n\n\nAbbildung 7: Verlustlandschaft (aus Hao Li 2017)\n\n\n\nIm U-Net werden entlang des Encoder-Pfads die Dimensionen der Feature-Maps reduziert und gleichzeitig die semantische Tiefe erhöht. Bei diesem Prozess gehen globale Details verloren. Um diese Verluste auszugleichen, werden im U-Net die Feature-Maps aus jeder Schicht im Encoder-Pfad durch Skip Connections direkt den ensprechenden Schichten auf dem Decoder Pfad zugeordnet (die grauen Pfeile in Abbildung 6).\n\nDefinition 7 Sei \\(F_{enc}\\) eine Feature-Map auf dem Encoder-Pfad und \\(F_{dec}\\) die rekonstruktive Feature-Map auf dem Decoder-Pfad selbiger Hierrachiestufe. Der zusammengesetzte Tensor ist definiert als:\n\\[\nF_{concat} = \\text{concat}(F_{enc}, F_{dec})\n\\]\n\nDiese Verknüpfungen erfolgen jedoch nicht aditiv wie zuvor beschrieben, sondern durch Konkatenation der Feature-Maps entlang der Kanalachse (siehe Definition 7). Diese Strategie ermöglicht es dem Decoder, sowohl die tiefen, abstrakten Merkmale aus dem Encoder als auch globalen Informationen zu berücksichtigen. Dadurch können präzisere Segmentierungen und Rekonstruktionen erzielt werden. [5]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#vision-transformer-vit",
    "href": "kapitel/modellentwicklung.html#vision-transformer-vit",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Vision Transformer ist ein Ansatz zur Verarbeitung von Bilddaten, der Transformer-Architekturen, die aus dem Bereich der natürlichen Sprachverarbeitung (NLP) bekannt sind, für Bilder nutzbar macht. Anstatt sich auf Faltungen wie bei CNNs (siehe Kapitel 1.1.1) zu stützen, basiert ViT auf der Selbstaufmerksamkeitsmechanik (Self-Attention), die es dem Modell ermöglicht, globale Abhängigkeiten innerhalb eines Bildes effizient zu erfassen. [7]\n\n\nIm Gegensatz zu CNNs behandelt ein ViT ein Bild nicht als ein einzelnes, zusammenhängendes Gitter von Pixeln, sondern zerlegt es in kleinere Teilbereiche, sogenannte Patches.\n\nDefinition 8 Sei \\(I \\in \\mathbb{R}^{H \\times W \\times C}\\) ein Bild mit Höhe \\(H\\), Breite \\(W\\) und \\(C\\) Farbkanälen. Dieses Bild wird in \\(n\\) Patches der Größe \\(P^2\\) zerlegt, wobei:\n\\[\nn = \\frac{HW}{P^2}\n\\]\nJeder Patch wird dann zu einem Vektor der Länge \\(P^2 C\\) abgeflacht und durch eine lineare Projektion in einen \\(D\\)-dimensionalen Vektorraum eingebettet.\n\nDadurch wird das Bild als Sequenz von eingebetteten Patches dargestellt, vergleichbar mit einer Wortfolge in einem Sprachmodell.\n\n\n\nDie Architektur eines Vision Transformers besteht aus mehreren Hauptkomponenten, die systematisch aufeinander aufbauen, um ein leistungsfähiges Bildverarbeitungsmodell zu bilden. Die grundlegende Struktur wird in Abbildung 8 dargestellt.\n\n\n\n\n\n\nAbbildung 8: Vision Transformer Architektur (aus GeeksForGeeks 2025)\n\n\n\n\n\n\nMulti-Head Attention ist ein zentrales Konzept im Vision Transformer, welches es dem Modell ermöglicht, verschiedene Aspekte der Eingabesequenz parallel zu erfassen. Statt nur eine einzige Aufmerksamkeit (Attention) zu berechnen, werden mehrere unabhängige Attention-Köpfe parallel berechnet, deren Ergebnisse anschließend kombiniert werden.\n\nDefinition 9 Seien \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) die Abfrage-Matrix (Query), \\(K \\in \\mathbb{R}^{n \\times d_k}\\) die Schlüssel-Matrix (Key) und \\(V \\in \\mathbb{R}^{n \\times d_v}\\) die Wert-Matrix (Value), wobei \\(n\\) die Anzahl der Patches (Sequenzlänge), \\(d_k\\) die Dimension der Query- und Key-Vektoren und \\(d_v\\) die Dimension der Value-Vektoren ist.\nDie skalierte Punktprodukt-Attention wird berechnet als:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n\\]\nHierbei wird \\(QK^\\top\\) genutzt, um die Ähnlichkeit zwischen Abfragen und Schlüsseln zu messen, \\(\\sqrt{d_k}\\) dient als Skalierungsfaktor, um numerische Stabilität bei großen Dimensionen zu gewährleisten, und die Softmax-Funktion sorgt dafür, dass die resultierenden Gewichte normalisiert sind.\n\nUm verschiedene Repräsentationen gleichzeitig zu lernen, wird die Attention mehrfach mit unterschiedlichen Parametern berechnet. Jeder Attention-Kopf erhält dabei eigene Gewichtsmatrizen und kann sich auf unterschiedliche Aspekte der Eingabedaten konzentrieren.\n\nDefinition 10 Sei \\(h\\) die Anzahl der Köpfe. Für jeden Kopf \\(i \\in \\{1, \\dotsc, h\\}\\) existieren eigene Gewichtsmatrizen \\(W_i^Q \\in \\mathbb{R}^{D \\times d_k}\\) für Queries, \\(W_i^K \\in \\mathbb{R}^{D \\times d_k}\\) für Keys und \\(W_i^V \\in \\mathbb{R}^{D \\times d_v}\\) für Values.\nDie Multi-Head Attention berechnet für jeden Kopf:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\]\nAnschließend werden die Ergebnisse aller Köpfe zusammengefügt und nochmals linear projiziert:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dotsc, \\text{head}_h) W^O\n\\]\nmit \\(W^O \\in \\mathbb{R}^{h d_v \\times D}\\) als finaler Projektionsmatrix.\n\nDurch die Verwendung mehrerer Attention-Köpfe kann das Modell gleichzeitig verschiedene Beziehungen und Muster innerhalb der Bild-Patches erfassen. Stellen Sie sich vor, dass jeder Kopf wie ein Spezialist arbeitet, der bestimmte Eigenschaften im Bild sucht – einer achtet vielleicht mehr auf Farben, ein anderer auf Formen, wieder ein anderer auf Texturen. Diese parallele Verarbeitung verbessert die Fähigkeit des Modells, komplexe Strukturen in den Daten zu erkennen. Am Ende werden all diese spezialisierten Beobachtungen zusammengeführt, um ein umfassendes Verständnis des Bildes zu ermöglichen.\n\n\n\nVision Transformer unterscheiden sich in mehreren wesentlichen Aspekten von CNNs2:\n\nVerarbeitungsmechanismus: CNNs verwenden lokale Faltungsoperationen, während ViTs auf globaler Selbstaufmerksamkeit basieren.\nInduktive Bias: CNNs haben einen starken induktiven Bias für die lokale Struktur und Translationsinvarianz von Bildern, während ViTs weniger inhärente Annahmen über die Bildstruktur treffen.\nSkalierbarkeit: ViTs skalieren gut mit größeren Datenmengen und Modellgrößen und können bei ausreichendem Training CNNs in der Leistung übertreffen.\nRessourcenbedarf: Die quadratische Komplexität der Selbstaufmerksamkeit bezüglich der Sequenzlänge kann bei hochauflösenden Bildern zu erheblichem Berechnungsaufwand führen.\n\nIn der praktischen Anwendung werden häufig hybride Ansätze verfolgt, die Stärken beider Architekturen kombinieren. Beispielsweise können CNN-basierte Feature-Extraktoren mit Transformer-Modulen ergänzt werden, um sowohl lokale Details als auch globale Kontextinformationen effektiv zu verarbeiten. [8]\nVision Transformer haben in vielen Computer-Vision-Aufgaben Benchmark-Ergebnisse erzielt und werden zunehmend für medizinische Bildverarbeitung, einschließlich der Analyse von Hautläsionen, eingesetzt. Ihre Fähigkeit, komplexe Beziehungen über große räumliche Entfernungen hinweg zu modellieren, macht sie besonders wertvoll für die Erkennung subtiler Muster in dermatologischen Bildern. [9]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#wahl-des-modellansatzes-für-die-hautläsionsklassifikation",
    "href": "kapitel/modellentwicklung.html#wahl-des-modellansatzes-für-die-hautläsionsklassifikation",
    "title": "Modellentwicklung",
    "section": "",
    "text": "Für die Klassifikation von Hautläsionen wird auf einen hybriden Modellansatz gesetzt, der die Stärken von Convolutional Neural Networks und Vision Transformern kombiniert.\nAls Grundlage dient EfficientNetV2-B0, ein kompakter und schneller CNN-Backbone, der speziell für eine hohe Trainingsgeschwindigkeit und verbesserte Skalierbarkeit entwickelt wurde. EfficientNetV2 nutzt klassische Faltungen sowie optimierte MBConv- und Fused-MBConv-Blöcke, wodurch sowohl die Modellgröße verringert als auch die Extraktion lokaler Merkmale effizient gestaltet wird. Die resultierenden Feature-Maps besitzen bereits eine reduzierte räumliche Auflösung und werden als Eingabe für nachgelagerte Vision Transformer Encoder-Blöcke verwendet.\nDie genaue Struktur von EfficientNetV2-B0 ist in Tabelle 1 dargestellt. Das Modell beginnt mit einer klassischen 3x3-Faltung (Stage 0) zur ersten Merkmalsextraktion, gefolgt von einer Reihe von Fused-MBConv-Blöcken (Stages 1–3), die eine besonders effiziente Kombination aus Faltung und Punktweise-Faltung darstellen. Ab Stage 4 kommen reguläre MBConv-Blöcke mit Squeeze-and-Excitation (SE)-Modulen zum Einsatz, die wichtige Merkmale selektiv verstärken. Die Anzahl der Kanäle steigt dabei schrittweise von 24 auf 256, während gleichzeitig die räumliche Auflösung durch Strides von 2 reduziert wird. Die finale Stufe (Stage 7) verwendet eine 1x1-Faltung, Global Average Pooling und eine Fully Connected Schicht, um die endgültige Repräsentation zu erzeugen.\n\n\n\nTabelle 1: EffiecientNetV2-B0 Architektur (aus Tan 2021)\n\n\n\n\n\n\n\n\n\n\n\n\nStage\nOperator\nStride\nChannels\nLayers\n\n\n\n\n0\nConv3x3\n2\n24\n1\n\n\n1\nFused-MBConv1, k3x3\n1\n24\n2\n\n\n2\nFused-MBConv4, k3x3\n2\n48\n4\n\n\n3\nFused-MBConv4, k3x3\n2\n64\n4\n\n\n4\nMBConv4, k3x3, SE0.25\n2\n128\n6\n\n\n5\nMBConv6, k3x3, SE0.25\n1\n160\n9\n\n\n6\nMBConv6, k3x3, SE0.25\n2\n256\n15\n\n\n7\nConv1x1 & Pooling & FC\n-\n1280\n1\n\n\n\n\n\n\nDurch diese Architektur werden die Vorteile der lokalen Detailerfassung des CNNs mit der globalen Kontextmodellierung des Transformers kombiniert. Zudem profitiert der Ansatz davon, dass der Transformer nur auf die komprimierten Feature-Maps angewendet wird, was die Berechnungslast erheblich reduziert, ohne die Modellqualität zu beeinträchtigen. [10]",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#bildnormalisierung",
    "href": "kapitel/modellentwicklung.html#bildnormalisierung",
    "title": "Modellentwicklung",
    "section": "Bildnormalisierung",
    "text": "Bildnormalisierung",
    "crumbs": [
      "Modellentwicklung"
    ]
  },
  {
    "objectID": "kapitel/modellentwicklung.html#footnotes",
    "href": "kapitel/modellentwicklung.html#footnotes",
    "title": "Modellentwicklung",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nBausteine der Feature Extraktion, die verschiede Aspekte des Eingabebilds wie Kanten, Texturen, Farben, etc. kodieren.↩︎\nChatGPT Prompt: Wie unterscheiden sich ViTs von CNNs im Bereich der medizinischen Bildverarbeitung. Bitte nenne 4 Punkte.↩︎",
    "crumbs": [
      "Modellentwicklung"
    ]
  }
]